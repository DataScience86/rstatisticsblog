'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/text-mining/text-pre-processing/','title':"Text Pre-Processing",'content':"Overview It is not surprising to know that most of the information out in the world is available in an unstructured form. An unstructured data refers to the one that does not reside in a traditional relational database. Some of the examples include images, webpages, word documents, pdf documents, and reports. In this section, we will talk about some basic concepts and functions which you can use for text processing in R.\nAlthough R provides several packages to perform tasks related to text processing, we will be using the tm, SnowballC, textstem, and stringr to perform several tasks related to text processing.\nHow to obtain the text from different sources As you can imagine, the primary source of text data is webpages, pdfs, and other forms of documents. So it becomes vital for us to learn how to extract data from different sources and prepare it for feature extraction.\nReading text file in R Reading a text file is relatively simple in R. You can use read.delim() file to read any text file in R.\n#reading text file\rmy_data \u0026lt;- read.delim(\u0026quot;rstatisticsblog.txt\u0026quot;)\rReading pdf file in R To read a pdf files we will be using pdf_text() function from pdftools package. A sample code is provided below:\ninstall.packages(\u0026quot;pdftools\u0026quot;)\rlibrary(pdftools)\r#setting the working directory\rsetwd(\u0026quot;C:/Users/EAOHSHM/Documents/Personal\u0026quot;)\r# Reading pdf file\rfile \u0026lt;- pdf_text(\u0026quot;Paradis-rdebuts_en.pdf\u0026quot;)\r# Number of pages\rlength(file)\r# Viewing 7th page\rfile[7]\rExtracting text from the website The process of extracting data from the website is also called as web scraping. The method of web scraping consists of a couple of steps and can become complicated, depending on the type of site and variety of components you wish to extract from the site. Some of the packages which you can use to extract data from different websites are rvest, RCurl and XML. For now, we will skip this part as it is beyond the scope of this article. But I encourage you to go and read more about these packages in R.\nGeneral text processing steps The different sets of actions we need to do as part of text cleaning or pre-process are as given below.\n Convert to lower case Remove numbers Remove punctuations Remove whitespace Remove stop words Stemming Lemmatization  Preparation Here We will go through all the basic text cleaning steps using some of the most popular text processing packages in R. List of packages used in this article is as given below:\nlibrary(stringr)\rlibrary(tm)\rlibrary(SnowballC)\rlibrary(textstem)\rConvert text to lower It is always a good idea to convert all of your text to the same case. R is a case sensitive language, and having the same word in two different cases can result in a program reading it as two separate entities.\ntext \u0026lt;- \u0026quot;This is a TEXT which Has different Cases\u0026quot;\rtext_lower \u0026lt;- tolower(text)\r[1] \u0026quot;this is a text which has different cases\u0026quot;\rRemove numbers In most cases, numbers present no real information, and thus it is a standard process to remove numerical values from the text. However, in some cases, you may be interested in extracting or collecting this information. We will be using a regular expression to remove the number. You can learn more about regular expressions here - https://regexone.com.\ntext \u0026lt;- \u0026quot;I contain number like 223, 12, and 24\u0026quot;\rgsub('[[:digit:]]+', '', text)\r[1] \u0026quot;I contain number like , , and \u0026quot;\rRemove punctuations To remove punctuation marks from the string, we will again fall back on regular expressions.\ntext \u0026lt;- \u0026quot;This string contains punctuation ?, % $ @ and #\u0026quot;\rgsub('[[:punct:]]', '', text)\r[1] \u0026quot;This string contains punctuation and \u0026quot;\rRemoving whitespaces You must have noticed that when we removed things like numbers or punctuations, than whitespace is created, and it is essential to remove those spaces else, they will be counted as empty strings.\ntext \u0026lt;- \u0026quot;This string contains punctuation and \u0026quot;\rgsub(' +',' ',text)\r[1] \u0026quot;This string contains punctuation and \u0026quot;\rBut with this approach, you can see that single whitespace is still left at the end. We can use the str_trim() function to get rid of the trailing whitespace.\ntext \u0026lt;- \u0026quot;This string contains punctuation and \u0026quot;\rstr_trim(text, side = \u0026quot;both\u0026quot;)\r[1] \u0026quot;This string contains punctuation and\u0026quot;\rRemove stop words Stop words are a collection of common words that do not provide any information about the content of the text. Here removeWords() function is being used to get rid of predefined stop words under the tm package. Based on one\u0026rsquo;s requirement, additional terms can be added to this list.\nlibrary(tm)\rtext \u0026lt;- \u0026quot;The goal of the present document (created in 2012) is to give a starting point for people newly interested in R. I chose to emphasize 30 times on the understanding of how R works, with the aim of a beginner, rather than expert, use.\u0026quot;\rtext \u0026lt;- tolower(text)\rremoveWords(text, stopwords())\r[1] \u0026quot;goal present document (created 2012) give starting point people newly interested r. chose emphasize 30 times understanding r works, aim beginner, rather expert, use.\u0026quot;\rIt is good practice to remove whitespaces at the end. For now, go ahead and clean the whitespaces from the above output string.\nStemming The stemming algorithm is a collection of rules that work by cutting the end or beginning of the word. This chopping of the word can be helpful in some cases, but not always. There are different algorithms, but the most common one is Porter stemmer. However, the purpose of all the algorithms is to reduce the word to its root level. Here we will use wordStem() function for stemming from SnowballC package in R.\nlibrary(SnowballC)\rtext \u0026lt;- c('companies', 'authorized', 'learning', \u0026quot;cooking\u0026quot;)\rtext \u0026lt;- tolower(text)\rwordStem(text)\r[1] \u0026quot;compani\u0026quot; \u0026quot;author\u0026quot; \u0026quot;learn\u0026quot; \u0026quot;cook\u0026quot; You can see from the above output, stemming does not always return a word which makes sense, or we can agree that it is the word representing the root of the actual word.\nLemmatization Just like stemming, lemmatization is also a process of finding a word\u0026rsquo;s lemma meaning root. Lemmatization is generally more accurate as compared to stemming because it considers a morphological analysis of the word by going through the dictionaries to link back the word to its lemma. Here we use lemmatize_words() function from textstem package in R.\nlibrary(textstem)\rtext \u0026lt;- c('companies', 'authorized', 'learning', \u0026quot;cooking\u0026quot;)\rtext \u0026lt;- tolower(text)\rlemmatize_words(text)\r[1] \u0026quot;company\u0026quot; \u0026quot;authorize\u0026quot; \u0026quot;learn\u0026quot; \u0026quot;cook\u0026quot;\rIt is quite evident that lemmatization returns much better results when compared to that of stemming.\nBonus - Create a corpus A corpus is a collection of text that has been put together with a certain set of predetermined criteria. The tm package provides two different implementations I.E. \u0026ldquo;VCorpus\u0026rdquo; and \u0026ldquo;PCorpus\u0026rdquo;. The default implementation is VCorpus(short for Volatile Corpus). It is called volatile since once the R object is destroyed, the whole corpus is gone, as it is stored in RAM. Another implementation is \u0026ldquo;PCorpus\u0026rdquo;, here P is short for permanent corpus. You will not lose the documents even if the R object is destroyed as PCorpus stores the documents in a physical form outside of R.\nBelow we will create a VCorpus corpus.\nlibrary(tm)\rCorpus_text = Corpus(VectorSource(text))\rImportant Notice\nStay updated by subscribing to our newsletter.\nWe hate spam just like you.\r\rSubscribe\r\r"});index.add({'id':1,'href':'/docs/useful-r-packages/ggplot2-tutorial/','title':"ggplot2",'content':"Overview The ggplot2 package in R provides a reliable system for describing and building graphs. The package is capable of creating elegant and aesthetically pleasing graphics. The framework of ggplot2 is quite different (in comparison to graphics package) and is based on the grammar of graphics(introduced initially by Leland Wilkinson). At first, you may not find it intuitive, but don\u0026rsquo;t worry, we are here to help. Together, we will master it to the core.\nBasic plotting framework for ggplot ggplot(data = dataset name) +\r\u0026lt;GEOM_FUNCTION\u0026gt;(mapping = aes(variable name))\rMapping the aesthetics(using aes) The aesthetic represents the object which you wish to plot in your graph. In other words, aesthetics represent different ways in which you can plot your data points. To showcase the data points, you can change things like size, shape, or color of the points. Thus by using aesthetics (represented by aes()) you can convey the information which is hidden in your dataset.\nFor example, you can map color to cylinder variable to reveal the relationship between mileage and weight. So let us take our framework and add aesthetics to it. Here we have three variables, and that means we have to pass three arguments to the aes() function.\n# Loading the library\rlibrary(ggplot2)\r# loading data and converting cyl variable to factor\rdata(mtcars)\rmtcars$cyl \u0026lt;- as.factor(mtcars$cyl)\r# Adding aesthetics\rggplot(data = mtcars) +\rgeom_point(mapping = aes(x = mpg, y = wt))\r\r\rMapping Geometric shapes(using geom) The geometric shapes in ggplot are visual objects which you can use to describe your data. For example, one can plot histogram or boxplot to describe the distribution of a variable.\nBelow mentioned two plots provide the same information but through different visual objects. These objects are defined in ggplot using geom. That means you can use geom to define your plot. For example, the histogram uses histogram geom, barplot uses bar geom, line plot uses line geom, and so on. There is one exception. We use point geom to plot the scatter plots.\nLet\u0026rsquo;s see how we can draw the charts, which we mentioned in the above example using geoms for the total sleep hours of animals.\nEvery geom function requires you to map an aesthetic to it. However, not every aesthetic requires a geom. For example, one can set the shape of a point, but you cannot set the shape of a line.\r\rBuilding histogram # Building a histogram\rggplot(data = msleep) +\rgeom_histogram(mapping = aes(x = sleep_total, col = \u0026quot;orange\u0026quot;))\r\r\rBuilding boxplot # Building a histogram\rggplot(data = msleep) +\rgeom_boxplot(mapping = aes(y = sleep_total))\r\r\rUsing Facets in ggplot2 Facet is a way in which you can add additional categorical variables to your plot. The facet helps in building the chart by dividing the data into two or more groups. The data from these groups are used for plotting the data.\nNow there are two ways in which you can use facets:\nA. If you want to split the data by only one variable, then use facet_wrap() function. In the following syntax, you will notice tilder(~). By default, this is the first argument. After this, you should mention the variable name by which you want to do the split.\nLet\u0026rsquo;s check the distribution of total sleep by kind of animal.\n# Working example of facet_wrap\rggplot(data = msleep) +\rgeom_histogram(mapping = aes(x = sleep_total)) +\rfacet_wrap(~ vore)\r\r\rB. If you want to split the data by a combination of two variables, then you can use facet_grid(). Here the two variables should be separated by the tilder(~).\nBuilding the scatter plot between mpg and disp variable by cyl and am type.\n# loading data\rdata(mtcars)\r# Converting cylinder(cyl) and automatic(am) variable to factor variables.\rmtcars$cyl \u0026lt;- as.factor(mtcars$cyl)\rmtcars$am \u0026lt;- as.factor(mtcars$am)\r# Working example of facet_grid\rggplot(data = mtcars) +\rgeom_point(mapping = aes(x = mpg, y = disp)) +\rfacet_grid(cyl ~ am)\r\r\rMapping colors to variables in ggplot2 Colors can play a game-changer role in any data visualization, and thus it becomes important for us to learn about it. The default color in ggplot is on the greyscale. But if you want, you can change the color.\nIn ggplot, there are a couple of ways in which you can use color.\nA. You can assign the colors to the objects, lines, and points. To color the objects, you can use fill() argument. To set colors to the lines and points, you can use the color argument. Below is a quick example of both cases.\nUsing color argument # Making the points blue color in the scatter plot\rggplot(data = mtcars) +\rgeom_point(mapping = aes(x = mpg, y = wt), color = \u0026quot;blue\u0026quot;)\r\r\rUsing fill argument # Making the bars of histogram blue\rggplot(data = iris) +\rgeom_histogram(mapping = aes(x = Sepal.Width), fill = \u0026quot;blue\u0026quot;)\r\r\rB. We can use color to map the values of the third variable, which we have already learned in the very first example under mapping aesthetics.\nBy default the ggplot2 uses scale_fill_hue() and scale_colour_hue() for color selection. However you can choose to change the luminance of these colors. Also there are other color scales available in R from RColorBrew package.\r\rExample 1 - Showcasing Default RColorBrew setup ggplot(data = mtcars) +\rgeom_boxplot(mapping = aes(x = cyl, y = mpg, fill=cyl)) +\rscale_fill_brewer()\r\r\rExample 2 - Showcasing Set1 pallette colors ggplot(data = mtcars) +\rgeom_boxplot(mapping = aes(x = cyl, y = mpg, fill=cyl)) +\rscale_fill_brewer(palette=\u0026quot;Set1\u0026quot;)\r\r\rExample 3 - Showcasing Spectral palette colors ggplot(data = mtcars) +\rgeom_boxplot(mapping = aes(x = cyl, y = mpg, fill=cyl)) +\rscale_fill_brewer(palette=\u0026quot;Spectral\u0026quot;)\r\r\rFor Your reference sharing the RBrewColor Pallet chart.\r\r\r\rUnderstanding the Coordinate System of ggplot2 The coordinates system of ggplot is a little complicated. But don\u0026rsquo;t worry, we will not dig too much. As of now, we will provide you with some examples of coordinate systems. If you pay attention to these, I think most of the job is done, and you are on your way to creating awesome charts using ggplot2. With time, I am sure you will be able to take deeper plunges into ggplot coordinate system. To start with, I have shortlisted some five functions as given below:\n1. coord_cartesian() - This is the default coordinate system in ggplot2. According to this system the X and Y positions of each point act independently to determine its location on the graph.\n2. coord_flip() - This is helpful in cases when you want to build horizontal graphs. This function switches the X and Y-axis. For example, you can use coord_flip to draw horizontal boxplots.\nggplot(data = mtcars) +\rgeom_boxplot(mapping = aes(x = cyl, y = mpg, fill=cyl)) +\rscale_fill_brewer(palette=\u0026quot;Set1\u0026quot;) +\rcoord_flip()\r\r\r3. coord_polar() - This creates a nice combination charts of bar and coxcomb or pie graphs by using polar coordinates.\n# Loading girbExtra Library\rlibrary(gridExtra)\r# Generating a barplot\rbar \u0026lt;- ggplot(data = mtcars) +\rgeom_bar(\rmapping = aes(x = cyl, fill = cyl),\rwidth = 1\r)\r# Saving the two plots\rplot1 \u0026lt;- bar + coord_flip()\rplot2 \u0026lt;- bar + coord_polar()\r# Plotting the graphs by cloumn\rgrid.arrange(plot1, plot2, ncol = 2)\rIn the above code, we have used a gridExtra package. I love this package it makes plotting multiple charts on the same canvas very easy.\r\r\r\r4. coord_map() - This functions creates a 2D map of the desired earth location. We use coor_polygon along with coord_map to a map with maintained aspect ratio. If you do not understand what this means then just run the code once without the coord_map part.\n# An example showcasing the map of USA\ritaly \u0026lt;- map_data(\u0026quot;italy\u0026quot;)\rggplot(italy, aes(long, lat, group = group)) +\rgeom_polygon(fill = \u0026quot;lightblue\u0026quot;, colour = \u0026quot;black\u0026quot;) +\rcoord_map()\r\r\r5. coord_fixed() - This coordinate system ensures that the aspect ratio of axes is kept inside the specified range. Check out the below examples:\n# Building a scatter plot\rplot \u0026lt;- ggplot(mtcars, aes(mpg, wt)) + geom_point()\r# Setting the ratio to 1\rratio1 \u0026lt;- plot + coord_fixed(ratio = 1)\r# Setting ratio to 10\rratio10 \u0026lt;- plot + coord_fixed(ratio = 3)\r# plotting then in grid\rgrid.arrange(ratio1, ratio10, ncol = 2)\r\r\rSupport for statistical transformation in ggplot Among many useful features of ggplot2, the one which may become dear to you is the support for statistical transformations. These functions save a lot of time as you don\u0026rsquo;t have to prepare the data for it, and the statistical calculations can be done on the go. Again there are multiple statistical functions, and we encourage you to explore them. However, below I have listed some of the most widely used statistical functions.\n1. stat_count - Creates a bar plot showcasing the frequency count of each level of categorical variable.\n# Plotting the bar chart of cylinder counts\rggplot(data = mtcars) +\rstat_count(mapping = aes(x = cyl))\r\r\r2. stat_density() - Creates a kernel density plot. Kernel density estimate is a smoothed version of histogram. A very useful alternative for histogram to plot the histogram.\n# Plotting the bar chart of cylinder counts\rggplot(data = iris) +\rstat_density(mapping = aes(x = Petal.Length))\r\r\r3. stat_summary() - The function summarises the Y Variable for each unique values of X Variable.\n# Plotting the bar chart of cylinder counts\rggplot(data = iris) +\rstat_summary(mapping = aes(x = Species, y = Petal.Length),\rfun.ymin = min,\rfun.ymax = max,\rfun.y = mean)\r\r\r4. stat_smooth() - Adds a smooth line to a scatter plot.\n# Adding smooth line to the scatter plot\rggplot(mpg, aes(displ, hwy)) +\rgeom_point() +\rgeom_smooth()\r\r\rThemes Themes Themes You must have noticed that the default theme for ggplot2 is pretty much greyish in color. If you are not a great fan of grey color, then don\u0026rsquo;t worry. Ggplot2 has a couple of themes for you to choose from.\nlibrary(gridExtra)\rp1 \u0026lt;- ggplot(mpg, aes(displ, hwy)) +\rgeom_point() +\rgeom_smooth() +\rggtitle(\u0026quot;theme_bw\u0026quot;) +\rtheme_bw()\rp2 \u0026lt;- ggplot(mpg, aes(displ, hwy)) +\rgeom_point() +\rgeom_smooth() +\rggtitle(\u0026quot;theme_linedraw\u0026quot;) +\rtheme_linedraw()\rp3 \u0026lt;- ggplot(mpg, aes(displ, hwy)) +\rgeom_point() +\rgeom_smooth() +\rggtitle(\u0026quot;theme_gray\u0026quot;) +\rtheme_gray()\rp4 \u0026lt;- ggplot(mpg, aes(displ, hwy)) +\rgeom_point() +\rgeom_smooth() +\rggtitle(\u0026quot;theme_dark\u0026quot;) +\rtheme_dark()\rp5 \u0026lt;- ggplot(mpg, aes(displ, hwy)) +\rgeom_point() +\rgeom_smooth() +\rggtitle(\u0026quot;theme_minimal\u0026quot;) +\rtheme_minimal()\rp6 \u0026lt;- ggplot(mpg, aes(displ, hwy)) +\rgeom_point() +\rgeom_smooth() +\rggtitle(\u0026quot;theme_void\u0026quot;) +\rtheme_void()\rgrid.arrange(p1,p2,p3,p4,p5,p6, ncol = 3, nrow = 2)\r\r\r"});index.add({'id':2,'href':'/docs/pre-processing-tasks/Identifying-Outliers/','title':"Analysing Outliers",'content':" Outlier is a value that does not follow the usual norms of the data. For almost all the statistical methods, outliers present a particular challenge, and so it becomes crucial to identify and treat them. Let\u0026rsquo;s see which all packages and functions can be used in R to deal with outliers.\n Overview The presence of outliers in the dataset can be a result of an error, or it can be a real value present in the data as a result of the actual distribution of the data. In either case, it is the responsibility of the analyst to identify and treat outlier values.\nUsing tukey formula to identify outlier The tukey formula uses quantiles to produce upper and lower range values beyond which all values are considered as outliers.\nOutlier on the upper side = 3rd Quartile + 1.5 * IQR Outlier on the lower side = 1st Quartile – 1.5 * IQR\nThe same formula is also used in a boxplot. The whiskers on either side represent the upper and lower threshold values.\nUsing histograms to identify outliers In a histogram, values beyond ± 2 standard deviations can be tossed as an outlier. However, this is not a rigid rule, and one can modify this on a need basis.\n# Building a histogram\rhist(iris$Sepal.Width,\rcol = \u0026quot;blue\u0026quot;,\rmain = \u0026quot;Histogram\u0026quot;,\rxlab = \u0026quot;Sepal width of flowers\u0026quot;)\r\r\rUsing boxplot to identify outliers # Building a box plot\rlibrary(ggplot2)\rggplot(iris, aes(x = \u0026quot;\u0026quot;, y=Sepal.Width)) +\rgeom_boxplot(outlier.colour=\u0026quot;red\u0026quot;,\routlier.shape=16,\routlier.size=2, notch=FALSE)\r\r\rIn the above plot, red dots represent the outlier values. We can also take a deeper plunge by looking at the boxplot by flower type. This should help us explore and understand if missing values are coming from a particular group or not.\n# Building a box plot\rlibrary(ggplot2)\rggplot(iris, aes(x = Species, y=Sepal.Width)) +\rgeom_boxplot(outlier.colour=\u0026quot;red\u0026quot;,\routlier.shape=16,\routlier.size=2, notch=FALSE)\r\r\rThere are flowers in Setosa and Virginia categories that have an unusual sepal width. However, Versicolor species of flowers have no outlier.\nUsing cook\u0026rsquo;s distance to identify outliers Cooks Distance is a multivariate method that is used to identify outliers while running a regression analysis. The algorithm tries to capture information about the predictor variables through a distance measure, which is a combination of leverage and each value in the dataset.\nA higher value of cooks distance indicates that the values are outliers. Following rules are used to make a decision using cooks distance.\n A value is considered an outlier if it is three times the mean value. Another rule states that all values which are higher than 4/n (n is the total number of observations) must be investigated.  # Detecting outliers in cars dataset\rmod \u0026lt;- glm(mpg ~ ., data = mtcars)\rmtcars$cooksd \u0026lt;- cooks.distance(mod)\r# Defining outliers based on 4/n criteria\rmtcars$outlier \u0026lt;- ifelse(mtcars$cooksd \u0026lt; 4/nrow(mtcars), \u0026quot;keep\u0026quot;,\u0026quot;delete\u0026quot;)\r# Inspecting the dataset\rmtcars[5:12, ]\r#Output\rmpg cyl disp hp drat wt qsec vs am gear carb cooksd outlier\rHornet Sportabout 18.7 8 360.0 175 3.15 3.44 17.02 0 0 3 2 0.00408313839 keep\rValiant 18.1 6 225.0 105 2.76 3.46 20.22 1 0 3 1 0.03697080496 keep\rDuster 360 14.3 8 360.0 245 3.21 3.57 15.84 0 0 3 4 0.00006907413 keep\rMerc 240D 24.4 4 146.7 62 3.69 3.19 20.00 1 0 4 2 0.03454201049 keep\rMerc 230 22.8 4 140.8 95 3.92 3.15 22.90 1 0 4 2 0.37922064047 delete\rMerc 280 19.2 6 167.6 123 3.92 3.44 18.30 1 0 4 4 0.00428238516 keep\rMerc 280C 17.8 6 167.6 123 3.92 3.44 18.90 1 0 4 4 0.02404481831 keep\rMerc 450SE 16.4 8 275.8 180 3.07 4.07 17.40 0 0 3 3 0.04013728968 keep\rMerc 230 is detected as an outlier by cooks distance\nUsing DBSCAN - A machine learning approach DBSCAN is a method that uses a clustering algorithm to separate dense areas with sparse areas. DBSCAN identifies collective outliers, and thus one should ensure that not more than 5% of values are chosen to be recognized as outliers while running the algorithm.\nlibrary(fpc)\r# Compute DBSCAN using fpc package\rset.seed(86)\rdb \u0026lt;- fpc::dbscan(iris[, -c(5,4,3)], eps = 0.20, MinPts = 3)\r# Plot DBSCAN results\rplot(db, iris[, -c(5,4,3)], main = \u0026quot;DBSCAN\u0026quot;, frame = FALSE)\r\r\rOptimal value of eps eps is the maximum distance between two points, and based upon this distance; the algorithm decides whether a point is an outlier or not. So it becomes essential to identify the optimal eps value in order to make the correct decision. The below code dictated how we can identify optimal value for eps.\nlibrary(dbscan)\rdbscan::kNNdistplot(iris[, -c(5,4,3)], k = 4)\rabline(h = 0.15, lty = 2)\rThe point where you see an elbow like bend corresponds to the optimal eps value or where the dotted line crosses the solid line is considered as optimal.\n\r\rPossible actions towards outliers The possible actions which we can take outliers are mentioned below:\n1. Capping And flooring - We cap every value that is greater or lesser than the tukey formula by the value returned by the tukey method. When you seal the value on the higher side, we call it as capping. The same action for the lower side values is called as flooring. 2. Delete Outliers - Another solution is to delete all the values which are unusual and do not represent the major chunk of the data. 3. Model Outliers - In cases where outliers are a significant percentage of total data, you are advised to separate all the outliers and build a different model for these values.\nIn this chapter, we learned different statistical algorithms and methods which can be used to identify the outliers. These methods included univariate and multivariate techniques. We also learned what possible actions could a data scientist take in case data has outliers. In the next chapter, we will learn how to train linear regression models and validate the same before using it for scoring in R.\n"});index.add({'id':3,'href':'/docs/quick-r-tutorial/introduction-to-r/','title':"Getting Started With R",'content':"Introduction In this quick tutorial, we will cover essential concepts of R programming language. It is not a zero to hero tutorial. However, this course does proffer you with all the necessary information to achieve data analysis related tasks in R.\nInstallation The installation of R software is pretty straight forward and is like any other software. First, you need to download the R software IDE (Interactive Development Environments).\nDownload R\r\rIf you have installed the R software as guided. You can move forward and download the most popular and famous IDE called RStudio.\nWhy R Studio\nRStudio is a collection of integrated tools that will help you to be more productive. The IDE includes a console, syntax-highlighting editor, plotting tools, and supports direct code execution. It also allows one to view history, debug, and manage workspace.\r\rDownload RStudio\r\rNote\nRStudio requires R 3.0.1+.\r\rFrequently asked questions?   Question 1 - What is CRAN ?\nCRAN stands for Comprehensive R Archive Network. This network is a collection of (File Transfer Protocol)FTP, and web servers present around the world.\n  Question 2 - What does this collection of servers comprise of?\nOn these web servers, which are placed all over the world, you can find R source code, documentation, contributed packages, R manuals, and datasets.\n  Question 3 - What is a CRAN mirror?\nEach web server of the network is a mirror.\n  Question 4 - Why do we have so many CRAN mirrors?\nThe idea was to reduce long-distance or international traffic. You can find a list of CRAN mirrors at CRAN mirrors.\n  Question 5 - How can I permanently save information about my preferred CRAN mirror?\nThe information about the CRAN mirror gets saved in a file named .RProfile. This file is present in the user\u0026rsquo;s home directory. For example, one can use the below line of code to add Sweden as your permanent mirror in .RProfile file.\n  options(\u0026quot;repos\u0026quot; = c(CRAN = \u0026quot;https://ftp.acc.umu.se/mirror/CRAN/\u0026quot;))\r  Question 6 - Can I download previous RGui version?\nYes, you can download the source code of older versions of R. Here is the link to the page from where you can download the older versions.\n  Question 7 - How do I update my R software?\nYou can use installr package to update your R software on windows. Use the below code to install the package and update the software.\n  # Downloading the package from CRAN\rinstall.packages(\u0026quot;installr\u0026quot;)\r# Loading the package\rlibrary(installr)\r# Use update function to update.\rupdateR()\r"});index.add({'id':4,'href':'/docs/foundational-algorithms/Linear-Regression/','title':"Linear Regression",'content':" Linear regression is a supervised machine learning algorithm that is used to predict the continuous variable. The algorithm assumes that the relation between the dependent variable(Y) and independent variables(X), is linear and is represented by a line of best fit. In this chapter, we will learn how to execute linear regression in R using some select functions and test its assumptions before we use it for a final prediction on test data.\n Overview - Linear Regression In statistics, linear regression is used to model a relationship between a continuous dependent variable and one or more independent variables. The independent variable can be either categorical or numerical. The case when we have only one independent variable then it is called as simple linear regression. If we have more than one independent variable, then it is called as multivariate regression.\nA mathematical representation of a linear regression model is as give below:\n\r\r\\(\rY = β_0 \u0026#43; β_1*X_1 \u0026#43; β_2*X_2 \u0026#43; β_3*X_3 \u0026#43; ….. \u0026#43; β_n*X_n \u0026#43; error\r\\)\r\rIn the above equation, \r\\(β_0\\)\r\rcoefficient represents intercept and \r\\(β_i\\)\r\rcoefficient represents slope. To help you understand how to implement linear regression, we will be directly diving into a case study.\nIn the below case study, we will be using USA housing data to predict the price. Let us look at the top six observations of USA housing data.\n# Reading data\rhousing \u0026lt;- read.csv(\u0026quot;./static/data/USA_Housing.csv\u0026quot;,\rheader = TRUE, sep = \u0026quot;,\u0026quot;)\r# Print top 6 observations\rhead(housing)\r# Output\rAreaIncome AreaHouse AreaNumberofRooms AreaNumberofBedrooms AreaPopulation Price\r1 79545.46 5.682861 7.009188 4.09 23086.80 1059033.6\r2 79248.64 6.002900 6.730821 3.09 40173.07 1505890.9\r3 61287.07 5.865890 8.512727 5.13 36882.16 1058988.0\r4 63345.24 7.188236 5.586729 3.26 34310.24 1260616.8\r5 59982.20 5.040555 7.839388 4.23 26354.11 630943.5\r6 80175.75 4.988408 6.104512 4.04 26748.43 1068138.1\rGlimpse of Exploratory Data Analysis 1. Checking distribution of target variable - First, you should always try to understand the nature of your target variable. To achieve this, we will be drawing a histogram with a density plot.\nlibrary(ggplot2)\r# Building histogram\rggplot(data=housing, aes(housing$Price)) +\rgeom_histogram(aes(y =..density..), fill = \u0026quot;orange\u0026quot;) +\rgeom_density()\r\r\rIt is good that the price variable follows a normal distribution.\n2. Analyzing Summary Statistics - Here, we will simply create summary statistics for all the variables to understand the behavior of all the independent variables. It will also provide information about missing values or outliers, if any.\n# loading psych package\rlibrary(psych)\rpsych::describe(housing)\r# Output\rvars n mean sd median trimmed mad min max range\rAreaIncome 1 5000 68583.11 10657.99 68804.29 68611.84 10598.27 17796.63 107701.75 89905.12\rAreaHouse 2 5000 5.98 0.99 5.97 5.98 0.99 2.64 9.52 6.87\rAreaNumberofRooms 3 5000 6.99 1.01 7.00 6.99 1.01 3.24 10.76 7.52\rAreaNumberofBedrooms 4 5000 3.98 1.23 4.05 3.92 1.33 2.00 6.50 4.50\rAreaPopulation 5 5000 36163.52 9925.65 36199.41 36112.49 9997.21 172.61 69621.71 69449.10\rPrice 6 5000 1232072.65 353117.63 1232669.38 1232159.69 350330.42 15938.66 2469065.59 2453126.94\rskew kurtosis se\rAreaIncome -0.03 0.04 150.73\rAreaHouse -0.01 -0.09 0.01\rAreaNumberofRooms -0.04 -0.08 0.01\rAreaNumberofBedrooms 0.38 -0.70 0.02\rAreaPopulation 0.05 -0.01 140.37\rPrice 0.00 -0.06 4993.84\r3. Checking Outliers Using Boxplots - As we have mentioned in the chapter [Analysing Outliers](../Identifying Outliers) that any point which lies beyond whispers is called as outliers.\nlibrary(reshape)\rmeltData \u0026lt;- melt(housing)\rp \u0026lt;- ggplot(meltData, aes(factor(variable), value))\rp + geom_boxplot() + facet_wrap(~variable, scale=\u0026quot;free\u0026quot;)\r\r\rApart from Area of Number of Bedrooms all other variables seem to have outliers\n4. Correlation Matrix Visualization - We will use corrgram plot package to visualize and analyse the correlation matrix.\nrequire(corrgram)\rcorrgram(housing, order=TRUE)\r\r\rTraining Regression Model To build a linear regression, we will be using lm() function. The function takes two main arguments. Formula stating the dependent and independent variables separated by ~(tilder). The dataset name. There are other useful arguments and thus would request you to use help(lm) to read more from the documentation.\nDiving data into train and test subsets The housing data is divided into 70:30 split of train and test. The 70:30 split is the most common and is mostly used during the training phase. 70% of the data is used for training, and the rest 30% is for testing how good we were able to learn the data behavior.\nlibrary(caret)\r# Split data into train and test\rindex \u0026lt;- createDataPartition(housing$Price, p = .70, list = FALSE)\rtrain \u0026lt;- housing[index, ]\rtest \u0026lt;- housing[-index, ]\r# Checking the dim of train\rdim(train)\r# Output\r[1] 3500 6\rYou can see we have 70% of the random observations in the training dataset.\nBuilding Model # Taining model\rlmModel \u0026lt;- lm(Price ~ . , data = train)\r# Printing the model object\rprint(lmModel)\r# Output\rCall:\rlm(formula = Price ~ ., data = housing)\rCoefficients:\r(Intercept) AreaIncome AreaHouse -2637299.03 21.58 165637.03 AreaNumberofRooms AreaNumberofBedrooms AreaPopulation 120659.95 1651.14 15.20\rInterpreting Regression Coefficients In the above output, Intercept represents that the minimum value of Price that will be received, if all the variables are constant or absent.\nPlease note - intercept may not always make sense in business terms.\r\rSlope(represented by independent variables) tells us about the rate of change that the Price variable will witness, with every one unit change in the independent variable. For example – if AreaHouse of house increases by one more unit, the Price of the house will increase by 165,637.\nValidating Regression Coefficients and Models We must ensure that the value of each beta coefficient is significant and has not come by chance. In R, the lm function runs a one-sample t-test against each beta coefficient to ensure that they are significant and have not come by chance. Similarly, we need to validate the overall model. Just like a one-sample t-test, lm function also generates three statistics, which help data scientists to validate the model. These statistics include R-Square, Adjusted R-Square, and F-test, also known as global testing.\nTo view these statistics, we need to pass the lmModel object to the summary() function.\n# Checking model statistics\rsummary(lmModel)\r# Output\rCall:\rlm(formula = Price ~ ., data = housing)\rResiduals:\rMin 1Q Median 3Q Max\r-337020 -70236 320 69175 361870\rCoefficients:\rEstimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -2637299.0333 17157.8092 -153.708 \u0026lt;0.0000000000000002 ***\rAreaIncome 21.5780 0.1343 160.656 \u0026lt;0.0000000000000002 ***\rAreaHouse 165637.0269 1443.4130 114.754 \u0026lt;0.0000000000000002 ***\rAreaNumberofRooms 120659.9488 1605.1604 75.170 \u0026lt;0.0000000000000002 ***\rAreaNumberofBedrooms 1651.1391 1308.6712 1.262 0.207 AreaPopulation 15.2007 0.1442 105.393 \u0026lt;0.0000000000000002 ***\r---\rSignif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\rResidual standard error: 101200 on 4994 degrees of freedom\rMultiple R-squared: 0.918, Adjusted R-squared: 0.9179\rF-statistic: 1.119e+04 on 5 and 4994 DF, p-value: \u0026lt; 0.00000000000000022\rIn the above output, Pr(\u0026gt;|t|) represents the p-value, which can be compared against the alpha value of 0.05 to ensure if the corresponding beta coefficient is significant or not. The lm function here lends a helping hand. All values in the output that have (.) period or (*) astric against the variable names indicates that these values are significant. Based upon this, we now know that all variables are statistically significant except AreaNumberofBedrooms.\nFor overall model accuracy, let\u0026rsquo;s discuss statistics generated by lm function one by one.\n1. Multiple R-squared: 0.918 - The R-squared value is formally called a coefficient of determination. Here, 0.918 indicates that the intercept, AreaIncome, AreaHouse, AreaNumberofRooms, and AreaPopulation variables, when put together, are able to explain 91.8% of the variance in the Price variable. The value of R-squared lies between 0 to 1. In practical applications, if the R2 value is higher than 0.70, we consider it a good model.\n2. Adjusted R-squared: 0.9179 - The Adjusted R-squared value tells if the addition of new information ( variable ) brings significant improvement to the model or not. So as of now, this value does not provide much information. However, the increase in the adjusted R-squared value with the addition of a new variable will indicate that the variable is useful and brings significant improvement to the model.\nA large difference between the R-Squared and Adjusted R-squared is not appreciated and generally indicates that multicollinearity exists within the data.\r\r3. F-statistic: 1.119e+04 on 5 and 4994 DF, p-value: \u0026lt; 0.00000000000000022 - This line talks about the global testing of the model. The lm function runs an ANOVA test to check the significance of the overall model. Here the null hypothesis is that the model is not significant, and the alternative is that the model is significant. According to the p-values \u0026lt; 0.05, our model is significant.\nAlbeit, looking at these statistics is enough to take a call on the model significance. But there are other validation methods for linear regression that can be of help while deciding how good or bad the model is. Some of them are mentioned below:\n4. AIC and BIC values - The AIC(Akaike’s information criterion, 1974) and BIC(Bayesian information criterion, 1978) are penalized-likelihood criteria. Both these measures use a \u0026ldquo;measure of fit + complexity penalty\u0026rdquo; to get the final values.\nAIC = - 2 * ln(likelihood) + 2 * p  BIC = - 2 * ln(likelihood) + ln(N) * p\nHere p = number of estimated parameters and N = sample size.\nThe AIC and BIC values can be used for choosing the best predictor subsets in regression and for comparing different models. When comparing different models, the model with minimum AIC and BIC values is considered the best model.\nNote\nAIC is likely to overfit the data, whereas BIC is susceptible to underfit the data.\r\r# Using AIC function\rAIC(lmModel)\r# Using BIC function\rBIC(lmModel) # Output AIC\r[1] 129441.3\r# Output BIC\r[1] 129486.9\r5. Root Mean Square Error(RMSE) - By comparing the RMSE statistics of different models, we can decide which is a better model. The model with the lower RMSE value is considered a better model. There are other similar functions like MAE, MAPE, MSE, and so on that can be used. These functions can be found in Metrics R Package. These functions take majorly two arguments: One is the actual value and second, predicted values. So let\u0026rsquo;s see how we can get these values. The actuals can be 100% found from the original dataset or the training data in our case. However, to find the fitted values, we need to explore the model object.\n# Checking model object for actual and predicted values\rnames(lmModel)\r# Output\r[1] \u0026quot;coefficients\u0026quot; \u0026quot;residuals\u0026quot; [3] \u0026quot;effects\u0026quot; \u0026quot;rank\u0026quot; [5] \u0026quot;fitted.values\u0026quot; \u0026quot;assign\u0026quot; [7] \u0026quot;qr\u0026quot; \u0026quot;df.residual\u0026quot; [9] \u0026quot;xlevels\u0026quot; \u0026quot;call\u0026quot; [11] \u0026quot;terms\u0026quot; \u0026quot;model\u0026quot;\rThe above vector presents the names of the object that constitute the model object. Here, fitted values are the predicted values. Now, we will use these values to generate the rmse values.\nlibrary(Metrics)\rrmse(actual = train$Price, predicted = lmModel$fitted.values)\r# Output\r[1] 493370.4\rChecking Assumptions of Linear Regression Linear regression is parametric, which means the algorithm makes some assumptions about the data. A linear regression model is only deemed fit is these assumptions are met. There are about four assumptions and are mentioned below. If the model fails to meet these assumptions, then we simply cannot use this model.\n1. Errors should follow normal distribution - This can be checked by drawing a histogram of residuals or by using plot() function. The plot function creates 4 different charts. One of which is an NPP plot. The chart confirms if the errors follow a normal distribution or not.\nGenerating histogram # Histogram to check the distribution of errors\rhist(lmModel$residuals, color = \u0026quot;grey\u0026quot;)\r![Histogram to check linear regression assumption](/images/regression/normalerror1.jpeg \"Histogram - errors are normally distributed\")\rThe above histogram of errors clearly states that errors are normally distributed.\nGenerating NPP plot We except the points to be very close to the dotted line in an NPP plot. Points being close to the line means that errors follow a normal distribution.\nplot(lmModel)\r![npp plot to check linear regression assumptions](/images/regression/nppplot.jpeg \"npp plot - errors are normally distributed\")\r2. There should be no heteroscedasticity - This means that the variance of error terms should be constant. We shall not see any patterns when we draw a plot between residuals and fitted values. And the mean line should be close to Zero.\nGenerating the scatterplot between residuals and fitted values # Using plot function\rplot(lmModel)\r\r\rA straight red line closer to the zero value represents that we do not have heteroscedasticity problem in our data.\n3. There should be no multicollinearity - The linear model assumes that the predictor variables do not correlate with each other. If they exhibit high correlation, it is a problem and is called multicollinearity. A variation inflation factor test can help check for the multicollinearity assumption.\nVIF = 1/(1-R2)\nThe R implementation of the below function can be found here.\nVIF is an iterative process. The function will remove one variable at a time, which is cause for multicollinearity and repeats the process until all problem causing variables are removed. So, finally, we are left with the list of variables that have no or very weak correlation between them.\r\rvif_func(housing[, 1:5])\rvar vif AreaIncome 1.00115868968647\rAreaHouse 1.00057658981485\rAreaNumberofRooms 1.27353508823836\rAreaNumberofBedrooms 1.27441273719468\rAreaPopulation 1.00126579728799\rAll variables have VIF \u0026lt; 10, max VIF 1.27\r[1] \u0026quot;AreaIncome\u0026quot; [2] \u0026quot;AreaHouse\u0026quot; [3] \u0026quot;AreaNumberofRooms\u0026quot; [4] \u0026quot;AreaNumberofBedrooms\u0026quot;\r[5] \u0026quot;AreaPopulation\u0026quot; There is no multicollinearity problem in the dataset. Generally, VIF values which are greater than 5 or 7 are the cause of multicollinearity.\n3. There should be no auto serial correlation - The autocorrelation means that error terms should not be correlated with each other. To check this, we can run the Durbin-Watson test(dw test). The test returns a value between 0 and 4. If the value is two, we say there is no auto serial correlation. However, a value higher than 2 represents (-) ve correlation and value lower than 2 represents (+) ve correlation.\nlibrary(\u0026quot;lmtest\u0026quot;)\rdwtest(lmModel)\r# Output\rDurbin-Watson test\rdata: lmModel\rDW = 1.9867, p-value = 0.3477\ralternative hypothesis: true autocorrelation is greater than 0\rWe got a value of 1.9867 which suggests that there is no auto serial correlation.\nOur model met all the four assumptions of linear regression.\r\rPredicting Dependent Variable(Y) in Test Dataset We test the model performance on test data set to ensure that our model is stable, and we get the same or closer enough results to use this trained model to predict and forecast future values of dependent variables. To predict, we use predict function, and then we generate R-Squared value to see if we get the same result as we got in the training dataset or not.\n# Predicting Price in test dataset\rtest$PreditedPrice \u0026lt;- predict(lmModel, test)\r# Priting top 6 rows of actual and predited price\rhead(test[ , c(\u0026quot;Price\u0026quot;, \u0026quot;PreditedPrice\u0026quot;)])\r# Output\rPrice PreditedPrice\r2 1505890.9 1494997.3\r3 1058988.0 1253539.2\r8 1573936.6 1572965.3\r12 663732.4 629153.8\r23 718887.2 727497.8\r24 743999.8 991034.7\rGenerating R-Squared Value for the test dataset We are using a user-defined formula to generate the R-Squared value here.\nactual \u0026lt;- test$Price\rpreds \u0026lt;- test$PreditedPrice\rrss \u0026lt;- sum((preds - actual) ^ 2)\rtss \u0026lt;- sum((actual - mean(actual)) ^ 2)\rrsq \u0026lt;- 1 - rss/tss\rrsq\r# Output\r[1] 0.9140436\rOur model is performing fantastic.  In test dataset, we got an accuracy of 0.9140436 and training data set, we got an accuracy of 0.918.\nIn this chapter, We learned many things related to linear regression from a practical and theoretical point of view. We learned when to use linear regression, how to use it, how to check the assumptions of linear regression, how to predict the target variable in test dataset using trained model object, and we also learned how to validate the linear regression model using different statistical methods. In the next chapter, we will learn about an advanced linear regression model called ridge regression.\n"});index.add({'id':5,'href':'/docs/subscribe/','title':"Subscribe",'content':"Loading…\r"});index.add({'id':6,'href':'/docs/text-mining/word-document-analysis/','title':"Word and Document Analysis",'content':"Overview In the real world, the use of text analytics is growing at a swift pace. Organizations across the globe have started to realize that textual the analysis of textual data can reveal significant insights that can help with decision making. As of today, the text analytics field is seeing exponential growth and will continue to see this high growth rate, even for the coming years. The increase in text analysis use cases can be attributed to the continuous growth of unstructured data both within and outside of the organization.\nIn this section, we will learn how to figure out and quantify what a document is all about through the use of term-frequency, inverse document frequency, and a technique called tf-idf(combination of tf and idf). We will generate all these statistics using tidytext package in R.\nFor example, we are using text from Jane Austen\u0026rsquo;s published novels. The package janeaustenr in R provides a collection of 6 different novels by Jane formatted in a convenient form for text analysis. Let\u0026rsquo;s print the total number of lines by each book to get an idea about the length of each document.\nFor data manipulations, we are uing dplyr package.\nlibrary(tidytext)\rlibrary(dplyr)\rausten_books() %\u0026gt;% group_by(book) %\u0026gt;%\rsummarise(total_lines = n())\r# A tibble: 6 x 2\rbook total_lines\r\u0026lt;fct\u0026gt; \u0026lt;int\u0026gt;\r1 Sense \u0026amp; Sensibility 12624\r2 Pride \u0026amp; Prejudice 13030\r3 Mansfield Park 15349\r4 Emma 16235\r5 Northanger Abbey 7856\r6 Persuasion 8328\rTerm frequency(tf) If you think about a text document, it is nothing but a collection of sentences, and sentences are eventually a collection of words. So for a document, its features are the words. That means a similar set of documents will have related terms. Thus, the frequency of a term is a good measure of how important the word is to a given text. This method is called as term frequency.\nIt is no surprise that there are going to be words that occur many times but are not relevant; these are words like \u0026ldquo;is\u0026rdquo;, \u0026ldquo;the\u0026rdquo;, \u0026ldquo;that\u0026rdquo;, \u0026ldquo;this\u0026rdquo;, \u0026ldquo;of\u0026rdquo;, and so forth. It is thus vital to clean the text before getting the term-frequency of a document. To learn more about the text cleaning process and steps read the Text Pre-Processing chater.\nLet us check the frequency of most common words used by Jane Austeneach throughout the novels. Here we unnest_tokens() function converts the text into tokens using tokenizers package. It also coverts the text to lower by default. We also remove stopwords using anti_join() from dplyr package. The object stop_words is present in tidytext and contains about 1400+ stopwords. The count function counts the occurrence of each word by a novel.\nIn the next chunk of code, we are also looking at a total number of words by each book. This is achieved by using group_by() and summarise() functions. Finally, we join the two tables using left_join().\nlibrary(tidytext)\rlibrary(dplyr)\rword_frequency \u0026lt;- austen_books() %\u0026gt;%\runnest_tokens(word, text) %\u0026gt;%\ranti_join(stop_words)%\u0026gt;%\rcount(book, word, sort = TRUE)\rtotal_words_per_book \u0026lt;- word_frequency %\u0026gt;%\rgroup_by(book) %\u0026gt;%\rsummarise(total_words = sum(n))\rword_frequency \u0026lt;- left_join(word_frequency, total_words_per_book)\rword_frequency\r# A tibble: 37,225 x 4\rbook word n total_words\r\u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r1 Mansfield Park fanny 816 47968\r2 Emma emma 786 46775\r3 Sense \u0026amp; Sensibility elinor 623 36330\r4 Emma miss 599 46775\r5 Pride \u0026amp; Prejudice elizabeth 597 37246\r6 Mansfield Park crawford 493 47968\r7 Sense \u0026amp; Sensibility marianne 492 36330\r8 Persuasion anne 447 25488\r9 Mansfield Park miss 432 47968\r10 Northanger Abbey catherine 428 23803\r# ... with 37,215 more rows\rNow, let\u0026rsquo;s calculate term frequency by dividing n/total_words for each novel.\nfreq_by_rank \u0026lt;- word_frequency %\u0026gt;%\rgroup_by(book) %\u0026gt;%\rmutate(rank = row_number(),\r`term_frequency` = n/total_words)\r# A tibble: 37,225 x 6\r# Groups: book [6]\rbook word n total_words rank term_frequency\r\u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r1 Mansfiel~ fanny 816 47968 1 0.0170\r2 Emma emma 786 46775 1 0.0168\r3 Sense \u0026amp; ~ elinor 623 36330 1 0.0171\r4 Emma miss 599 46775 2 0.0128\r5 Pride \u0026amp; ~ eliza~ 597 37246 1 0.0160\r6 Mansfiel~ crawf~ 493 47968 2 0.0103\r7 Sense \u0026amp; ~ maria~ 492 36330 2 0.0135\r8 Persuasi~ anne 447 25488 1 0.0175\r9 Mansfiel~ miss 432 47968 3 0.00901\r10 Northang~ cathe~ 428 23803 1 0.0180\r# ... with 37,215 more rows\rInverse document frequency(idf) Inverse document frequency decreases the weight of words that are common and increases the weight of words, which are more important to the document. Here the frequency of each term in a document is divided by the document frequency of that term. In other words, idf is a measure of whether a term is common or rare to the given document.\n\r\r\\[\r{idf_{term}} = \\ln(\\frac {N_{documnets}} {N_{documents\\space{containing}\\space{terms}}} )\r\\]\r\rTerm frequency-inverse document frequecny(tf-idf) The intention behind tf-idf statistic is to measure the importance of a word in a collection of documents. tf-idf is calculated using the following formula.\n\r\\[\rtfidf(t,d,D) = tf(t, d) * idf(t, D)\r\\]\r\rHere, \r\\[`t` \\dashrightarrow term\\]\r\r\r\\[`d` \\dashrightarrow document\\]\r\r\r\\[`D` \\dashrightarrow total\\space{number}\\space{documents}\\space{in}\\space{collection}\\]\r\rNote\nIn tidytext idf and tf-idf can be calculated using bind_tf_idf() function.\r\rword_frequency \u0026lt;- austen_books() %\u0026gt;%\runnest_tokens(word, text) %\u0026gt;%\ranti_join(stop_words)%\u0026gt;%\rcount(book, word, sort = TRUE)\rword_tf_idf \u0026lt;- word_frequency %\u0026gt;%\rbind_tf_idf(word, book, n)\rword_tf_idf\r# A tibble: 37,225 x 6\rbook word n tf idf tf_idf\r\u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r1 Mansfield Park fanny 816 0.0170 0.693 0.0118\r2 Emma emma 786 0.0168 1.10 0.0185\r3 Sense \u0026amp; Sensibility elinor 623 0.0171 1.79 0.0307\r4 Emma miss 599 0.0128 0 0 5 Pride \u0026amp; Prejudice elizabeth 597 0.0160 0.693 0.0111\r6 Mansfield Park crawford 493 0.0103 1.79 0.0184\r7 Sense \u0026amp; Sensibility marianne 492 0.0135 1.79 0.0243\r8 Persuasion anne 447 0.0175 0.182 0.00320\r9 Mansfield Park miss 432 0.00901 0 0 10 Northanger Abbey catherine 428 0.0180 0.693 0.0125\rVisualizing high tf-idf words To visualize the plot, we will use ggplot2 package in R.\nlibrary(ggplot2)\rtf_idf_plot \u0026lt;- word_tf_idf %\u0026gt;%\rarrange(desc(tf_idf)) %\u0026gt;%\rmutate(word = factor(word, levels = rev(unique(word)))) %\u0026gt;%\rgroup_by(book) %\u0026gt;%\rtop_n(10) %\u0026gt;%\rungroup() %\u0026gt;%\rggplot(aes(word, tf_idf, fill = book)) +\rgeom_col(show.legend = FALSE) +\rlabs(x = NULL, y = \u0026quot;tf-idf\u0026quot;) +\rfacet_wrap(~book, ncol = 3, scales = \u0026quot;free\u0026quot;) +\rcoord_flip()\r\r\rBonus - Convert above static chart to interactive chart You can use ggplotly() function to convert any ggplot2 chart to an interactive chart as given blow:\nlibrary(plotly)\rggplotly(tf_idf_plot)\rImportant Notice\nStay updated by subscribing to our newsletter.\nWe hate spam just like you.\r\rSubscribe\r\r"});index.add({'id':7,'href':'/docs/useful-r-packages/datatable-tutorial-data-manipulation/','title':"data.table",'content':"Overview If you are looking for fast execution of your code on large datasets, then you must read through this tutorial. Data manipulation tasks such as aggregations, add/remove/update of columns, joins, reading large files, etc., are all very important for any data science-related project. Keeping all these operations into mind, Matt Dowle and Arun Shrinivasan created a package called data.table.\ndata.table package is an extension of data.frame package in R. It is one of the first choices for data scientists while they work on large datasets. Some of the notable features which makes the data.table package popular are:-\n Irrespective of what set of operations you like to perform the data.table offers a concise and consistent syntax. It automatically provides fast primary and secondary indexing of rows. The package is capable of automatically optimizing the internal operations, leading to fast and memory-efficient code. Especially, tasks like join and group by.  So, if you are looking to reduce the execution and programming time of your r code, then this package named data.table is for you.\nReading large datasets Generally, the performance of R programming is not up to the mark when it comes to working with large datasets as everything is loaded into the RAM. As part of the solution, the data.table package was designed. Lets us see how you can load large datasets using a fread() function from this package.\nBelow is a code using which we will upload a file containing 22,489,348 rows and 11 columns. The dataset we are using here is UK Housing prices paid. It is an extensive collection of records of all the individual transactions in England and Wales since 1995.\n# Reading a large file using fread() function\rukHousing \u0026lt;- fread(\u0026quot;price_paid_records.csv\u0026quot;)\rWhen you read a file using fread() function from data.table package the function loads it as a data table object. We will talk about object type in the coming sessions. For now, it is important to note that if an object is not in a data table format, then you must convert it in order to take advantage of data.table package.\n Apart from reading the csv and text files fread() can also accepts http and https URLs as input. Unlike read.csv(), the columns of character data types are not converted to factors by default while reading the data file. data.table never uses row names. To visually separate the row numbers from the first columns, row numbers are printed with a colon(:).  Below is the comparison of time taken by at least three different functions in R:\n# Reading a large file using fread() function\rsystem.time(fread(\u0026quot;price_paid_records.csv\u0026quot;))\r# Time difference of 2.711095 mins\r# reading a large file using read.csv() function\rsystem.time(read.csv(\u0026quot;price_paid_records.csv\u0026quot;))\r#Time difference of 22.23424 mins\r\u0026gt; system.time(fread(\u0026quot;price_paid_records.csv\u0026quot;))\ruser system elapsed\r51.04 3.61 40.63\r\u0026gt; system.time(read.csv(\u0026quot;price_paid_records.csv\u0026quot;))\ruser system elapsed\r1053.73 9.36 1073.19\rQuerying A Data Table Mostly when we use DT(short form for data table object), we refer to it as \u0026ldquo;querying DT\u0026rdquo;. DT is designed to do a lot more than just subjecting the data frames by row and columns. To understand the querying bit, we need to look and understand the general form of data.table syntax. The syntax is given below:\nDT[i, j, by]\rThe above syntax can be compared to an SQL query. Here i inside the square [] brackets is an equivalent for WHERE or ORDER BY. Similarly, j represents SELECT or UPDATE, and finally, by represents the GROUP BY from the SQL. Individuals who are familiar with SQL will now understand why it is referred to as \u0026ldquo;querying DT\u0026rdquo;. The above statement can also be read as subset/reorder rows using i, then calculate j, grouped by.\nYou can use as.data.table() or simply data.table() and setDT() functions to convert any regular R data frame to a data table.\n# Using as.data.table() function\rdataTableIris = as.data.table(iris)\r# Printing top 6 rows\rhead(dataTableIris)\r Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r1: 5.1 3.5 1.4 0.2 setosa\r2: 4.9 3.0 1.4 0.2 setosa\r3: 4.7 3.2 1.3 0.2 setosa\r4: 4.6 3.1 1.5 0.2 setosa\r5: 5.0 3.6 1.4 0.2 setosa\r6: 5.4 3.9 1.7 0.4 setosa\rNotice that a data.table prints the row numbers with a colon. This is done to visually separate row numbers from the first column. By using any of the above two functions, one can easily convert existing data.frame object to a data.table object.\nData manipulation - Common Tasks In data.table package columns are referred to as if they are variables, much like SQL. This means that when we pass column names for extracting the information from a data.table object, we need not add data table names as prefix(e.g. matcars$ ). Nevertheless, using mtcars$mpg and mtcars$cyl would work just fine. Below we have provided the list of some of the everyday tasks related to data manipulation.\nGet first 10 rows of Uk Housing data # Getting the top 10 rows\rukHousing[1:10]\rWe passed row indices in the i, and as there are no conditions, the code returns the top 10 rows from the row indices. You can also use a head() to get the top 10 rows.\nSorting data in data.table 1.Sorting data by single variable - For example, sorting house price in descending order - This task can be achieved using multiple functions. We can use either order() from {base} package or setorder() function from {data.table} package to achieve this task.\n# Sorting data by using order()\rsort1 \u0026lt;- ukHousing[order(-Price)] # (-)ve sign is used to get data in descending order\rhead(sort1)\r# Sorting data by using setorder()\rsort2 \u0026lt;- setorder(ukHousing, -Price) # For Ascending order remove (-)ve sign\rhead(sort2)\r2.Sorting data by multiple variables - For example, sort house price data by duration and price. To add various variables, just pass column names separated by comma(,) in both the functions.\n# Sorting data by using order()\rsort1 \u0026lt;- ukHousing[order(Duration, -Price, )]\rhead(sort1)\r# Sorting data by using setorder()\rsort1 \u0026lt;- setorder(ukHousing, Duration, -Price)\rSubsetting data by column(s) in data.table using j As mentioned earlier, we can directly use column(s) names for sub-setting in data.table. Also, if you want all the rows, then you can skip i as a section.\n1.Select single column - When we select a single column, the output returned is a vector. In case you wish to keep the structure as data.table, you need to wrap the variable name within list() function. If you find list() to boring you can instead use .(), it is an alias list() in data.table package. Most people prefer using .() instead of list() and going forward we will be continue to use .(), hereafter.\n# Subsetting data.table by selecting one column\rsingleColumn \u0026lt;- ukHousing[ , County]\rhead(singleColumn)\r# Output\r[1] \u0026quot;GREATER MANCHESTER\u0026quot; \u0026quot;THURROCK\u0026quot; \u0026quot;SOMERSET\u0026quot; [4] \u0026quot;BEDFORDSHIRE\u0026quot; \u0026quot;WEST YORKSHIRE\u0026quot; \u0026quot;WILTSHIRE\u0026quot;\r# Sub setting data.table by selecting one column keeping structure as data.table\rsingleColumn \u0026lt;- ukHousing[ , list(County)]\rhead(singleColumn)\r# Output\rCounty\r1: GREATER MANCHESTER\r2: THURROCK\r3: SOMERSET\r4: BEDFORDSHIRE\r5: WEST YORKSHIRE\r6: WILTSHIRE\r2.Select multiple columns - This task is easy, and all we need to do is pass the column names separated by comma(,) inside the .().\nYou must pass the column names inside list alias to avoid the second column going into by section.\n# Subsetting data.table by selecting one column\rmultiColumn \u0026lt;- ukHousing[ , .(County, Price)]\rhead(multiColumn)\r# Output\rCounty Price\r1: GREATER MANCHESTER 25000\r2: THURROCK 42500\r3: SOMERSET 45000\r4: BEDFORDSHIRE 43150\r5: WEST YORKSHIRE 18899\r--- 22489344: WEST YORKSHIRE 175000\r22489345: WEST YORKSHIRE 586945\r22489346: WEST YORKSHIRE 274000\r22489347: WEST YORKSHIRE 36000\r22489348: WEST YORKSHIRE 145000\r3.Creating new columns using expressions - You can use := operator to create new variables by applying operations on rows. In the below example, we are creating a new variable c, which is a sum of a and b variable. This is also called as Assignment by reference.\ndf \u0026lt;- data.table(a = c(1,2,3), b = c(4,5,6))\rdf[, c := a + b]\r# Output\ra b c\r1: 1 4 5\r2: 2 5 7\r3: 3 6 9\rRename column in data.table As column names n data.table is passed inside list, we can rename columns as we would do while creating list. In below renaming country and price columns to A_Country and A_Price.\n# Renaming Columns in data.table\rrenameColumn \u0026lt;- ukHousing[ , .(A_Country = County, A_Price = Price)]\rcolnames(renameColumn)\r# Output\r[1] \u0026quot;A_Country\u0026quot; \u0026quot;A_Price\u0026quot;\rUsing expressions with columns in data.table The in j in data.table can handle much more than just selecting columns, and it can also be used for computing on columns, also referred to as using expressions. Let\u0026rsquo;s say; you want to combine country and district variables separated by \u0026ldquo;_\u0026quot;.\n# concatinating two Columns in data.table\rexpressionExample1 \u0026lt;- ukHousing[ , .(paste(County, District, sep = \u0026quot;_\u0026quot;))]\rhead(expressionExample1)\r# Output\rV1\r1: GREATER MANCHESTER_OLDHAM\r2: THURROCK_THURROCK\r3: SOMERSET_SEDGEMOOR\r4: BEDFORDSHIRE_NORTH BEDFORDSHIRE\r5: WEST YORKSHIRE_LEEDS\r6: WILTSHIRE_SALISBURY\rSub setting data using i and applying expressions using j While working with data.table you can subject the data using i IE identify rows on which you want to perform your further data analysis. It could also be phrased as subset in i and do in j.\n1.Calculate and compare the average price of houses in OLDHAM District\nIn the below code, we first subset the data and get row where District is \u0026ldquo;OLDHAM\u0026rdquo;. We then take these rows and apply the mean function in the j part of the syntax.\n# Avg house prices in OLDHAM\rans \u0026lt;- ukHousing[ District == \u0026quot;OLDHAM\u0026quot;, .(Avg_Price = mean(Price))]\rans\r# Output\rAvg_Price\r1: 91258.15\r2.How many houses do we have in OLDHAM\nHere we need to count the total number of rows in the subject. To achieve this, we can use two different functions. One we can use length() and another we can use is .N() function. Please note, length() will require us to pass the argument. This could be any variable name. However, .N does not require a function to be passed.\n# Total number of rows where Dristrict is OLDHAM\rans \u0026lt;- ukHousing[ District == \u0026quot;OLDHAM\u0026quot;, length(District)]\rans\r# Output\r[1] 76576\r# Total number of rows where Dristrict is OLDHAM\rans \u0026lt;- ukHousing[ District == \u0026quot;OLDHAM\u0026quot;, .N]\rans\r# Output\r[1] 76576\rThe special symbol .N is a built-in variable that saves the total number of observations in the current group. In the next section, we will see how we can combine .N with the by. But before we move ahead, let us see how to refer to columns by name in j.\nReferring columns by name in j (just like data.frame) If you are explicitly calling out the names, then you can follow the data.frame way of calling the column names. However, if you have stored the column names in a vector than there are two options -\nA. Using .. prefix - The .. prefix requests the data.table to look for the selected colNames by going \u0026ldquo;up-one-level,\u0026rdquo; I.E., in the global environment. If you are familiar with the Unix terminal, you should be able to connect to the .. command, which also means \u0026ldquo;up-on-level\u0026rdquo;.\n# Calling multimple column names using `..` prefix\rcolNames = c(\u0026quot;Duration\u0026quot;, \u0026quot;County\u0026quot;, \u0026quot;Price\u0026quot;)\rhead(ukHousing[ , ..colNames])\r# Output\rDuration County Price\r1: F GREATER MANCHESTER 25000\r2: F THURROCK 42500\r3: F SOMERSET 45000\r4: F BEDFORDSHIRE 43150\r5: F WEST YORKSHIRE 18899\r6: F WILTSHIRE 81750\rB. **Using with argument ** - By default the with argument is set as TRUE. That allows data.table to refer to the columns as variables. Setting the argument with = FALSE actually disables this property thereby restoring the data.frame mode.\n# Calling multimple column names using `with` argumnet\rcolNames = c(\u0026quot;Duration\u0026quot;, \u0026quot;County\u0026quot;, \u0026quot;Price\u0026quot;)\rhead(ukHousing[ , colNames, with = FALSE])\r# Output\rDuration County Price\r1: F GREATER MANCHESTER 25000\r2: F THURROCK 42500\r3: F SOMERSET 45000\r4: F BEDFORDSHIRE 43150\r5: F WEST YORKSHIRE 18899\r6: F WILTSHIRE 81750\rTO DESELECT THE COLUMNS YOU CAN USE ! or -.\nHow to summarize a larger list of variables To summarise a large list of variables in data.table you can use .SD and .SDCols operators. Here SD stands for subset of data.\nIn the below code snippet, we will see how to get the following:\n1.Mean of multiple variables\nukHousing[, lapply(.SD, mean), .SDcols = c(\u0026quot;Price\u0026quot;, \u0026quot;var2\u0026quot;)]\r2.Getting summary statistics of all the numeric variable\nukHousing[, lapply(.SD, mean)]\r3.using UDF for calculating different statistics\nukHousing[, lapply(.SD, function(x) c(mean = median(x), mode(x)))]\rAggregations using by in data.table We have explored how to usei as WHERE or ORDER BY. We also learned how to use SELECT or UPDATE as part of j. Next, we will learn how to combine these two with by to perform data operations by groups.\nHow to get number of rows by District The below code will return the top 5 and below 5 results. Also, note if you do not pass the name of the column in j it will be named as N.\nbyExample1 \u0026lt;- ukHousing[, .(Count = .N), by = .(District)]\rbyExample1\r# Output\rDistrict Count\r1: OLDHAM 76576\r2: THURROCK 69498\r3: SEDGEMOOR 52762\r4: NORTH BEDFORDSHIRE 2801\r5: LEEDS 299133\r--- 450: CITY OF DERBY 87152\r451: RADNOR 13\r452: BRYHER 1\r453: ISLES OF SCILLY 397\r454: ST MARTIN'S 2\rCompare average prices of old and new houses in LEEDS District To calculate this, we need to do the following things:\n Take the subset of data WHERE District == \u0026ldquo;LEEDS\u0026rdquo; - This will go in i Take Average of Price variable and rename it - This will go in j Finally, a group by on the Old/New Variable - This will go in by  byExample2 \u0026lt;- ukHousing[District == \u0026quot;LEEDS\u0026quot;,\r.(Average_House_Price = mean(Price)),\rby = .(`Old/New`)]\rbyExample2\r# Output\rOld/New Average_House_Price\r1: N 131776.4\r2: Y 152065.4\rYOU CAN ALSO PASS MULTIPLE COLUMNS SEPARATED BY (,) IN by.\nbyExample3 \u0026lt;- ukHousing[District == \u0026quot;LEEDS\u0026quot;,\r.(Average_House_Price = mean(Price)),\rby = .(`Old/New`, Duration)]\r# Outlook\rOld/New Duration Average_House_Price\r1: N F 133247.2\r2: N L 120133.6\r3: Y F 164630.3\r4: Y L 136957.9\r5: Y U 98157.0\r6: N U 56837.5\rHow to get ordered results by grouping variable in data.table Although data.table retains the original order of groups by design, at times, you may be required to sort the values by each group. One may be interested in doing so to understand top performers in each group. You can achieve this by changing by to keyby. It will automatically update the values within each group in the ascending order. Below code results can be compared with the previous one, and you will notice that it is not ordered in ascending order.\nkeyby also sets a key after ordering the values by setting an attribute called sorted.\nbyExample3 \u0026lt;- ukHousing[District == \u0026quot;LEEDS\u0026quot;,\r.(Average_House_Price = mean(Price)),\rkeyby = .(`Old/New`, Duration)]\rbyExample3\r# Output\rOld/New Duration Average_House_Price\r1: N F 133247.2\r2: N L 120133.6\r3: N U 56837.5\r4: Y F 164630.3\r5: Y L 136957.9\r6: Y U 98157.0\rHow to write Sub Queries like SQL using Chaining operation Chaining is a process using which we can avoid intermediate assignments of temporary variables. Here the results of the previous operation are passed directly to the next one.\nIn the below code, I will order the output of the previous code by Average_House_Price variable but in descending order.\nbyExample4 \u0026lt;- ukHousing[District == \u0026quot;LEEDS\u0026quot;,\r.(Average_House_Price = mean(Price)),\rkeyby = .(`Old/New`, Duration)][order(-Average_House_Price)]\rbyExample4\r# Output\rOld/New Duration Average_House_Price\r1: Y F 164630.3\r2: Y L 136957.9\r3: N F 133247.2\r4: N L 120133.6\r5: Y U 98157.0\r6: N U 56837.5\rUsing data.table for joining large tables Merging tables in data.table is very similar to data.frame. You can merge tables using merge() function. However, in data.table The merging is done based on the common key variable as a primary key. On the other hand, data.frame takes a common variable as a primary key to merge.\nFor this exercise, we will first define two tables, as given below.\n# Defining the tables\rdata.table1 \u0026lt;- data.table(primaryKey = letters[rep(1:10)], X= 1:10, key = \u0026quot;primaryKey\u0026quot;)\rdata.table2 \u0026lt;- data.table(primaryKey = letters[rep(2:8)], Y= 10:1, key = \u0026quot;primaryKey\u0026quot;)\r# Printing the output\r\u0026gt; data.table1\rA X\r1: a 1\r2: b 2\r3: c 3\r4: d 4\r5: e 5\r6: f 6\r7: g 7\r8: h 8\r9: i 9\r10: j 10\r\u0026gt; data.table2\rA Y\r1: b 10\r2: b 3\r3: c 9\r4: c 2\r5: d 8\r6: d 1\r7: e 7\r8: f 6\r9: g 5\r10: h 4\rWe will now look at how to get the common elements between the two by doing inner join.\nmerge(data.table1, data.table2, by = \u0026quot;primaryKey\u0026quot;)\r# Output\rprimaryKey X Y\r1: b 2 10\r2: b 2 3\r3: c 3 9\r4: c 3 2\r5: d 4 8\r6: d 4 1\r7: e 5 7\r8: f 6 6\r9: g 7 5\r10: h 8 4\rTo perform Left Join, we need to pass all.x = TRUE argument. Below are the code snippet and its output.\nmerge(data.table1, data.table2, by = \u0026quot;primaryKey\u0026quot;, all.x = TRUE)\rprimaryKey X Y\r1: a 1 NA\r2: b 2 10\r3: b 2 3\r4: c 3 9\r5: c 3 2\r6: d 4 8\r7: d 4 1\r8: e 5 7\r9: f 6 6\r10: g 7 5\r11: h 8 4\r12: i 9 NA\r13: j 10 NA\rI am sure with this example you will be able to figure out how to perform other joins like Right Join, Full Join, and other joins. If not please refer the documentation of data.table package.\nMust-know functions from data.table package Below is the list of some must-know functions from data.table package. For this section, we will not be sharing the output.\nHow to determine duplicated rows in data.table There are a couple of functions that we can use from data.table to deal with duplicated rows. The use of a particular function depends on the task you want to achieve.\n1.unique() - the function returns a data table with all the duplicated rows removed. The function also can be used to specify a particular column by which you wish to check for duplicated values.\n# Removing duplicates considering all variables\runique(df)\r# Removing duplicates considering a particular variable(s)\runique(df, by = \u0026quot;var1\u0026quot;)\r2.duplicated() - The output of this function is a logical vector indicating which rows are duplicates.\n# getting logical vectors indicating duplicate rows\rduplicated(df)\r3.anyDuplicated() - The function is similar to the duplicated() function. The only difference is that this function returns inter values as output. It returns index i of the first duplicated entry if there is one, and 0 otherwise.\nanyDuplicated(df, by=c(\u0026quot;Var1\u0026quot;))\rHow to define range in data.table between() function can be used to define a range. The range generated includes values of both start and end values. You can use this function to check if the values of a variable lie between a certain range of values.\n# Getting houses prices where prices range between 100k and 200k\rukHousing(Price %between% c(100000, 200000))\rHow to reshape large datasets faster in data.table dcast.data.table and melt.data.table are two functions which provides a very fast version of reshape2:dcast() and reshape2:melt. These functions can handle very large datasets quite efficiently and are also able to manage memory quite efficiently in comparison to functions from reshape2 package.\ndcast(df, time ~ variable, fun=mean)\rmelt(df, id=1:2, measure=\u0026quot;f_1\u0026quot;)\rHow to rank large datasets using data.table data.table provides a function named frank() to achieve faster ranking over the large datasets. The function is similar to base rank() function, but performance-wise, it is much faster. You can think of this function as a translation of RANK OVER PARTITION windows function in SQL.\nThe function is capable of accepting vectors, lists, data.frames, or data.tables as input.\nx = c(4, 1, 4, NA, 1, NA, 4)\rdf = data.table(x, y=c(1, 1, 1, 0, NA, 0, 2))\rfrank(df, cols=\u0026quot;x\u0026quot;)\rSome essential set of functions The set functions consists of union, intersect, setdiff and setequal. These functions can be very crucial when working with multiple datasets. The data.table package in R provides a set of these functions which perform these tasks at a super-fast speed when it comes to large datasets.\n fintersect will return copies of common rows. fsetdiff will return copies of rows that are not common. funion will return copies of all the rows. fsetequal will return FALSE unless all the rows are similar.  How to write large datasets to the local You can use fwrite() function to quickly write the larger file back to your local system.\nHow to generate lead/lag values for time series with data.table To generate lead/lag values, the data.table provides shift() function.\n# Shifting data by 1 lag\rdf \u0026lt;- data.table(x = seq(1, 10, 2))\rdf[, x1:= shift(x, 1, type = \u0026quot;lag\u0026quot;)]\r# Output\r\u0026gt; df\rx x1\r1: 1 NA\r2: 3 1\r3: 5 3\r4: 7 5\r5: 9 7\rTo get the leading values in the next column, all you need to do is mention type = \u0026ldquo;lead\u0026rdquo;. I encourage you to try it on your local machine.\nIn this chapter, we learned how to use data.table to deal with very large datasets with efficient memory utilization. This package provides an excellent solution for data wrangling tasks in R. In the next tutorial, we will talk about dplyr package. We understand that data.table can become a bit complicated, and for simplicity, some prefer using dplyr package.\n"});index.add({'id':8,'href':'/docs/foundational-algorithms/Ridge-Regression/','title':"Ridge Regression",'content':" Ridge Regression is a variation of linear regression. We use ridge regression to tackle the multicollinearity problem. Due to multicollinearity, the model estimates (least square) see a large variance. Ridge regression is a method by which we add a degree of bias to the regression estimates.\n Overview - Ridge Regression Ridge regression is a parsimonious model that performs L2 regularization. The L2 regularization adds a penalty equivalent to the square of the magnitude of regression coefficients and tries to minimize them. The equation of ridge regression looks like as given below.\n LS Obj + λ (sum of the square of coefficients)\r Here the objective is as follows:\n  If λ = 0, the output is similar to simple linear regression.\n  If λ = very large, the coefficients will become zero.\n  The following diagram is the visual interpretation comparing OLS and ridge regression.\n\r\rTraining Ridge Regression Model To build the ridge regression in r, we use glmnetfunction from glmnet package in R. Let\u0026rsquo;s use ridge regression to predict the mileage of the car using mtcars dataset.\n# Loaging the library\rlibrary(glmnet)\r# Getting the independent variable\rx_var \u0026lt;- data.matrix(mtcars[, c(\u0026quot;hp\u0026quot;, \u0026quot;wt\u0026quot;, \u0026quot;drat\u0026quot;)])\r# Getting the dependent variable\ry_var \u0026lt;- mtcars[, \u0026quot;mpg\u0026quot;]\r# Setting the range of lambda values\rlambda_seq \u0026lt;- 10^seq(2, -2, by = -.1)\r# Using glmnet function to build the ridge regression model\rfit \u0026lt;- glmnet(x_var, y_var, alpha = 0, lambda = lambda_seq)\r# Checking the model\rsummary(fit)\r# Output\rLength Class Mode a0 41 -none- numeric\rbeta 123 dgCMatrix S4 df 41 -none- numeric\rdim 2 -none- numeric\rlambda 41 -none- numeric\rdev.ratio 41 -none- numeric\rnulldev 1 -none- numeric\rnpasses 1 -none- numeric\rjerr 1 -none- numeric\roffset 1 -none- logical\rcall 5 -none- call nobs 1 -none- numeric\rChoosing Optimal Lambda Value The glmnet function trains the model multiple times for all the different values of lambda, which we pass as a sequence of vector to the lambda = argument in the glmnet function. The next task is to identify the optimal value of lambda that will result in a minimum error. This can be achieved automatically by using cv.glmnet() function.\n# Using cross validation glmnet\rridge_cv \u0026lt;- cv.glmnet(x_var, y_var, alpha = 0, lambda = lambdas)\r# Best lambda value\rbest_lambda \u0026lt;- ridge_cv$lambda.min\rbest_lambda\r# Output\r[1] 79.43000\rExtracting the best model using K-cross validation The best model can be extracted by calling the glmnet.fit from the cross-validation object. Once you have that, we can rebuild the model by passing lambda as 79.43000.\nbest_fit \u0026lt;- ridge_cv$glmnet.fit\rhead(best_fit)\r# Output\rDf %Dev Lambda\r[1,] 3 0.1798 100.00000\r[2,] 3 0.2167 79.43000\r[3,] 3 0.2589 63.10000\r[4,] 3 0.3060 50.12000\r[5,] 3 0.3574 39.81000\r[6,] 3 0.4120 31.62000\rBuilding the final model # Rebuilding the model with optimal lambda value\rbest_ridge \u0026lt;- glmnet(x_var, y_var, alpha = 0, lambda = 79.43000)\rChecking the coefficients coef(best_ridge)\r# Output\r4 x 1 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot;\rs0\r(Intercept) 20.099502946\rhp -0.004398609\rwt -0.344175261\rdrat 0.484807607\rThe next task is to use the predict function and compute the R2 value for both the train and test dataset. In this, the example we did not create the train and test split. So, I am only providing a sample code. However, you can read the linear regression chapter to understand this step in detail.\n# here x is the test dataset\rpred \u0026lt;- predict(best_ridge, s = best_lambda, newx = x)\r# R squared formula\ractual \u0026lt;- test$Price\rpreds \u0026lt;- test$PreditedPrice\rrss \u0026lt;- sum((preds - actual) ^ 2)\rtss \u0026lt;- sum((actual - mean(actual)) ^ 2)\rrsq \u0026lt;- 1 - rss/tss\rrsq\rBias and variance of ridge regression Bias and variance trade-off is generally complicated when it comes to building ridge regression models on an actual dataset. However, following the general trend which I would like to highlight here:\n The bias increases as λ increases. The variance decreases as λ increases.  Assumptions of Ridge Regressions The assumptions of ridge regression are the same as that of linear regression: linearity, constant variance, and independence. However, as ridge regression does not provide confidence limits, the distribution of errors to be normal need not be assumed.\nIn this chapter, we learned about ridge regression in R using functions from glmnet package. We also saw how to use cross-validation to get the best model. In the next chapter, we will learn how to lasso regression.\n"});index.add({'id':9,'href':'/docs/pre-processing-tasks/Missing-Value-Imputation/','title':"Missing Value Imputation",'content':"Overview Having missing values in a data set is a very common phenomenon. There are many reasons due to which a missing value occurs in a dataset. It is vital to figure out the reason for missing values. However, in this article, we will only focus on how to identify and impute the missing values.\nBefore we start, let us first, randomly add some missing values to the iris dataset.\n# Injecting missing value\rset.seed(86)\riris[sample(1:nrow(iris), 5), \u0026quot;Sepal.Width\u0026quot;] \u0026lt;- NA\riris[sample(1:nrow(iris), 10), \u0026quot;Petal.Length\u0026quot;] \u0026lt;- NA\riris[sample(1:nrow(iris), 8), \u0026quot;Sepal.Length\u0026quot;] \u0026lt;- NA\rNow that we have added few missing values in the dataset let\u0026rsquo;s proceed further to see how a data analyst or a data scientist deals with this problem of missing values.\nHow to identify missing values  The very first thing is to determine if a dataset has a missing value. We can check this using functions like is.na or complete.cases.  # Using is.na() function\rany(is.na(iris))\r# Output\r[1] TRUE\rYes, we have missing values in the iris data\n# Using complete.cases() function to get percentage of missing value\rnrow(iris[!complete.cases(iris), ])/nrow(iris)*100\r# Output\r[1] 14\rOverall 14% of the data is missing\n Next is to identify which variables and what percentage of observations from each variable are missing. To achieve this we can use md.pattern function from mice package in R.  # Looking at the missing data pattern\rlibrary(mice)\rmd.pattern(iris)\r# Output\rPetal.Width Species Sepal.Width Sepal.Length Petal.Length 129 1 1 1 1 1 0\r8 1 1 1 1 0 1\r7 1 1 1 0 1 1\r1 1 1 1 0 0 2\r4 1 1 0 1 1 1\r1 1 1 0 1 0 2\r0 0 5 8 10 23\rThe above table states that there are 5 missing values in Sepal.Width, 8 missing values in Sepal.Length, and 10 missing values in Petal.Length. In the last column, it is mentioned that overall, 23 values are missing.\nHow to delete missing observations When the total number of missing observations is significant, then we can think of removing those observations from the data. Imputing too many missing observations can lead to bias in the dataset. Also, it can result in poor statistical models. We can delete the missing values at the data preparation stage or at the time of building the model. However, not all algorithms provide this option of deleting missing values while we train the model.\n# Deleting missing observations\riris \u0026lt;- iris[complete.cases(iris), ]\r# or we can use na.omit() function\riris \u0026lt;- na.omit(iris)\r# Ignoring missing values while building lm model\rlm(mpg ~ cyl + disp, data=mtcars, na.action=na.omit)\rFor showcasing linear regression example we used mtcars dataset\nHow to delete variables with missing values Sometimes one or two variables contribute to the most number of missing values. In such cases, deleting these variables with a high percentage of missing values will help save lots of observations. According to one thumb rule we delete all variable which has more than 30% of missing values.\n## Removing columns with more than 30% NA\riris[, -which(colMeans(is.na(iris)) \u0026gt; 0.3)]\rImputing missing values Replacing missing values with a rough approximate value is acceptable and could result in a satisfactory result. Let us look at some of the ways in which we can replace the missing values.\nUsing mean/median/mode To replace missing values with mean, median, or mode, we can use impute function from Hmisc package. This can also be achieved by using square brackets[] or ifelse statement.\n# Using impute function from Hmisc package\rlibrary(Hmisc)\rimpute(iris$Sepal.Length, mean) # replace with mean\rimpute(iris$Sepal.Length, median) # median\rOther ways are using which we can replace missing values.\n# Filling missing values with Mean\riris$Sepal.Length[is.na(iris$Sepal.Length)] = mean(iris$Sepal.Length, na.rm=TRUE)\r# alternative way is to use ifelse\riris = transform(iris, y = ifelse(is.na(iris), mean(iris, na.rm=TRUE), Sepal.Length))\rMultivariate Imputation By Chained Equations(mice R Package) The mice function from the package automatically detects the variables which have missing values. Once identified, the missing values are then replaced by Predictive Mean Matching (PMM). The other methods which mice support are listed below:\n logreg(Logistic Regression) – Used For Binary Variables Proportional odds model – Used For ordered levels \u0026gt;= 2 polyreg(Bayesian polytomous regression) – Used For unordered levels\u0026gt;= 2  Let us look at an example:\nlibrary(mice)\r# Imputing the values using mice\rimputed_iris \u0026lt;- mice(iris, m=5, method = 'pmm', seed = 101)\r# checking the summary\rsummary(imputed_iris)\r# Output\rClass: mids\rNumber of multiple imputations: 5\rImputation methods:\rSepal.Length Sepal.Width Petal.Length Petal.Width Species\r\u0026quot;pmm\u0026quot; \u0026quot;pmm\u0026quot; \u0026quot;pmm\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot;\rPredictorMatrix:\rSepal.Length Sepal.Width Petal.Length Petal.Width Species\rSepal.Length 0 1 1 1 1\rSepal.Width 1 0 1 1 1\rPetal.Length 1 1 0 1 1\rPetal.Width 1 1 1 0 1\rSpecies 1 1 1 1 0\rYou can also look at the imputed values by using the following code\n# Checking imputed values of sleep variable\rimputed_iris$imp\r# Output\r$`Sepal.Length`\r1 2 3 4 5\r29 4.6 4.7 4.6 5.1 4.9\r32 4.6 5.0 5.0 4.9 4.9\r65 5.7 5.6 4.8 5.7 5.7\r73 6.9 6.4 6.4 6.8 6.8\r81 4.8 5.1 5.5 5.1 5.5\r91 5.8 5.8 6.3 6.2 6.0\r108 7.2 7.4 7.2 7.2 7.2\r123 7.6 7.4 7.9 7.7 7.7\r$Sepal.Width\r1 2 3 4 5\r33 3.2 3.8 3.8 3.1 3.4\r68 3.0 2.5 2.2 2.2 2.4\r74 3.0 2.2 3.0 3.0 2.6\r115 3.0 2.8 2.5 3.1 2.8\r134 3.0 2.6 2.5 2.5 2.5\r$Petal.Length\r1 2 3 4 5\r32 1.4 1.6 1.5 1.5 1.5\r33 1.5 1.5 1.5 1.4 1.2\r41 1.5 1.5 1.5 1.4 1.4\r48 1.6 1.6 1.5 1.4 1.4\r50 1.4 1.5 1.6 1.4 1.6\r60 3.5 4.5 3.9 3.5 4.2\r82 3.0 3.5 4.1 4.1 3.5\r106 6.4 6.4 6.7 6.4 6.1\r109 5.8 5.8 5.6 6.0 5.1\r133 5.3 5.6 6.0 6.0 5.1\r$Petal.Width\r[1] 1 2 3 4 5\r\u0026lt;0 rows\u0026gt; (or 0-length row.names)\r$Species\r[1] 1 2 3 4 5\r\u0026lt;0 rows\u0026gt; (or 0-length row.names)\rEach missing value is filled with 5 different numbers and is represented by columns\r\rUsing Machine Learning Algorithms There are actually a couple of nice and very popular machine learning algorithms that we can use to fill the missing values. Algorithms like KNN and random forest are some such popular algorithms.\nUsing KNN to fill the missing values library(bnstruct)\rknn.impute(iris, k = 5, cat.var = 1:ncol(iris), to.impute = 1:nrow(iris),\rusing = 1:nrow(iris))\rUsing random forest to fill the missing values # Imputing values using Random forest\rset.seed(86)\riris \u0026lt;- rfImpute(Species ~ ., iris.na)\rAnalysing imputed values using graphical methods When it comes to visualizing the missing and imputed values, my personal favorite is VIM package in R. The package is very useful in identifying the underlying mechanism responsible for missing values. It has many built-in functions to visualize the missing and imputed values. Also, this package has some nice functions for the imputation of missing values. Below I am providing a few examples on how to generate some beautiful visualizations. However, I would encourage you to read the documentation or for quick in-depth reference read this article visualization of imputed values using vim\nGenerating boxplot comparing distribution of all observations with imputed values library(VIM)\r# imputting values in iris dataset using knn\riris_imputed \u0026lt;- kNN(iris[, c(\u0026quot;Petal.Length\u0026quot;, \u0026quot;Sepal.Width\u0026quot;, \u0026quot;Sepal.Length\u0026quot;)])\r# Using pbox function to draw boxplot\rpbox(iris_imputed, delimiter=\u0026quot;imp\u0026quot;, selection=\u0026quot;any\u0026quot;)\rGenerating a graph show cashing scatter and box plot in one graph library(VIM)\r# imputting values in iris dataset using knn\riris_imputed \u0026lt;- kNN(iris[, c(\u0026quot;Petal.Length\u0026quot;, \u0026quot;Sepal.Width\u0026quot;)])\r# Using tao dataset to draw the margin plot\rmarginplot(iris_imputed, delimiter=\u0026quot;imp\u0026quot;,\ralpha = 0.8)\r\r\rThe above graph is fascinating and provides a detailed insight into the actual and imputed values. There are many other types of visualization that VIM package supports. It is undoubtedly helpful to spend some time on learning and exploring this powerful R Package.\nWhich is the best imputation method? It is a very frequently asked question, and although there are many ways in which we can impute missing values, one cannot say with certainty that a particular method provides the best result. Therefore, it is advised to test out some of these methods and see which one is providing the best result. As we know, statistics is all about trial and error.\r\r"});index.add({'id':10,'href':'/docs/quick-r-tutorial/r-basics/','title':"R Basics",'content':"Overview Ross Ihaka and Robert developed R at the University of Auckland in New Zealand. They started working on the tool in 1933 to help their students. However, they were then encouraged to make it open source. The language is based on another single letter programming language called S, primarily it is called S+, and it still exists.\nOne of the major reasons for the popularity of R is that R and its packages are Open Source and Free.\nDo You Know R, in its original form is a command-line language.\r\rGetting Help in R R has an extensive help system, and this is one of the best features of R programming. One can quickly access the documentation of functions and packages by using help() or ?. These functions provide access to the documentation pages for R functions, data sets, and other objects. Almost all the documents of R packages and functions contain a couple of examples showcasing how to use the function.\nhelp(mean)\r?mean\rOperators in R R supports almost all the popular binary and logical operators. I am sure you will be familiar with nearly all of them. The operators mentioned below can be used with scalars, vectors, and matrices.\nBinary/Arithmetic Operators    Operator Description     + Addition   - Subtraction   * Multiplication   / Division   ** or ^ Exponentiation   X%/%Y Integer Division   X%%Y Modulus gives remainder    Arithmetic operators in action # adding two values\r2 + 2\r# Multiplying\r23*34\r# Integer division\r1990%/%23\r# Calculating Modulus\r7%%2\rAlthough R is a remarkable statistical tool, there is one exasperating thing about R that it is a case-sensitive language. That means that view (with smaller case v)and View(with capital V) are considered as two different objects.\nLogical Operators Logical operators return TRUE if the condition is met and return FALSE if the condition is not met. They can be used with both numbers and strings.\n   Operator Description     \u0026gt; Greater than   \u0026gt;= Greater than or equal to   \u0026lt; Less than   \u0026lt;= Less than or equal to   == Equal to   != Not equal to   x y   x \u0026amp; y x and y   !x Not x    Logical Operators in action # Using Great than\r10 \u0026gt; 11\r# Using equal to\r\u0026quot;Hanna\u0026quot;==\u0026quot;hanna\u0026quot;\r# Using not x\r!10 == 11\r#Using AND operator\r(10 == 10) \u0026amp; (2 ==2)\rAssignment operator The assignment operator is used in programming languages to save or assign a value to the variable. The variable can then be used for further processing. In R, we use assignment operator (\u0026lt;-) to assign a value. We can also use equal to (=) symbol as assignment operator. However, assignment operator (\u0026lt;-) are far more prevalent in R than the equal to sign.\n# Assigning number values\rnum \u0026lt;- 23\rnum\r# Assigning string value\rstrng \u0026lt;- \u0026quot;Hanna\u0026quot;\rstrng\rStrings like most other programming languages are defined using double or single quotes.\r\rNumbers and Strings Numbers and strings are what constitute any dataset in general. So it becomes crucial we understand some of the most common tasks and functions you will be required to execute while dealing with them in general.\nWorking With Numbers Generating sequence of numbers To generate sequence of numbers one can either use semicolon(:) or can use seq() function.\n# Using (:) to generate sequence of integer numbers\r1:10\r# Using seq() function to generate sequence of numbers\rseq(10, 20, by = 0.7)\rGenerating uniformly distributed random numbers Among many functions, the functions which I like the most are runif() and sample() functions.\n# Using runif() function to generate 10 random numbers\r# By default generates number between 0 and 1\rrunif(10)\r# Generating numbers between 200 to 500\rrunif(10, min = 200, max = 500)\r# Generating four random numbers **REPLACEMENT**\rsample(10:15, 4, replace=TRUE)\r# Generating three random numbers **WITHOUT REPLACEMENT**\rsample(10:15, 4, replace=FALSE)\rsample() function is often used for creating random samples from the dataset. The sample data can then be used for training Machine Learning models or to deep dive and understand the data.\nGenerate random numbers from a normal distribution A normal distribution is a distribution that follows a bell curve. Statistically speaking, its mean, median, and mode are all the same.\n# Using rnorm() function to generate 10 random numbers\rrnorm(10)\r# Setting the desired standard deviation and mean\rrnorm(10, mean = 5, sd = 2)\rGenerating the same sequence of random numbers The set.seed() function can be used to generate the same set of random numbers. A useful function that can help you to generate reproducible results. The function takes one argument, which is an integer number. Keeping that number the same gives you identical results.\n# With set.seed()\r# Output 1\rset.seed(23)\rrnorm(10, mean = 5, sd = 2)\r# Output 2\rset.seed(23)\rrnorm(10, mean = 5, sd = 2)\r# Without set.see()\r# Output 1\rrnorm(10, mean = 5, sd = 2)\r# Output 2\rrnorm(10, mean = 5, sd = 2)\rRounding numbers to the nearest value We have a couple of ways to achieve this. One can round the values to the nearest integer, to the upper side, to the lower side, or towards zero. The following set of functions can be used to achieve either of the said tasks.\n# Generating a sequence of numbers\rnumerSeq\u0026lt;- seq(0, 1, by=.05)\r# Rounding to nearest integer - it uses that .5 rule\rround(numerSeq)\r# Rounding to one decimal point\rround(numerSeq, 1)\r# Rounding towards upper side value\rceiling(numerSeq)\r# Rounding towards lower side value\rfloor(numerSeq)\r# Rounding towards Zero\rtrunc(numerSeq)\rWorking With Strings The two tasks which are very critical from the data analysis point of view are: Combining Strings Searching \u0026amp; Replacing Strings\nCombining strings Knowing how to combine strings or a string with a number can be of great help. I often use this to represent or print my final output. Another use comes from the analysis point of view. Considering these two tasks in mind the two most widely used functions are paste()(space is a default separator) or paste0()(there is no separator) function and sprintf() function.\n# Combing two strings using paste() function\rpaste(\u0026quot;Hanna\u0026quot;, \u0026quot;Ask\u0026quot;)\r# Choosing different separator\rpaste(\u0026quot;Hanna\u0026quot;, \u0026quot;Ask\u0026quot;, sep = \u0026quot;$\u0026quot;)\r# Using paste0() function\rpaste0(\u0026quot;Hanna\u0026quot;, \u0026quot;Ask\u0026quot;)\rYou can also pass a collection of string inside the paste() function. This collection of similar elements in R is formally called a vector. More on this later.\n# Creating a vector of string\rstrgVec \u0026lt;- c(\u0026quot;Cat\u0026quot;, \u0026quot;Dog\u0026quot;, \u0026quot;Fish\u0026quot;, \u0026quot;Cow\u0026quot;)\r# Combing the values by +\rpaste(strgVec, collapse = \u0026quot;+\u0026quot;)\rsprintf() function is derived from c programming.\n# Using sprintf() function to combine two string\rsprintf(\u0026quot;My name is %s\u0026quot;, \u0026quot;Hanna\u0026quot;)\r# Combining a string and an integer\rsprintf(\u0026quot;My name is %s and I am %d years old\u0026quot;, \u0026quot;Hanna\u0026quot;, 30)\rSearching and Replacing strings We will cover three very useful functions here are those are sub(), gsub() and grep().\n# Defining a string\rstrng \u0026lt;- “You’re gonna need a bigger boat boat.”\r# Replacing boat with car\rsub(\u0026quot;boat\u0026quot;, \u0026quot;car\u0026quot;, strng)\r# Replacing boat by with car at all instances\rgsub(\u0026quot;boat\u0026quot;, \u0026quot;car\u0026quot;, strng)\r# Returns the index where the string matches\rgrep(\u0026quot;[car]\u0026quot;, letters)\rData types and structure In R, there are six data types and four data structures.\nData Types  Character - it the collection of string. Example - \u0026ldquo;Hanna\u0026rdquo;, \u0026ldquo;Dog\u0026rdquo;, \u0026ldquo;Male\u0026rdquo;. Numeric - it is a numeric value which is represented by decimal points. Example - 10.4, 12.45. Integer - its is also a number but only the integer part. Example - 109, 123, 34. Logical - the Boolean values. Example - TRUE, FALSE Factor - qualitative variable which can be either of nominal or ordinal type. If it is ordinal then it is called as ordered factor. Example Nominal - \u0026ldquo;Male\u0026rdquo; and \u0026ldquo;Female\u0026rdquo;. Example Ordinal - \u0026ldquo;Good\u0026rdquo;, \u0026ldquo;Average\u0026rdquo; and \u0026ldquo;Best\u0026rdquo;. Complex - a number which has got an imaginary part to it.  Integer values can also represent the factor variable. So always make a habit of going back and looking at the metadata for variable information.\r\rData Structure The data structures in R are defined based on the dimensionality and homogeneity of data type it can hold.\n  Vector - They are also formally known as Atomic Vectors. A Vector can hold only one type of data and is one-dimensional.\n  List - List is also one-dimensional structure; however it can store multiple data types.\n  Matrix - Matrix is two-dimensional structure and can only save one data type.\n  Data Frame - Data Frame is also a two-dimensional structure but can save multiple types of data.\n  Vector manipulation Now we will learn about some of the most basic data manipulation functions. The knowledge of these functions is an absolute must for anyone to move forward and perform any kind of data analysis task.\nDefining vectors Here is a collection of all the functions which are used to define different data types and structures in R programming.\n# Defining character vectors\rcharacterVector \u0026lt;- c(\u0026quot;Football\u0026quot;, \u0026quot;Cricket\u0026quot;, \u0026quot;Tennis\u0026quot;, \u0026quot;Badminton\u0026quot;)\r# Defining numeric vectors\rnumericVector \u0026lt;- c(12.3, 23.4, 17.9, 89.7)\r# Defining integer vectors\rintegerVector \u0026lt;- c(12L, 23L, 17L, 89L)\r# Defining logical vectors\rlogicVector \u0026lt;- c(TRUE, FALSE, TRUE, TRUE)\r# Defining factor - nominal\rfactorVector \u0026lt;- factor(characterVector)\r# Defining factor - ordinal\rorderedFactorVector \u0026lt;- factor(characterVector, ordered = TRUE)\rVerifying and checking the class of the vectors For vectors, when we check the data structure type, it returns the type of the data which it holds. For checking the class of the vector we can use either class() function or typeof() function. There are other functions, but these are the common ones.\n# Using class() function to check the object type\rclass(numericVector)\r# Using typeof function to check the object type\rtypeof(numericVector)\rIf you just wish to check the type of vector, then we can use is family functions. These functions will return TRUE if the vector belongs to a specific kind else it returns FALSE.\n# Checking if the vector is character type\ris.character(numericVector)\r# Checking if the vector is numeric type\ris.numeric(numericVector)\rAccessing the elements of a vector The elements inside a vector can be accessed using an index. Unlike other programming languages like C or Python, the indexing in R starts from 1 and not zero.\n# Extracting third elements\rcharacterVector[3]\r# Extracting multiple elements\rcharacterVector[c(1,3)]\r# Deleting element\rcharacterVector[-1]\r# Deleting multiple element\rcharacterVector[-c(1,3)]\rNote One is not allowed to pass both positive and negative index values.\r\rReplacing and adding values to a vector To replace existing values in a vector. First, call the value using square brackets [] and then simply assign a new value to it.\n# Replacing football with basketball\rcharacterVector[1] \u0026lt;- \u0026quot;Basketball\u0026quot;\rcharacterVector\r# Replacing more than one values\rnumericVector[c(1,4)] \u0026lt;- c(55, 66)\rnumericVector\rTo add new value to a vector, you can use either of the below approaches based upon your requirement.\nUsing Index The numericVector contains 4 elements. We will add a new element to this vector by using the index. However, this method only allows us to add a new element at the end of the vector.\n# Adding element at the end.\rnumericVector[5] \u0026lt;- 77\rnumericVector\rUsing c() function By using c() function, you can add new element either at the beginning or at the end of a vector.\n# Adding element at the end.\rnumericVector \u0026lt;- c(numericVector, 99)\rnumericVector\r# Adding element at the beginning\rnumericVector \u0026lt;- c(99, numericVector)\rnumericVector\rUsing append() function If you wish to add a new element at any given index in a vector, then append() function is the correct choice. The function takes three arguments.\n# Using append function to add value after 4th positon\rnumericVector \u0026lt;- append(numericVector, # vector\r99, # element to be interted\r4) # index after which to be inserted\rGetting the index of a particular element The below code can be used to get the index of an element in R.\n# Printing the index of values which are equal to 99\rwhich(numericVector == 99.0)\rOther important vector manipulation functions Below is the list of essential and useful functions for the manipulation of vectors in R.\nSorting a vector # Sorting in ascending order\rnumericVector[order(numericVector)]\r# Sorting in descending order\rnumericVector[order(numericVector, decreasing = TRUE)]\rChecking and Removing missing values # Adding NA value to a vector\rnumericVector[2] \u0026lt;- NA\r# Checking if missing value is present\ris.na(numericVector)\r# Removing NA values using ! not\rnumericVector[!is.na(numericVector)]\r# Removing NA values using na.omit() function\rna.omit(numericVector)\rSub setting the vector and getting length of a vector # Getting elements greater than 30\rnumericVector \u0026lt;- numericVector[numericVector \u0026gt; 30]\r# Checking the total number of elements in the new vector\rlength(numericVector)\rList manipulation Defining list To define a list we use list() function. The function can be used to create simple list or named list.\n\r# Defining list\rexample1 \u0026lt;- list(c(2,3,4), c(\u0026quot;aa\u0026quot;, \u0026quot;bb\u0026quot;, \u0026quot;cc\u0026quot;, \u0026quot;dd\u0026quot;), c(TRUE, TRUE))\rexample1\r# Defining vectors\rempName \u0026lt;- c(\u0026quot;Chris\u0026quot;, \u0026quot;Robin\u0026quot;, \u0026quot;Matt\u0026quot;)\rempSalary \u0026lt;- c(2000, 4000, 6000)\rbonusGiven \u0026lt;- c(TRUE, TRUE, FALSE)\r# Defining list using vectors\rlistStruct \u0026lt;- list(empName, empSalary, bonusGiven)\rlistStruct\r# Defining Named list\rnamedListStruct \u0026lt;- list(\u0026quot;empName\u0026quot; = empName,\r\u0026quot;empSalary\u0026quot; = empSalary,\r\u0026quot;bonusGiven\u0026quot; = bonusGiven)\rnamedListStruct\rReferencing values of a list A value inside a list can be accessed using an index or by using the name(if it is a named list). Fundamentally list is nothing but a collection of vectors. That means we can apply all the data manipulations, which we have just learned in the Vector Manipulation section.\nExtracting values from a list # Extracting a value list of emp names from unnamed list\rlistStruct[[1]]\r# Extracting a value list of emp names from the named list\rnamedListStruct$empName\r# Extract Robin from the emp names\rlistStruct[[1]][2]\r# Extracting a value list of emp names from named list\rnamedListStruct$empName[2]\rReplacing values in a list # Replace salary for Robin by 8000\rlistStruct[[2]][2] \u0026lt;- 8000\rlistStruct\r# or in named list\rnamedListStruct$empSalary[2] \u0026lt;- 8000\rnamedListStruct\rUnlisting the list Unlist() function can be used to flatten out the list to one level.\nunlist(listStruct)\rChecking the class of each vector in a list lapply(listStruct, class)\rA list can consist of multiple levels, and one can also create a nested list. Also, lists can be used to bundle objects of different lengths.\r\rMatrix Manipulation Defining Matrix As it is a two-dimensional structure while defining, we need to mention the number of rows and number of columns.\n# Defining a matrix\rmatStruct \u0026lt;- matrix(integerVector,\rnrow = 2, ncol = 2,\rbyrow = TRUE)\r# Defining a matrix\rmatStruct1 \u0026lt;- matrix(integerVector,\rnrow = 2, ncol = 2,\rbyrow = FALSE)\rBasic operations related to matrix In the below code snippet we are sharing some functions which are good to know and will help you with your data science work.\n# naming columns\rcolnames(matStruct) \u0026lt;- c(\u0026quot;col1\u0026quot;, \u0026quot;col2\u0026quot;)\r# naming rows\rrownames(matStruct) \u0026lt;- c(\u0026quot;row1\u0026quot;, \u0026quot;row2\u0026quot;)\r# Getting the dimension of the matrix\rdim(matStruct)\r# Getting the count of the rows\rnrow(matStruct)\r# Getting the count of the columns\rncol(matStruct)\r# Accessing 2 column values\rmatStruct[, 2]\r# Accessing 1 row values\rmatStruct[1, ]\r# Combing two matrix by columns\rcbind(matStruct, matStruct1)\r# Combing two matrix by rows - appending\rrbind(matStruct, matStruct1)\rIn this chapter, we looked at all the basic concepts of R Programming. We learned and spent some time looking at things like different operators, data types, structures, and some must-know functions, which will enable you to manipulate these structures. We hope you got some sense of how powerful this software can be. In the next chapter, we’ll look at an extensive list of data manipulation tasks related to data frames.\n"});index.add({'id':11,'href':'/docs/useful-r-packages/Leaftlet-Package-for-Maps/','title':"Leaftlet For Maps",'content':"In progress\n"});index.add({'id':12,'href':'/docs/text-mining/sentiment-analysis/','title':"Sentiment Analysis",'content':"Under Construction\n"});index.add({'id':13,'href':'/docs/foundational-algorithms/Lasso-Regression/','title':"Lasso Regression",'content':" LASSO stands for Least Absolute Shrinkage and Selection Operator. The algorithm is another variation of linear regression, just like ridge regression. We use lasso regression when we have a large number of predictor variables.\n Overview - Lasso Regression Lasso regression is a parsimonious model that performs L1 regularization. The L1 regularization adds a penalty equivalent to the absolute magnitude of regression coefficients and tries to minimize them. The equation of lasso is similar to ridge regression and looks like as given below.\n LS Obj + λ (sum of the absolute values of coefficients)\r Here the objective is as follows: If λ = 0, We get the same coefficients as linear regression If λ = vary large, All coefficients are shrunk towards zero\nThe two models, lasso and ridge regression, are almost similar to each other. However, in lasso, the coefficients which are responsible for large variance are converted to zero. On the other hand, coefficients are only shrunk but are never made zero in ridge regression.\nLasso regression analysis is also used for variable selection as the model imposes coefficients of some variables to shrink towards zero.\nWhat does a large number of variables mean?  The large number here means that the model tends to over-fit. Theoretically, a minimum of ten variables can cause an overfitting problem. When you face computational challenges due to the presence of n number of variables. Although, given today\u0026rsquo;s processing power of systems, this situation arises rarely.  The following diagram is the visual interpretation comparing OLS and lasso regression.\n\r\r\rThe LASSO is not very good at handling variables that show a correlation between them and thus can sometimes show very wild behavior.\r\rTraining Lasso Regression Model The training of the lasso regression model is exactly the same as that of ridge regression. We need to identify the optimal lambda value and then use that value to train the model. To achieve this, we can use the same glmnet function and passalpha = 1 argument. When we pass alpha = 0, glmnet() runs a ridge regression, and when we pass alpha = 0.5, the glmnet runs another kind of model which is called as elastic net and is a combination of ridge and lasso regression.\n We use cv.glmnet() function to identify the optimal lambda value Extract the best lambda and best model Rebuild the model using glmnet() function Use predict function to predict the values on future data  For this example, we will be using swiss dataset to predict fertility based upon Socioeconomic Indicators for the year 1888.\n# Loading the library\rlibrary(glmnet)\r# Loading the data\rdata(swiss)\rx_vars \u0026lt;- model.matrix(Fertility~. , swiss)[,-1]\ry_var \u0026lt;- swiss$Fertility\rlambda_seq \u0026lt;- 10^seq(2, -2, by = -.1)\r# Splitting the data into test and train\rset.seed(86)\rtrain = sample(1:nrow(x_var), nrow(x_var)/2)\rx_test = (-train)\ry_test = y_var[test]\rcv_output \u0026lt;- cv.glmnet(x_vars[train,], y_var[train],\ralpha = 1, lambda = lambda_seq)\r# identifying best lamda\rbest_lam \u0026lt;- cv_output$lambda.min\r# Output\r[1] 1.995262\rUsing this value, let us train the lasso model again.\n# Rebuilding the model with best lamda value identified\rlasso_best \u0026lt;- glmnet(x_vars[train,], y_var[train], alpha = 1, lambda = best_lam)\rpred \u0026lt;- predict(lasso_best, s = best_lam, newx = x_vars[test,])\rFinally, we combine the predicted values and actual values to see the two values side by side, and then you can use the R-Squared formula to check the model performance. Note - you must calculate the R-Squared values for both the train and test dataset.\nfinal \u0026lt;- cbind(y_var[test], pred)\r# Checking the first six obs\rhead(final)\r# Output\rActual Pred\rCourtelary 80.2 69.92666\rDelemont 83.1 76.15793\rFranches-Mnt 92.5 75.16697\rMoutier 85.8 70.33981\rGlane 92.4 76.61480\rVeveyse 87.1 76.34404\rSharing the R Squared formula The function provided below is just indicative, and you must provide the actual and predicted values based upon your dataset.\nactual \u0026lt;- test$actual\rpreds \u0026lt;- test$predicted\rrss \u0026lt;- sum((preds - actual) ^ 2)\rtss \u0026lt;- sum((actual - mean(actual)) ^ 2)\rrsq \u0026lt;- 1 - rss/tss\rrsq\rGetting the list of important variables To get the list of important variables, we just need to investigate the beta coefficients of the final best model.\n# Inspecting beta coefficients\rcoef(lasso_best)\r# Output\r6 x 1 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot;\rs0\r(Intercept) 55.16706057\rAgriculture . Examination -0.30124968\rEducation . Catholic 0.04700893\rInfant.Mortality 0.84730322\rThe model indicates that the coefficients of Agriculture and Education have been shrunk to zero. Thus we are left with three variables, namely; Examination, Catholic, and Infant.Mortality\nIn this chapter, we learned how to build a lasso regression using the same glmnet package, which we used to build the ridge regression. We also saw what\u0026rsquo;s the difference between the ridge and the lasso is. In the next chapter, we will discuss how to predict a dichotomous variable using logistic regression.\n"});index.add({'id':14,'href':'/docs/pre-processing-tasks/Summarizing-Data/','title':"Summarizing Data",'content':" The very first task in any project related to data modeling is to explore the data, formally known as,\n Overview Exploratory Data Analysis(EDA) - There are many statistics that we calculate as part of EDA. However, in this chapter we will learn how to summarize data using descriptive statistics.\nData is a collection of observations and there features (known as variables). When we try to summarize the data, the variable type plays an important role in deciding which statistic we will be considering to summarize the variable. For qualitative variables, we prepare a contingency table and try to visualize them. On the other hand, when summarizing quantitative data, we need to do much more than just create tables. We need to consider central tendency, variance, and shape for each of these values.\nIdentifying the variable type is the first and foremost task. You can use str function to quickly understand the data and each variable type.\n# Quick look into data structure\rstr(ToothGrowth)\r\r\rThe above output mentions that toothGrowth is a data frame that has 60 observations and 3 variables. The function also prints the variable names, their type, and the top 10 values of each variable.\nFrequency \u0026amp; contingency table for qualitative variables To summarise the categorical variable, we can generate a frequency table or a contingency table. A frequency table helps us understand and represent categorical data in a compact form. It represents the number of times each level of a categorical variable appears in the respective variable. On the other hand, a contingency table is a special case of frequency table which represents two categorical variables simultaneously.\n# Building table for one-way table - Frequency Table\rtable(CO2$Treatment)\r# Output\rnonchilled chilled\r42 42\r# Building table for two-way table - Contingency Table\rtable(CO2$Treatment, CO2$Type)\r# Output\rQuebec Mississippi\rnonchilled 21 21\rchilled 21 21\rYou can also get the proportion of each level by passing the output of table function to the prop.table function. Below are a few examples of how you can use this function.\n# creating the freq table\rtab \u0026lt;- table(CO2$Treatment)\r# Proportion by row\rprop.table(tab)\r# Output\rnonchilled chilled\r0.5 0.5\rTo make the above output a little bit more readable, we will multiply the table by 100 and then round the value to one decimal point. An example of the same is given below, where we print the column-wise proportion.\n# Proportion by column\rround(prop.table(table(mtcars$cyl, mtcars$am), 2)*100, 1)\r# Output\r0 1\r4 15.8 61.5\r6 21.1 23.1\r8 63.2 15.4\rSummarizing numeric variables We use descriptive statistics for summarising the variables. As a data analyst, you must look into the three things while trying to understand your data.\n1. Measures of central tendency - These statistics help us understand the central value of a given variable. For numeric variables, we can look into a mean or median. For categorical variables, we use mode.\n# Calculating the mean and median values\rmean(CO2$uptake, na.rm = TRUE)\rmedian(CO2$uptake, na.rm = TRUE)\r)\rFor mode, we don\u0026rsquo;t have a straight forward function. However, we can use which.max along with the table function to get the level that has the maximum count.\n# Getting the mode\rwhich.max(table(mtcars$cyl))\r2. Measures of dispersion- This measure helps us quantify the spread of the data. The stats which are used to understand the dispersion in a variable are - variance, standard deviation, and coefficient of covariance. Below are a few example functions which you can use to get this information.\n# For variance\rvar(mtcars$mpg)\r# Output\r[1] 36.3241\r# For standard deviation\rsd(mtcars$mpg)\r# Output\r[1] 6.026948\rThe coefficient of variance is also known as the relative standard deviation. It is a standardized measure of deviation. We will be using the formula to calculate the CV in the below code.\n# For coefficient of variance\rsd(mtcars$mpg)/mean(mtcars$mpg)*100\r# Output\r[1] 29.99881\r3. Measures of shape- Using these statistics, we try to look for the skewness and the peakedness of the distribution(also known as kurtosis). We have statistical measures that are used to define these parameters. But most of the time, we use histogram, density plots, and boxplots to understand the shape of the distribution.\n# Generating histogram with density line\rhist(AirPassengers, col = \u0026quot;lightblue\u0026quot;, prob = TRUE)\rlines(density(AirPassengers))\r\r\rSo far, we have been using one variable and one measure at a time to generate the summary statistics. However, in R, we have many functions that can generate these statistics for all the variables in a dataset. I am sharing some of the most popular and widely used summary functions.\n1. summary() function - generates a five number summary, available in base R package.\n# Using summary function to generate summary of toothGrowth\rsummary(ToothGrowth)\r# Output\rlen supp dose Min. : 4.20 OJ:30 Min. :0.500 1st Qu.:13.07 VC:30 1st Qu.:0.500 Median :19.25 Median :1.000 Mean :18.81 Mean :1.167 3rd Qu.:25.27 3rd Qu.:2.000 Max. :33.90 Max. :2.000 2. describe() function - The function comes from Hmisc R package and provides a little more detailed information about the variable of interest.\n# Loading the psych package\rlibrary(Hmisc)\r# Using describe function to generate a summary of toothGrowth\rHmisc::describe(ToothGrowth)\r# Output\r3 Variables 60 Observations\r-----------------------------------------------------------------------------\rlen\rn missing distinct Info Mean Gmd .05 .10\r60 0 43 0.999 18.81 8.839 6.37 8.11\r.25 .50 .75 .90 .95\r13.07 19.25 25.27 27.30 29.57\rlowest : 4.2 5.2 5.8 6.4 7.0, highest: 29.4 29.5 30.9 32.5 33.9\r-----------------------------------------------------------------------------\rsupp\rn missing distinct\r60 0 2\rValue OJ VC\rFrequency 30 30\rProportion 0.5 0.5\r-----------------------------------------------------------------------------\rdose\rn missing distinct Info Mean Gmd\r60 0 3 0.889 1.167 0.678\rValue 0.5 1.0 2.0\rFrequency 20 20 20\rProportion 0.333 0.333 0.333\r-----------------------------------------------------------------------------\r3. describe() function - The function comes from psych R package and provides information about all the above-mentioned measures like mean, median, variance, standard deviation, min, max, range, skewness and kurtosis among others.\n# Loading the psych package\rlibrary(psych)\r# Using describe function to generate summary of toothGrowth\rdescribe(ToothGrowth)\r# Output\rvars n mean sd median trimmed mad min max range skew kurtosis se\rlen 1 60 18.81 7.65 19.25 18.95 9.04 4.2 33.9 29.7 -0.14 -1.04 0.99\rsupp* 2 60 1.50 0.50 1.50 1.50 0.74 1.0 2.0 1.0 0.00 -2.03 0.07\rdose 3 60 1.17 0.63 1.00 1.15 0.74 0.5 2.0 1.5 0.37 -1.55 0.08\rAs the function names in two different packages is same we used double colon(::) to ensure that the function is called from the respective package.\r\rSummarizing data by a grouped variable At times it may make much more sense to understand data from grouped variables perspectives. For example - it makes much more sense to look into central tendency, variance, and shape of mileage variable type of cylinder. Below is a collection of functions that can be used to get insight into grouped variables.\n1. aggregate() function\n Getting the aggregated value of one variable by a single grouped variable.  # Average mileage by cylinder type\raggregate(mtcars$mpg, by = list(cyl = mtcars$cyl), FUN = mean)\r# Output\rcyl x\r1 4 26.66364\r2 6 19.74286\r3 8 15.10000\r Getting the aggerated value for multiple variables by a single grouped variable.  # Average mileage, displacement, and weight for each cylinder type\raggregate(mtcars[, c(\u0026quot;mpg\u0026quot;, \u0026quot;disp\u0026quot;, \u0026quot;wt\u0026quot;)],\rby = list(cyl = mtcars$cyl), FUN = mean)\r# Output\rcyl mpg disp wt\r1 4 26.66364 105.1364 2.285727\r2 6 19.74286 183.3143 3.117143\r3 8 15.10000 353.1000 3.999214\r Getting the aggreagted value by more than one grouping variable.  # Average mileage, displacement, and weight for each cylinder and am type\raggregate(mtcars[, c(\u0026quot;mpg\u0026quot;, \u0026quot;disp\u0026quot;, \u0026quot;wt\u0026quot;)],\rby = list(cyl = mtcars$cyl, am = mtcars$am), FUN = mean)\r# Output\rcyl am mpg disp wt\r1 4 0 22.90000 135.8667 2.935000\r2 6 0 19.12500 204.5500 3.388750\r3 8 0 15.05000 357.6167 4.104083\r4 4 1 28.07500 93.6125 2.042250\r5 6 1 20.56667 155.0000 2.755000\r6 8 1 15.40000 326.0000 3.370000\r2. describe.by() function The describe.by function is present in psych package. The function generate summary statistics by group variable.\n# loading library\rlibrary(psych)\rdescribe.by(mtcars[, c(\u0026quot;mpg\u0026quot;, \u0026quot;disp\u0026quot;)], mtcars$cyl)\r# Output\rDescriptive statistics by group\rgroup: 4\rvars n mean sd median trimmed mad min max range skew kurtosis se\rmpg 1 11 26.66 4.51 26 26.44 6.52 21.4 33.9 12.5 0.26 -1.65 1.36\rdisp 2 11 105.14 26.87 108 104.30 43.00 71.1 146.7 75.6 0.12 -1.64 8.10\r---------------------------------------------------------------------------------\rgroup: 6\rvars n mean sd median trimmed mad min max range skew kurtosis se\rmpg 1 7 19.74 1.45 19.7 19.74 1.93 17.8 21.4 3.6 -0.16 -1.91 0.55\rdisp 2 7 183.31 41.56 167.6 183.31 11.27 145.0 258.0 113.0 0.80 -1.23 15.71\r---------------------------------------------------------------------------------\rgroup: 8\rvars n mean sd median trimmed mad min max range skew kurtosis se\rmpg 1 14 15.1 2.56 15.2 15.15 1.56 10.4 19.2 8.8 -0.36 -0.57 0.68\rdisp 2 14 353.1 67.77 350.5 349.63 73.39 275.8 472.0 196.2 0.45 -1.26 18.11\r3. tapply() function The tapply function computes a summary statistics or a function for each factor variable in a vector.\n# Getting average mileage for each cylinder type\rtapply(mtcars$mpg, mtcars$cyl, FUN = mean)\r# Output\r4 6 8\r26.66364 19.74286 15.10000\rIn this chapter, we looked into some of the basic and advanced functions which can be used to provide summary statistics. We learned how to generate a frequency table for categorical variables, how to summarise numerical data and deep dive into the same by producing descriptive statistics by grouped variables.\n"});index.add({'id':15,'href':'/docs/quick-r-tutorial/using-conditional-statements-and-loops/','title':"Conditions \u0026 Loops",'content':"Conditions and Loops In R Conditional statements in R programming language enables the user to execute a statement based upon a particular condition or criteria. On the other hand, the loops ensure that the same task is executed again and again.\nHaving knowledge and understanding of how the two work is extremely critical for any programmer. As a data analyst or a data science practitioner, you will be using them quite often.\nif statement in R Programming If statements are used to check a condition and if the condition is met, it executes a block of code. It returns TRUE if the condition is met and returns FALSE if the condition is not met.\nif statement syntax # Syntax of if statement\rif(condition){\r# block of code to be executed\r}\rWorking example Let us look at an example - Print the summary statistics of a variable only if the variable is numeric.\n# Getting the iris data\rdata(iris)\r# Executing the if statement\rif(is.numeric(iris$Petal.Length)){\rprint(summary(iris$Petal.Length))\r}\rIn the above example, you saw that the summary got printed because the variable type was numeric. However, if the class of variable would have been different, no output would have been written. In fact, nothing would have happened, and leaving it like that is not a good idea, especially if a user is involved. You may want to inform the user that the variable entered is not a numeric one, and thus nothing is printed.\r\rif-else statement syntax # Syntax of if statement\rif(condition){\r# block of code to be executed\r} else {\r# block of code to be executed in case if returns FALSE\r}\rWorking example Let us take the above example and add an else statement that will print a statement notifying the user that the input variable is not numeric.\n# Executing the if statement\rif(is.numeric(iris$Species)){\rprint(summary(iris$Species))\r} else {\rprint(\u0026quot;Tested variable is not a numeric variable.\u0026quot;)\r}\rif-else statement syntax The nested if else statement is used when you want to test two or more conditions.\n# Syntax of nested if-else statement\rif(first condition){\r# block of code to be executed\r} else if(second condition) {\r# block of code to be executed if the second condition is met\r} else {\r# block of code to be executed if none of the conditions is met.\r}\rWorking example Let us take the above example and say we should print the summary statistics if the variable is numeric or integer and table if the variable is a factor or character type. If none of the conditions are met; print - we are sorry we do not accept this variable type.\n# Resetting the data\rdata(iris)\r# Getting the variable name\rvarName \u0026lt;- \u0026quot;Species\u0026quot;\r# Executing the if statement\rif(is.numeric(iris[, varName]) | is.integer(iris[, varName])){\rprint(summary(iris$Species))\r} else if(is.character(iris[, varName]) | is.factor(iris[, varName])) {\rprint(table(iris[ , varName]))\r} else {\rprint(\u0026quot;Variable type unknown\u0026quot;)\r}\rYou can have as many conditions in a nested if-else as you wish. However, remember that the if-else statement works from top to bottom. That means it may lead to unnecessary computation at times based upon your analysis requirement.\r\rswitch statement in R Programming If you are sure about all the output cases based upon the condition, then it is recommended to use switch function over the if-else statement.\nWorking example Print the mean if the user passes \u0026ldquo;1\u0026rdquo; and print median if the user passes \u0026ldquo;2\u0026rdquo;. We will use a switch statement to achieve this task.\n# Refreshing the mtcars dataset\rdata(mtcars)\r# printing mean\rswitch(\u0026quot;1\u0026quot;,\r\u0026quot;1\u0026quot; = mean(mtcars$mpg),\r\u0026quot;2\u0026quot; = median(mtcars$mpg))\r}\rfor loop in R Programming for loops are used to execute the same block of code over a range of values. That means as the range of values gets over, the for loop ends.\nfor loop syntax The syntax is pretty simple and straightforward. There are three components of the syntax: one, the vector in which we have the values. Second, the index identifier that will call one value at a time from the vector and final part is the block of code, which will be executed over and over.\n# For loop syntax\rfor(i in vector){\r# Block of code\r}\rWorking example Print the class of all the variable using for loop.\nvarNamesList \u0026lt;- colnames(iris)\rfor(i in varNamesList){\rprint(sprintf(\u0026quot;The class of %s is %s.\u0026quot;, i, class(iris[, i])))\r}\rTo print the value while running a for loop, you must use print() function. In the above example, I used the sprintf() function to ensure that values get printed on the console.\nWhile loop in R Programming A while loop is designed to execute when a condition is met. The execution of the code is stopped if the condition is not met.\nWorking Example Print a sequence of numbers starting from 1 till 10.\nnum \u0026lt;- 1\rwhile(num \u0026lt;= 10){\rprint(num)\rnum = num + 1\r}\rHow to break or skip a value in a loop To skip a value or break out of a loop in R, we have two control statements called next and break statement.\nWorking example Print only those names from a character vector that have less than five alphabets and skip all the names which are spelled using more than five characters. To achieve this we will be using nchar() function and if statement inside a for loop.\n# Defining a list of vector containg names\rallNames \u0026lt;- c(\u0026quot;John\u0026quot;, \u0026quot;Michell\u0026quot;, \u0026quot;Danny\u0026quot;, \u0026quot;Silver\u0026quot;, \u0026quot;Olive\u0026quot;)\rfor (nam in allNames) {\rif (nchar(nam) \u0026gt; 5){\rnext\r}\rprint(nam)\r}\rIn this chapter, we learned how to use conditional statements, loops, and control statements in R. We used some simple and some little difficult examples to showcase the use of the same. But this is just the beginning. With time you will be able to create some very complicated programs using these simple things. In the next chapter, we will look into apply family functions, and we will also learn how to define custom functions.\n"});index.add({'id':16,'href':'/docs/text-mining/topic-modeling-lda/','title':"Topic Modeling",'content':"Under Construction\n"});index.add({'id':17,'href':'/docs/foundational-algorithms/Binary-Logistics-Regression/','title':"Binary Logistic Regression",'content':" Logistics Regression is used to explain the relationship between the categorical dependent variable and one or more independent variables. When the dependent variable is dichotomous, we use binary logistic regression. However, by default, a binary logistic regression is almost always called logistics regression.\n Overview - Logistic Regression The logistic regression model is used to model the relationship between a binary target variable and a set of independent variables. These independent variables can be either qualitative or quantitative. In logistic regression, the model predicts the logit transformation of the probability of the event. The following mathematical formula is used to generate the final output.\n\r\rIn the above equation, p represents the odds ratio, and the formula for the odds ratio is as given below:\n\r\rCase Study - What is UCI Adult Income ?  In this tutorial, we will be using Adult Income data from the UCI machine learning repository to predict the income class of an individual based upon the information provided in the data. You can download this Adult Income data from the UCI repository.\nBeta coefficient in logistics regression are chosen based upon maximum likelihood estimates.\nThe idea here is to give you a fair idea about how a data scientist or a statistician builds a predictive model. So, we will try to demonstrate all the essential tasks which are part of model building exercise. However, for the demo purpose, we will be using only three variables from the whole dataset.\nGetting the data The adult dataset is fairly large, and to read it faster, I will be using read_csv() from readr package to load the data from my local machine.\nlibrary(readr)\radult \u0026lt;- read_csv(\u0026quot;./static/data/adult.csv\u0026quot;)\r# Checking the structure of adult data\rstr(adult)\r# Output\rClasses ‘tbl_df’, ‘tbl’ and 'data.frame': 48842 obs. of 15 variables:\r$ Age : int 25 38 28 44 18 34 29 63 24 55 ...\r$ Workclass : chr \u0026quot;Private\u0026quot; \u0026quot;Private\u0026quot; \u0026quot;Local-gov\u0026quot; \u0026quot;Private\u0026quot; ...\r$ Fnlwgt : int 226802 89814 336951 160323 103497 198693 227026 104626 369667 104996 ...\r$ Education : chr \u0026quot;11th\u0026quot; \u0026quot;HS-grad\u0026quot; \u0026quot;Assoc-acdm\u0026quot; \u0026quot;Some-college\u0026quot; ...\r$ Education-num : int 7 9 12 10 10 6 9 15 10 4 ...\r$ Marital-status: chr \u0026quot;Never-married\u0026quot; \u0026quot;Married-civ-spouse\u0026quot; \u0026quot;Married-civ-spouse\u0026quot; \u0026quot;Married-civ-spouse\u0026quot; ...\r$ Occupation : chr \u0026quot;Machine-op-inspct\u0026quot; \u0026quot;Farming-fishing\u0026quot; \u0026quot;Protective-serv\u0026quot; \u0026quot;Machine-op-inspct\u0026quot; ...\r$ Relationship : chr \u0026quot;Own-child\u0026quot; \u0026quot;Husband\u0026quot; \u0026quot;Husband\u0026quot; \u0026quot;Husband\u0026quot; ...\r$ Race : chr \u0026quot;Black\u0026quot; \u0026quot;White\u0026quot; \u0026quot;White\u0026quot; \u0026quot;Black\u0026quot; ...\r$ Sex : chr \u0026quot;Male\u0026quot; \u0026quot;Male\u0026quot; \u0026quot;Male\u0026quot; \u0026quot;Male\u0026quot; ...\r$ Capital-gain : int 0 0 0 7688 0 0 0 3103 0 0 ...\r$ Capital-loss : int 0 0 0 0 0 0 0 0 0 0 ...\r$ Hours-per-week: int 40 50 40 40 30 30 40 32 40 10 ...\r$ Native-country: chr \u0026quot;United-States\u0026quot; \u0026quot;United-States\u0026quot; \u0026quot;United-States\u0026quot; \u0026quot;United-States\u0026quot; ...\r$ Class : chr \u0026quot;\u0026lt;=50K\u0026quot; \u0026quot;\u0026lt;=50K\u0026quot; \u0026quot;\u0026gt;50K\u0026quot; \u0026quot;\u0026gt;50K\u0026quot; ...\rAs mentioned earlier, we will be using three variables; WorkClass, Marital-status and Age to build the model. Out of these three variables - WorkClass and Marital-status are categorical variables where as Age is a continuous variable.\n# Subsetting the data and keeping the required variables\radult \u0026lt;- adult[ ,c(\u0026quot;Workclass\u0026quot;, \u0026quot;Marital-status\u0026quot;, \u0026quot;Age\u0026quot;, \u0026quot;Class\u0026quot;)]\r# Checking the dim\rdim(adult)\r# Output\r[1] 48842 4\rThe new dataset has 48842 observations and only 4 variables\nWe cannot use categorical variables directly in the model. So for these variables, we need to create dummy variables. A dummy variable takes the value of 0 or 1 to indicate the absence or presence of a particular level. In our example, the function will automatically create dummy variables.\r\rSummarizing categorical variable The best way to summarize the categorical variable is to create the frequency table, and that is what we will do using table function.\n# Generating the frequency table\rtable(adult$Workclass)\r# Output\r? Federal-gov Local-gov Never-worked\r2799 1432 3136 10\rPrivate Self-emp-inc Self-emp-not-inc State-gov\r33906 1695 3862 1981\rWithout-pay\r21\rThe table suggests that there are some 2799 missing values in this variable, which are represented by the (?) symbol. Also, the data is not uniformly distributed. Some of the levels have very few observations and looks like we have an opportunity to combine similar looking levels.\n# Combining levels\radult$Workclass[adult$Workclass == \u0026quot;Without-pay\u0026quot; | adult$Workclass == \u0026quot;Never-worked\u0026quot;] \u0026lt;- \u0026quot;Unemployed\u0026quot;\radult$Workclass[adult$Workclass == \u0026quot;State-gov\u0026quot; | adult$Workclass == \u0026quot;Local-gov\u0026quot;] \u0026lt;- \u0026quot;SL-gov\u0026quot;\radult$Workclass[adult$Workclass == \u0026quot;Self-emp-inc\u0026quot; | adult$Workclass == \u0026quot;Self-emp-not-inc\u0026quot;] \u0026lt;- \u0026quot;Self-employed\u0026quot;\r# Checking the table again\rtable(adult$Workclass)\r# Output\r? Federal-gov Private Self-employed\r2799 1432 33906 5557\rSL-gov Unemployed\r5117 31\rLet us do a similar treatment for our other categorical variable\n# Generating the frequency table\rtable(adult$Marital-status)\r# Output\rDivorced Married-AF-spouse\r6633 37\rMarried-civ-spouse Married-spouse-absent\r22379 628\rNever-married Separated\r16117 1530\rWidowed\r1518\rWe can reduce the above levels to never married, married and never married.\n# Combining levels\radult$Marital-status[adult$Marital-status == \u0026quot;Married-AF-spouse\u0026quot; | adult$Marital-status == \u0026quot;Married-civ-spouse\u0026quot; | adult$Marital-status == \u0026quot;Married-spouse-absent\u0026quot;] \u0026lt;- \u0026quot;Married\u0026quot;\radult$Marital-status[adult$Marital-status == \u0026quot;Divorced\u0026quot; |\radult$Marital-status == \u0026quot;Separated\u0026quot; |\radult$Marital-status == \u0026quot;Widowed\u0026quot;] \u0026lt;- \u0026quot;Not-Married\u0026quot;\r# Checking the table again\rtable(adult$Marital-status)\r# Output\rMarried Never-married Not-Married\r23044 16117 9681\rThis variable looks well-distributed then Workclass. Now, we must convert them to factor variables using as.factor() function.\n# Converting to factor variables\radult$Workclass \u0026lt;- as.factor(adult$Workclass)\radult$Marital-status \u0026lt;- as.factor(adult$Marital-status)\radult$Class \u0026lt;- as.factor(adult$Class)\rDeleting the missing values We will first convert all ? to NA and then use na.omit() to keep the complete observation.\n# Converting ? to NA\radult[adult == \u0026quot;?\u0026quot;] \u0026lt;- NA\r# Keeping only the na.omit() function\radult \u0026lt;- na.omit(adult)\rFinally taking a look into the target variable To save time, I will directly be going forward with the bivariate analysis. Let us see how the distribution of age looks for the two income groups.\nlibrary(ggplot2)\rggplot(adult, aes(Age)) +\rgeom_histogram(aes(fill = Class), color = \u0026quot;black\u0026quot;, binwidth = 2)\r\r\rData looks much more skewed for the lower-income people as compared to the high-income group.\nBuilding the Model We will be splitting the data into the test and train using the createDataPartition() function from the caret package in R. We will train the model using the training dataset and predict the values on the test dataset. To train the logistic model, we will be using glm() function.\n# Loading caret library\rrequire(caret)\r# Splitting the data into train and test\rindex \u0026lt;- createDataPartition(adult$Class, p = .70, list = FALSE)\rtrain \u0026lt;- adult[index, ]\rtest \u0026lt;- adult[-index, ]\r# Training the model\rlogistic_model \u0026lt;- glm(Class ~ ., family = binomial(), train)\r# Checking the model\rsummary(logistic_model)\r# Output\rCall:\rglm(formula = Class ~ ., family = binomial(), data = train)\rDeviance Residuals:\rMin 1Q Median 3Q Max -1.6509 -0.8889 -0.3380 -0.2629 2.5834 Coefficients:\rEstimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -0.591532 0.094875 -6.235 0.00000000045227 ***\rWorkclassPrivate -0.717277 0.077598 -9.244 \u0026lt; 0.0000000000000002 ***\rWorkclassSelf-employed -0.575340 0.084055 -6.845 0.00000000000766 ***\rWorkclassSL-gov -0.445104 0.086089 -5.170 0.00000023374732 ***\rWorkclassUnemployed -2.494210 0.766488 -3.254 0.00114 **\r`Marital-status`Never-married -2.435902 0.051187 -47.589 \u0026lt; 0.0000000000000002 ***\r`Marital-status`Not-Married -2.079032 0.045996 -45.200 \u0026lt; 0.0000000000000002 ***\rAge 0.023362 0.001263 18.503 \u0026lt; 0.0000000000000002 ***\r---\rSignif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\r(Dispersion parameter for binomial family taken to be 1)\rNull deviance: 36113 on 32230 degrees of freedom\rResidual deviance: 28842 on 32223 degrees of freedom\rAIC: 28858\rNumber of Fisher Scoring iterations: 5\rInterpreting Logistic Regression Output All the variables in the above output have turned out to be significant(p values are less than 0.05 for all the variables). If you look at the categorical variables, you will notice that n - 1 dummy variables are created for these variables. Here, n represents the total number of levels. The one level which is left is considered as the reference variable, and all other variable levels are interpreted in reference to this level.\n1. Null and Residual deviance - Null deviance suggests the response by the model if we only consider the intercept; lower the value better is the model. The Residual deviance indicates the response by the model when all the variables are included; again, lower the value, better is the model.\n2. (Intercept) - Intercept(β0) indicates the log of odds of the whole population of interest to be on higher-income class with no predictor variables in the model. We can convert the log of odds back to simple probabilities by using sigmoid function.\nSigmoid function, p = exp(-0.591532)/(1+exp(-0.591532))\r The other way is to convert this logit of odds to simple odds by taking exp(-0.591532) = 0.5534. The number indicates that the odds of an individual being in the high-income group decreases by 45% if we have no predictor variables.\n3. WorkclassPrivate - The beta coefficient against this variable is -0.717277. Let us convert this value into odds by taking the exp(-0.717277) = 0.4880795. The value indicates that the odds of an individual with Private work-class being in the high-income group decreases by 52% than the one in a Federal-gov job.\n Out of 5 levels, the Federal-gov level became the reference, and thus all other levels of workclass variables are inferred in comparison to the referenced variable. That is how we interpret the categorical variables.\n 4. Age - The beta coefficient of the age variable is 0.023362, which is in the logit of odds terms. When we convert this to odds by taking exp(0.023362) we get 1.023. The value indicates that as age increase by one more unit, then the odds of an individual being in the high-income group will increase by 2%.\nNote\nOdds value is never negative, and the value of 1 indicates that this variable has no impact on the target variables. If the value is less than one then the value is read as (1 - value) as a decrease in odds and a value greater than one indicates an increase in the odds.\r\rPredicting Dependent Variable(Y) in Test Dataset To predict the target variable in the unseen data, we use predict function. The output of the predict function is the probability.\n# Predicting in the test dataset\rpred_prob \u0026lt;- predict(logistic_model, test, type = \u0026quot;response\u0026quot;)\rEvaluating Logistic Regression Model There are number of ways in which we can validate our logistic regression model. We have picked all the popular once which you can use to evaluate the model. Let\u0026rsquo;s discuss and see how to run those in R.\n1. Classification Table - I would say this one is the most popular validation technique among all the known validation methods of the logistic model. It\u0026rsquo;s basically a contingency table that we draw between the actual values and the predicted values. The table is then used to dig in many other estimates like Accuracy, Misclassification Rate, True Positive Rate, also known as recall, True Negative Rate, and Precision.\nHere is the representation of the contingency table marking essential terms.\n\r\rBefore we create a contingency table, we need to convert the probability into the two levels IE class \u0026lt;=50K and \u0026gt;50K. To get these values, we will be using a simple ifelse() function and will create a new variable in the train data by the name pred_class.\nWe have to repeat the below steps for both the test and train dataset.\nConverting probability to class values in the training dataset # Converting from probability to actual output\rtrain$pred_class \u0026lt;- ifelse(logistic_model$fitted.values \u0026gt;= 0.5, \u0026quot;\u0026gt;50K\u0026quot;, \u0026quot;\u0026lt;=50K\u0026quot;)\r# Generating the classification table\rctab_train \u0026lt;- table(train$Class, train$pred_class)\rctab_train\r# Output\r\u0026lt;=50K \u0026gt;50K\r\u0026lt;=50K 1844 22391\r\u0026gt;50K 1697 6299\rTraining dataset converting from probability to class values # Converting from probability to actual output\rtest$pred_class \u0026lt;- ifelse(pred_prob \u0026gt;= 0.5, \u0026quot;\u0026gt;50K\u0026quot;, \u0026quot;\u0026lt;=50K\u0026quot;)\r# Generating the classification table\rctab_test \u0026lt;- table(test$Class, test$pred_class)\rctab_test\r# Output\r\u0026lt;=50K \u0026gt;50K\r\u0026lt;=50K 9602 784\r\u0026gt;50K 2676 750\rAccuracy Accuracy is calculated by adding the diagonal elements and dividing it by the sum of all the elements of the contingency table. We will also compare the accuracy of the training dataset with the test dataset to see if our results are holding in the unseen data or not.\nAccuracy = (TP + TN)/(TN + FP + FN + TP)\r # Accuracy in Training dataset\raccuracy_train \u0026lt;- sum(diag(ctab_train))/sum(ctab_train)*100\raccuracy_train\r#Output\r[1] 74.7355\rOur logistics model is able to classify 74.7% of all the observations correctly in training dataset.\n# Accuracy in Test dataset\raccuracy_test \u0026lt;- sum(diag(ctab_test))/sum(ctab_test)*100\raccuracy_test\r#Output\r[1] 74.94932\rThe over all correct classification accuracy in test dataset is 74.9% which is comparable to train dataset. This shows that our model is performing good.\n A model is considered fairly good if the model accuracy is greater than 70%.\n Misclassification Rate Misclassification Rate indicates how often is our predicted values are False.\nMisclassification Rate = (FP+FN)/(TN + FP + FN + TP)\r True Positive Rate - Recall or Sensitivity Recall or TPR indicates how often does our model predicts actual TRUE from the overall TRUE events.\nRecall Or TPR = TP/(FN + TP)\r # Recall in Train dataset\rRecall \u0026lt;- (ctab_train[2, 2]/sum(ctab_train[2, ]))*100\rRecall\r# Output\r[1] 21.22311\rTrue Negative Rate TNR indicates how often does our model predicts actual non events from the overall non events.\nTNR = TN/(TN + FP)\r # TNR in Train dataset\rTNR \u0026lt;- (ctab_train[1, 1]/sum(ctab_train[1, ]))*100\rTNR\r#Output\r92.39117\rPrecision Precision indicates how often does your predicted TRUE values are actually TRUE.\nPrecision = TP/FP + TP\r # Precision in Train dataset\rPrecision \u0026lt;- (ctab_train[2, 2]/sum(ctab_train[, 2]))*100\rPrecision\r#Output\r[1] 47.92432\rCalculating F-Score F-Score is a harmonic mean of recall and precision. The score value lies between 0 and 1. The value of 1 represents perfect precision \u0026amp; recall. The value 0 represents the worst case.\nF_Score \u0026lt;- (2 * Precision * Recall / (Precision + Recall))/100\rF_Score\r#Output\r[1] 0.2941839\rROC Curve The area under the curve(AUC) is the measure that represents ROC(Receiver Operating Characteristic) curve. This ROC curve is a line plot that is drawn between the Sensitivity and (1 - Specificity) Or between TPR and TNR. This graph is then used to generate the AUC value. An AUC value of greater than .70 indicates a good model.\nlibrary(pROC)\rroc \u0026lt;- roc(train$Class, logistic_model$fitted.values)\rauc(roc)\r# Output\rArea under the curve: 0.7965\rConcordance Concordance In how many pairs does the probability of ones is higher than the probability of zeros divided by the total number of possible pairs. The higher the values better is the model. The value of concordance lies between 0 and 1.\nSimilar to concordance, we have disconcordance which states in how many pairs the probability of ones was less than zeros. If the probability of ones is equal to 1 we say it is a tied pair.\nlibrary(InformationValue)\rConcordance(logistic_model$y,logistic_model$fitted.values)\r# Output\r$`Concordance`\r[1] 0.7943923\r$Discordance\r[1] 0.2056077\r$Tied\r[1] 0\r$Pairs\r[1] 193783060\r"});index.add({'id':18,'href':'/docs/pre-processing-tasks/Hypothesis-Testing/','title':"Hypothesis Testing",'content':" Hypothesis testing uses concepts from statistics to determine the probability that a given assumption is valid. In this chapter, you will learn about several types of statistical tests, their practical applications, and how to interpret the results of hypothesis testing.\n Overview Through hypothesis testing, one can make inferences about the population parameters by analysing the sample statistics.\nTypically hypothesis testing starts with an assumption or an assertion about a population parameter. For example, you may be interested in validating the claim of Philips that the average life of there bulb 10 years.\nApplications of hypothesis testing   When you want to compare the sample mean with the population mean. For example - You would like to determine if the average life of a bulb from brand X is 10 years or not. In this case, when you want to check if the sample mean represents the population mean, then you should run One Sample t-test.\n  When you want to compare the means of two independent variables. One of which can be a categorical variable. In this case, we run Two sample t-test.\n  When you want to compare the before and after-effects of an experiment or a treatment. Then, in that case, we run Paired t-test.\n  When you want to compare more than two independent variables; in that case, we run ANOVA test\n  In all the above applications, we assumed that variables are numeric. However, When you want to compare two categorical variables, we run Chi-square test.\n  Perform One-Sample t-test One sample t-test is a parametric test. It is used when you wish to check if the sample mean represents the population mean or not. It assumes that the data follows a normal distribution.\nset.seed(100)\r# Generating random data with normal distibution\rx \u0026lt;- round(rnorm(30, mean = 10, sd = 1),0)\r# Checking if mean is really 10 years\rt.test(x, mu=10)\r# Output\rOne Sample t-test\rdata: x\rt = 0.17668, df = 29, p-value = 0.861\ralternative hypothesis: true mean is not equal to 10\r95 percent confidence interval:\r9.647473 10.419193\rsample estimates:\rmean of x\r10.03333\rHow to interpret One-Sample t-test results? In the above case,\np-value = 0.861, this value is greater than alpha value, and thus we have to accept the null hypothesis. Here the null hypothesis was that the average life of the bulb is 10. And the alternative hypothesis was that it is not equal to 10.\n95 percent confidence interval: 9.647473 10.419193 - The 95% CI also includes the ten, and thus it is fine to state that the mean value is 10.\nPerform welch two-Sample t-test Two sample t-tests are used to compare the means of two independent quantitative variables. By default, the t.test() function runs a welch test, which is a parametric test. It assumes that the two populations have normal distributions and equal variances.\nIn the below example, we assumed that the x and y are samples taken from populations that follow a normal distribution.\n# Generating two variable x and y\rx \u0026lt;- c(3.80, 5.83, 11.89, 21.04, 12.45, 7.38, 6.97, 6.60)\ry \u0026lt;- c(10.54, 8.88, 9.89, 23.74, 14.65)\r# Checking if mean is really 10 years\rt.test(x,y)\r# Output\rWelch Two Sample t-test\rdata: x and y\rt = -1.2051, df = 7.9346, p-value = 0.2629\ralternative hypothesis: true difference in means is not equal to 0\r95 percent confidence interval:\r-11.796332 3.706332\rsample estimates:\rmean of x mean of y\r9.495 13.540\rHow to interpret two sample t-test results? Here, the null hypothesis is that the mean of x - mean of y = 0 and the alternative hypothesis is that the mean of x - mean of y != 0\nAs p-value(0.2629) is greater than the alpha value(0.05), we accept the null hypothesis and conclude that the mean of x is indeed equal to the mean of y.\n95 percent confidence interval: -11.796332 3.706332 - Also, it is evident that zero did appear in at least 95% of the experiments, and thus we conclude that our decision to accept the null hypothesis is correct.\nPerform a Paired t-test I am taking this example from datasciencebeginners.\nAn educational institute wants to check if their course helps in improving the scores of the students. So what they do is they give a test to a bunch of students before the class started and recorded the scores. After which all these students were trained on the subject and at the end of the course another test was given to the students, and the scores were noted. They now need to understand if the course or training has resulted in better scores.\n# Score of students before and after the course\rbefore \u0026lt;- c(73, 38, 46, 99, 5, 15, 58, 78, 99, 43)\rafter \u0026lt;- c(90, 78, 21, 70, 72, 59, 55, 39, 40, 99)\r# Running the paired ttest\rt.test(before, after, paired = TRUE)\r# Output\rPaired t-test\rdata: before and after\rt = -0.49552, df = 9, p-value = 0.6321\ralternative hypothesis: true difference in means is not equal to 0\r95 percent confidence interval:\r-38.39999 24.59999\rsample estimates:\rmean of the differences\r-6.9\rHow to interpret paired t-test results? The p-value of 0.63 is higher than the alpha value. That means we need to accept the null hypothesis and thus conclude that there is no significant change in test scores.\nPerform ANOVA test ANOVA stands for analysis of variance, and to test this, we run Fishers F-test. We run this test when we want to compare the means of more than two independent variables.\nFor example - we may want to know if the average sepal length across three different flower species is similar or not.\nHere, Null Hypothesis :: μ1 = μ2 = μ3 and, Alternative :: μ1 ≠ μ2 ≠ μ3 or μ1 = μ2 ≠ μ3 or μ1 ≠ μ2 = μ3\n You need to run the post adHoc test in case you reject the null hypothesis. It is done to check if all groups are different, or only one of them is different.\n # Running anova\rresult \u0026lt;- aov(Sepal.Length ~ Species, data = iris)\r# Checking the result\rsummary(result)\r# Output\rDf Sum Sq Mean Sq F value Pr(\u0026gt;F) Species 2 63.21 31.606 119.3 \u0026lt;0.0000000000000002 ***\rResiduals 147 38.96 0.265 ---\rSignif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\rHow to interpret ANOVA test results? The Pr(\u0026gt;F) = \u0026lt;0.0000000000000002 is less than the alpha value. That means we reject the null hypothesis stating that the average sepal length of three different flower species is not the same.\nAs we have rejected the null hypothesis, we now run a post-AdHoc test\r\rRunning post-Adhoc test As part of the post-Adhoc test, We are running the Tukey test.\nTukeyHSD(result) # pass the model output\r# Output\rTukey multiple comparisons of means\r95% family-wise confidence level\rFit: aov(formula = Sepal.Length ~ Species, data = iris)\r$`Species`\rdiff lwr upr p adj\rversicolor-setosa 0.930 0.6862273 1.1737727 0\rvirginica-setosa 1.582 1.3382273 1.8257727 0\rvirginica-versicolor 0.652 0.4082273 0.8957727 0\rEach line of output in the above table can be thought of as an individual independent test run for each pair. The p-value for which is represented by p adj. Comparing the padj value against the alpha value, we conclude that mean of all the three flowers is different.\nPerform Chi-Square test the Chi-sqaure test uses a contingency table to test if the two categorical variables are dependent on each other or not. To run the test, you first need to create a contingency table between the two categorical variables. This table is then passed to the chisq.test() function.\nFor example - Let us check if the treatment and type are dependent on each other in the CO2 dataset.\nHere, the null hypothesis is that they are not dependent And, the alternative is that they are dependent on each other\n# Creating contigency table\rtab \u0026lt;- table(CO2$Type, CO2$Treatment)\r# Running chi-square test\rchisq.test(tab)\r# Output\rPearson's Chi-squared test\rdata: tab\rX-squared = 0, df = 1, p-value = 1\rHow to interpret chi-square test results? We again look for the p-value and compare that with the present alpha value of 0.05. In this case, the p-value is greater than alpha, and thus we accept the null hypothesis. So the conclusion is that the plant and treatment are not dependent on each other.\nWilcoxon Signed Rank Test The Wilcoxon Signed Rank test is a nonparametric test. It is an alternative of one sample t-test when the data is not assumed to follow a normal distribution.\ndata \u0026lt;- c(30, 29, 29, 28, 26, 23, 28, 25, 24, 19)\rwilcox.test(data, mu=35, conf.int = TRUE)\r# Output\rWilcoxon signed rank test with continuity\rcorrection\rdata: data\rV = 0, p-value = 0.005857\ralternative hypothesis: true location is not equal to 35\r95 percent confidence interval:\r23.50001 28.99999\rsample estimates:\r(pseudo)median\r26.49994\rShapiro Test We use the Shapiro test to check if the data follows normal distribution or not. In the Shapiro test, the null hypothesis is that the data has a normal distribution, and the alternative hypothesis is that data does not follow a normal distribution.\nset.seed(123)\rdata \u0026lt;- rnorm(50, mean = 30, sd = 2)\rshapiro.test(data)\r# Output\rShapiro-Wilk normality test\rdata: data\rW = 0.98928, p-value = 0.9279\rAs p-value \u0026gt; 0.05, we accept the null hypothesis, which states that the data is normally distributed. We can confirm that result are correct as we used rnorm function to generate random numbers that follow a normal distribution.\nKolmogorov And Smirnov Test The test is also very famous by the name k-s test. The test is done to check whether two data sets follow the same distribution or not. Here, the null hypothesis is that the distribution of the two samples is the same, and the alternative hypothesis is that the distributions are different.\n# Generating random numbers\rx \u0026lt;- rnorm(100)\ry \u0026lt;- runif(100)\r# Performaing k-s test\rks.test(x, y)\r# Output\rTwo-sample Kolmogorov-Smirnov test\rdata: x and y\rD = 0.54, p-value = 0.0000000000004335\ralternative hypothesis: two-sided\rThe output above suggests that the distribution of x and y is different as p-value \u0026lt; 0.05, and thus we reject the null hypothesis.\nIn this chapter, we looked into different types of statistical tests. We learned when to use them, how to use them, how to interpret results, which R functions to use to run a particular test. In the next chapter, we will learn how to identify and treat missing values using R programming.\n"});index.add({'id':19,'href':'/docs/quick-r-tutorial/functions-and-apply-family/','title':"Its All About Functions",'content':"Functions in R By now you must have figured it out that R programming is not a traditional programming language. The language is a collection of functions that are packed together to form a package. All thanks to the open and free community, which has contributed to over 9000 packages over the years. Today one can find any function to achieve almost any statistical task in R by doing a little research. However, you may still be interested in creating your custom function in R. The custom function defined by the user is termed as User Defined Function in R. In this chapter, we will learn how to create a custom function.\nR Packages - A collection of similar functions Before we move forward and see how to define a custom function, let us see how to download an R package and start using the functions from that package. The task is fairly straightforward. One can use install.packages function to download the R package from CRAN to the local system. Once you have the package downloaded, all you need to do is load the package in your environment to have access to the functions from the package.\nTo load a package you can use either library() or require() function.\r\rThese two functions are a tad different in terms that the require function returns FALSE and gives a warning if the package does not exist. On the other hand, library function returns an error.\r\rGiven the above information, it is recommended to use require() function inside of a function in R. This way, when you call the function without loading the package, the program will return a warning message instead of throwing an error and exiting the program without completing the execution.\nInstalling and loading R Packages # Installing {devtools} package, passing arguments\rinstall.packages(\u0026quot;devtools\u0026quot;, # Package name\rdependencies = TRUE, # Download Pacakes used to build this package\rquiet = TRUE) # Control the info printed on the console\r# Loading a package\rlibrary(devtools)\r# Loading using require()\rrequire(devtools)\rInstalling R Packages from GitHub One can download an R package from the GitHub repository directly by using install_github() function. The function is available in devtools package.\n# Installing Package from github\rinstall_github(\u0026quot;tidyverse/dplyr\u0026quot;)\rHow to create a user-defined function in R A function in R consists of three parts. First is the name of the function. Second, function() keyword for defining the function. Inside this keyword, we pass our parameters. The third is the execution code, and it goes inside the curly brackets.\nFor example, Let’s create a function to add two numbers. This function thus requires two parameters.\n# Creating a function to add two numbers\raddTwoNumber \u0026lt;- function(a = 1, b = 1){\rreturn(a + b)\r}\r# Calling this function\raddTwoNumber(a = 10, b = 10)\r addTwoNumber - is the name of my function function(a = 1, b = 1) - here a and b are the arguments which this function can take. We have also provided one as a default value to both the parameters. return(a + b) - this section return the value after adding a and b.  Defining an anonymous function in R An anonymous function is a function which does not have any name and is also at times called as inline functions. Let’s see how we can create and use these functions. For example - You want to calculate the sum of each column in a data frame by ignoring the NA values in them.\n# Getting column wise sum for each variable\routput \u0026lt;- apply(mtcars, 2, function(x){ sum(x, na.rm = TRUE)})\rApply family functions In the previous example, we just saw an apply function that executes like a for loop but in a vectorized way. That makes the apply family functions faster and easy to write in most cases. Below is a list of my favorite functions from apply family.\nUsing apply function Using apply function, one can apply almost any function to either all the rows or columns. These functions can also be user-defined. In the above code, we saw how to use apply function to apply another function on all the columns. In the next example, we will apply the same function to all the rows. That should return 32 values as we have 32 rows in the mtcars dataset.\n# Getting row-wise sum for each variable\routput \u0026lt;- apply(mtcars, 1, function(x){ sum(x, na.rm = TRUE)})\rUsing lapply and sapply functions The lapply() and sapply() functions are very similar to the apply() function which we just learned. However, these functions apply the requested function only on columns. Also, these two functions are different from each other in terms of the output they produce. The lapply function generates a list output whereas sapply function generates a vector output.\nWorking example In this example, we will achieve the same task of getting sum by each column using sapply() and tapply functions. It will allow us to compare the final outputs of the functions.\n# Using lapply() function\rlapply(mtcars, FUN = function(x){ sum(x, na.rm = TRUE)})\r# Using sapply() function\rsapply(mtcars, FUN = function(x){ sum(x, na.rm = TRUE)})\rUsing tapply functions The tapply() function is useful in applying a function by a grouped variable. The function splits the data by a factor variable and returns the function output by each level.\nWorking example In this example, we will see how to get the average sepal length of flowers by the species.\n# Using tapply function to get the average\r# sepal length by flower species\rtapply(iris$Sepal.Length, iris$Species, mean)\rIn this chapter, we reviewed the concept of functions, packages, and learned how to use some of the important apply family functions. Now you’ve gathered enough knowledge about R tool and are ready to enter the exciting world of data analysis! In last chapters, we intend to create a list of useful functions for working with data objects. This will be a continuous effort and we request you to contribute to the same.\n"});index.add({'id':20,'href':'/docs/foundational-algorithms/Multinomial-Logistic-Regressions/','title':"Multinomial Logistic Regression",'content':" Multinomial logistic regression is used when the target variable is categorical with more than two levels. It is an extension of binomial logistic regression.\n Overview - Multinomial Regression Multinomial regression is used to predict the nominal target variable. In case the target variable is of ordinal type, then we need to use ordinal logistic regression. In this tutorial, we will see how we can run multinomial logistic regression. As part of data preparation, ensure that data is free of multicollinearity, outliers, and high influential leverage points.\nCase Study - What is UCI Breast Tissue? In this tutorial, we will be using Breast Tissue data from UCI machine learning repository the classification of breast tissue. Originally, the breast tissues have been classified into 6 groups.\nHowever, we will merge the fibro-adenoma, mastopathy, and glandular classes as their discrimination are not important. Check the tutorial on Dataframe Manipulations to learn about the merging of levels and other tasks related to dataframe in R programming.\nReading the breast tissue data library(readr)\rtissue \u0026lt;- read_csv(\u0026quot;./static/data/BreastTissue.csv\u0026quot;)\r# Checking the structure of adult data\rstr(tissue)\r# Output\rClasses ‘tbl_df’, ‘tbl’ and 'data.frame': 106 obs. of 11 variables:\r$ Case #: int 1 2 3 4 5 6 7 8 9 10 ...\r$ Class : chr \u0026quot;car\u0026quot; \u0026quot;car\u0026quot; \u0026quot;car\u0026quot; \u0026quot;car\u0026quot; ...\r$ I0 : num 525 330 552 380 363 ...\r$ PA500 : num 0.187 0.227 0.232 0.241 0.201 ...\r$ HFS : num 0.0321 0.2653 0.0635 0.2862 0.2443 ...\r$ DA : num 229 121 265 138 125 ...\r$ Area : num 6844 3163 11888 5402 3290 ...\r$ A/DA : num 29.9 26.1 44.9 39.2 26.3 ...\r$ Max IP: num 60.2 69.7 77.8 88.8 69.4 ...\r$ DR : num 220.7 99.1 253.8 105.2 103.9 ...\r$ P : num 557 400 657 494 425 ...\rCombining levels of target variable and deleting the case # as it is a unique variable.\ntissue \u0026lt;- tissue[, -1]\rtissue$Class \u0026lt;- as.factor(tissue$Class)\rlevels(tissue$Class)[levels(tissue$Class) %in% c(\u0026quot;fad\u0026quot;, \u0026quot;gla\u0026quot;, \u0026quot;mas\u0026quot;)] \u0026lt;- \u0026quot;other\u0026quot;\rlevels(tissue$Class)\r# Output\r[1] \u0026quot;adi\u0026quot; \u0026quot;car\u0026quot; \u0026quot;con\u0026quot; \u0026quot;other\u0026quot;\rBuilding Multinomial Regression Model We will be predicting Class of the breast tissue using Breast Tissue data from the UCI machine learning repository.\nSplitting the data in train and test #Splitting the data using a function from dplyr package\rlibrary(caret)\rindex \u0026lt;- createDataPartition(tissue$Class, p = .70, list = FALSE)\rtrain \u0026lt;- tissue[index,]\rtest \u0026lt;- tissue[-index,]\rSetting the reference level Unlike binary logistic regression in multinomial logistic regression, we need to define the reference level. Please note this is specific to the function which I am using from nnet package in R. There are some functions from other R packages where you don\u0026rsquo;t really need to mention the reference level before building the model.\n# Setting the reference\rtrain$Class \u0026lt;- relevel(train$Class, ref = \u0026quot;adi\u0026quot;)\rTraining the multinomial classification model To train the model, we will be using multinom function from nnet package. Once the model is trained, then we will use the summary() function to check the model coefficients.\nrequire(nnet)\r# Training the multinomial model\rmultinom_model \u0026lt;- multinom(Class ~ ., data = tissue)\r# Checking the model\rsummary(multinom_model)\r# Output\rCall:\rmultinom(formula = Class ~ ., data = tissue)\rCoefficients:\r(Intercept) I0 PA500 HFS DA\rcar 86.73299 -1.2415518 34.805551 -31.338876 -3.3819409\rcon 65.23130 -0.1313008 3.504613 5.178805 0.6902009\rother 94.25666 -0.7356228 -9.929850 47.648766 -0.7567586\rArea `A/DA` `Max IP` DR P\rcar -0.01439290 -0.6831729 3.1996740 3.9293080 0.92505697\rcon -0.01189647 2.3845927 0.4270486 -0.1631782 -0.03680047\rother -0.01590076 1.6362184 0.8789358 1.3702359 0.51944534\rStd. Errors:\r(Intercept) I0 PA500 HFS\rcar 0.038670563 0.11903866 0.019490301 0.0378583878\rcon 0.001741294 0.09413872 0.000350851 0.0005646109\rother 0.038878992 0.11367934 0.019507244 0.0378385159\rDA Area `A/DA` `Max IP` DR\rcar 0.65220286 0.02077879 0.55684441 0.4874646 0.6119659\rcon 0.09834323 0.01535892 0.06383315 0.4114490 0.1080821\rother 0.65428634 0.02068448 0.55666884 0.4881945 0.6102327\rP\rcar 0.1770202\rcon 0.1120580\rother 0.1728985\rResidual Deviance: 8.24364\rAIC: 68.24364\rJust like binary logistic regression, we need to convert the coefficients to odds by taking the exponential of the coefficients.\nexp(coef(multinom_model))\rThe predicted values are saved as fitted.values in the model object. Let\u0026rsquo;s see the top 6 observations.\nhead(round(fitted(multinom_model), 2))\r# Output\radi car con other\r1 0 0.97 0 0.03\r2 0 1.00 0 0.00\r3 0 1.00 0 0.00\r4 0 1.00 0 0.00\r5 0 1.00 0 0.00\r6 0 0.97 0 0.03\rThe multinomial regression predicts the probability of a particular observation to be part of the said level. This is what we are seeing in the above table. Columns represent the classification levels and rows represent the observations. This means that the first six observation are classified as car.\nPredicting \u0026amp; Validating the model To validate the model, we will be looking at the accuracy of the model. This accuracy can be calculated from the classification table.\n# Predicting the values for train dataset\rtrain$ClassPredicted \u0026lt;- predict(multinom_model, newdata = train, \u0026quot;class\u0026quot;)\r# Building classification table\rtab \u0026lt;- table(train$Class, train$ClassPredicted)\r# Calculating accuracy - sum of diagonal elements divided by total obs\rround((sum(diag(tab))/sum(tab))*100,2)\r# Output\r[1] 98.68\rOur model accuracy has turned out to be 98.68% in the training dataset.\nPredicting the class on test dataset. # Predicting the class for test dataset\rtest$ClassPredicted \u0026lt;- predict(multinom_model, newdata = test, \u0026quot;class\u0026quot;)\r# Building classification table\rtab \u0026lt;- table(test$Class, test$ClassPredicted)\rtab\r# Output\radi car con other\radi 6 0 0 0\rcar 0 6 0 0\rcon 0 0 4 0\rother 0 0 0 14\rWe were able to achieve 100% accuracy in the test dataset and this number is very close to train, and thus we conclude that the model is good and is also stable.\nIn this tutorial, we learned how to build the multinomial logistic regression model, how to validate and make a prediction on the unseen dataset.\n"});index.add({'id':21,'href':'/docs/pre-processing-tasks/Correlation/','title':"Correlation",'content':"Overview Correlation coefficients are used to describe the degree of association between quantitative variables. The value of the correlation lies between +1 to -1. The signs only indicate the direction of the relationship. That means a +0.86 value is equal to -0.86. However, -ve sign indicates that if one variable increases, the other decreases, and +ve sign indicates that if one variable increases, the other also increases. A value in the range of +0.20 to -0.20 indicates very week or no correlation.\nR Programming supports a variety of correlations. However, we will only be discussing Pearson, Spearman, and Kendall correlation, as these are used most of the time. You can briefly learn about the correlation and types, HERE\nThe cor function produces all the correlation, as mentioned above. Although the cor function finds the correlation for a matrix, it does not provide any information related to the statistical significance of the association. If you are interested in that, you can use corr.test function.\nQuick Look - Types of correlation Let us quickly learn when to use which correlation.\n1. Pearson correlation - Pearson correlation is used when we want to assess the degree of association between two quantitative variables.\ncor(x, method = \u0026quot;pearson\u0026quot;)\r2. Spearman correlation - Use spearman correlation when you want to assess the degree of association between rank-ordered variables.\ncor(x, method = \u0026quot;spearman\u0026quot;)\r3. Kendall’s correlation - Kendall\u0026rsquo;s correlation can also be used to assess the degree of association between rank-ordered variables. However, it is a non-parametric measure.\ncor(x, method = \u0026quot;kendall\u0026quot;)\rGenerate a correlation matrix One can generate a correlation matrix given any correlation type using cor function. However, ensure that you have carefully looked into the data type. That will ensure that you produce the correct results using appropriate correlation. Let us generate a correlation between the variables of iris data.\n# Computing correlation\rcorMat \u0026lt;- cor(x= iris[, -5], method = \u0026quot;pearson\u0026quot;)\r# Rounding the values to two decimal\rround(corMat, 2)\r# Output\rSepal.Length Sepal.Width Petal.Length Petal.Width\rSepal.Length 1.00 -0.12 0.87 0.82\rSepal.Width -0.12 1.00 -0.43 -0.37\rPetal.Length 0.87 -0.43 1.00 0.96\rPetal.Width 0.82 -0.37 0.96 1.00\rThe above matrix suggests that Petal.Width and Sepal.Length have a high correlation. Similarly, you find other variables that show a high correlation between each other.\nThe same function can be used to print the correlation matrix between the two ranked variables. For example, we can calculate the correlation between the cylinder type and gear. Both of these variables are rank variables.\n# Looking into the spearman correlation\rcor(mtcars[, c(\u0026quot;cyl\u0026quot;, \u0026quot;gear\u0026quot;)], method = \u0026quot;spearman\u0026quot;)\r# Output\rcyl gear\rcyl 1.0000000 -0.5643105\rgear -0.5643105 1.0000000\rTesting correlation for significance To check the statistical significance of the correlation, we can use cor.test function. The function generates the p-value which, when compared to alpha value, reveals if the correlation is statistically significant or not.\nAccording to the decision rule, if p-value is less than alpha(0.05) we reject the null hypothesis. Here null hypothesis is - that correlation between the two variables is equal to zero.\r\r# Checking significance of correlation\rcor.test(mtcars$mpg, mtcars$disp)\r# Output\rPearson's product-moment\rcorrelation\rdata: mtcars$mpg and mtcars$disp\rt = -8.7472, df = 30,\rp-value = 0.000000000938\ralternative hypothesis: true correlation is not equal to 0\r95 percent confidence interval:\r-0.9233594 -0.7081376\rsample estimates:\rcor\r-0.8475514\rBased on the above test results, we conclude that the correlation between mileage and displacement variable is significant.\nVisualizing correlation Matrix A visualization is a powerful tool. It speeds up the process of understanding and digesting the critical points. As your dataset grows, it gets more and more challenging to go through the numbers present in your correlation matrix. So the best way to represent your insights about the relationship between variables is through correlation charts.\nWe are sharing some of the examples below, and that means you can use whatever suits your needs. For building these graphs, we are using a package called corrgram and corrplot. If you don\u0026rsquo;t have these R Package, then use install.packages() to install it on your local system.\nExample 1 - Visualizing corrgram # loading package\rrequire(corrgram)\rcorrgram(mtcars[, c(\u0026quot;mpg\u0026quot;, \u0026quot;wt\u0026quot;, \u0026quot;disp\u0026quot;, \u0026quot;hp\u0026quot;, \u0026quot;qsec\u0026quot;)], order=TRUE)\r\r\rIn the above graph:\nThe Red Shade indicates a negative correlation between the variables, darker the shade stronger the association.\nThe Blue Shade indicates a positive correlation between the variables, darker the shade stronger the association.\nExample 2 - Visualizing correlation matrix using corrplot There are seven different shapes, or you can say ways in which you can represent the information - “pie”, “circle”, “square”, “number”, “ellipse”, “shade”, “color”.\n The first argument to the function is a correlation matrix\n # loading library\rrequire(corrplot)\r# Generating correlation matrix\rcorMat \u0026lt;- cor(mtcars[, c(\u0026quot;mpg\u0026quot;, \u0026quot;wt\u0026quot;, \u0026quot;disp\u0026quot;, \u0026quot;hp\u0026quot;, \u0026quot;qsec\u0026quot;)])\r# Building the correlation plot\rcorrplot(corMat, method=\u0026quot;pie\u0026quot;)\r\r\rExample 3 - Changing the shape to a square # Generating correlation matrix\rcorMat \u0026lt;- cor(mtcars[, c(\u0026quot;mpg\u0026quot;, \u0026quot;wt\u0026quot;, \u0026quot;disp\u0026quot;, \u0026quot;hp\u0026quot;, \u0026quot;qsec\u0026quot;)])\r# Building the correlation plot\rcorrplot(corMat, method=\u0026quot;square\u0026quot;)\r\r\rExample 4 - Representing the correlation information using numbers # Generating correlation matrix\rcorMat \u0026lt;- cor(mtcars[, c(\u0026quot;mpg\u0026quot;, \u0026quot;wt\u0026quot;, \u0026quot;disp\u0026quot;, \u0026quot;hp\u0026quot;, \u0026quot;qsec\u0026quot;)])\r# Building the correlation plot\rcorrplot(corMat, method=\u0026quot;number\u0026quot;)\r\r\rExample 5 - Changing the layout of the correlation graph So far, we have been drawing the full correlation matrix. However, as we know, that the upper triangle matrix and lower triangle matrix are similar. So you can choose to represent only one half of the table.\n# Generating correlation matrix\rcorMat \u0026lt;- cor(mtcars[, c(\u0026quot;mpg\u0026quot;, \u0026quot;wt\u0026quot;, \u0026quot;disp\u0026quot;, \u0026quot;hp\u0026quot;, \u0026quot;qsec\u0026quot;)])\r# Building the correlation plot\rcorrplot(corMat, method=\u0026quot;circle\u0026quot;, type = \u0026quot;upper\u0026quot;)\r\r\rIn this chapter, we learned about functions in R programming to generate the correlation coefficient. We also looked into how to check if the correlation is statistically significant. Finally, We learned about packages in R using which we can create beautiful visualizations to present the correlation matrix. In the next chapter, we will learn about the applications of statistical hypothesis testing.\n"});index.add({'id':22,'href':'/docs/quick-r-tutorial/dataframe-manipulations/','title':"Dataframe Manipulations",'content':"Data Manipulation Tasks With Example Code As a data analyst, you will be working mostly with data frames. And thus, it becomes vital that you learn, understand, and practice data manipulation tasks. This chapter will focus on listing down some of the most common data manipulation tasks along with example code.\nBefore, we start and dig into how to accomplish tasks mentioned above. Let\u0026rsquo;s see how to access the datasets which come along with the R packages.\r\rYou can get the list of all the datasets by using data() function and to get the list of datasets belonging to a particular package use argument data(package = \u0026quot;MASS\u0026quot;) function.\nStructure of a data frame With the structure of a data frame, we mean how to check the dimension of a data frame. The total number of observations present in a data frame and things like that. This information can be extracted by using dim()and str() functions. The str() function gives much more detailed information as compared to dim() function.\n# Getting number of rows and columns - dimension\rdim(iris)\r# Getting more elaborate information\rstr(iris)\rGetting column and row names To print the column and row names, you can use the colnames() and rownames() function. There exist another function called names() that can get the names of the columns. Both functions return a character vector.\n# Printing columns names\rcolnames(iris)\r# Printing columns names\rnames(iris)\r# Getting more elaborate information\rrownames(mtcars)\rConverting row names to columns The function add_rownames() from dplyr can add row names as a column into the data frame. This function will add the new column at the beginning of the data frame.\nlibrary(dplyr)\r# Adding row names as a column in mtcars\rmtcars \u0026lt;- add_rownames(mtcars, \u0026quot;carNames\u0026quot;)\rThe above task can also be accomplished by using functions like rownames() and cbind() from base R package. So let us see how one can achieve the desired output using these functions.\n# Resetting the data to the original form\rdata(mtcars)\r# Extracting row names\rnames \u0026lt;- rownames(mtcars)\r#Setting the row names to NULL - deleting basically\rrownames(mtcars) \u0026lt;- NULL\r# Combining the row names back to the mtcars dataset\rmtcars \u0026lt;- cbind(mtcars,names)\rAccessing columns and rows in a data frame A row or a column of a data frame can be accessed using either the index or name. For example,\nExtract mileage data from mtcars dataset\n# Using index\rmtcars[, 1] # mileage is the first columns\r# Using column name\rmtcars[, \u0026quot;mpg\u0026quot;]\r# Using dollar notation(to call by variable name)\rmtcars$mpg\rExtract two columns - mileage and cyl from mtcars dataset\n# Using index\rmtcars[, c(1,2)] # mileage is the first columns\r# Using column name\rmtcars[, c(\u0026quot;mpg\u0026quot;, \u0026quot;cyl\u0026quot;)]\rExtract data at row level - Hornet 4 Drive from mtcars dataset\n# Using index\rmtcars[4, ] # Hornet 4 Drive is the fourth observation\r# Using index for calling multiple rows\rmtcars[c(3,4), ] # Hornet 4 Drive is the fourth observation\r# Using row name\rmtcars[\u0026quot;Hornet 4 Drive\u0026quot;,]\r# Using row nams for calling multiple observations\rmtcars[c(\u0026quot;Datsun 710\u0026quot;,\u0026quot;Hornet 4 Drive\u0026quot;),]\rRenaming and rearranging columns of a data frame One can rename the columns using colnames() function. The same task can be achieved through rename() function from dplyr package in R.\n# Renaming all the columns\rcolnames(cars) \u0026lt;- c(\u0026quot;carSpeed\u0026quot;, \u0026quot;distanceCovered\u0026quot;)\r# Renaming only 2nd column\rcolnames(cars)[2] \u0026lt;- \u0026quot;distCovered\u0026quot;\r# Using rename() function from dplyr\rrequire(dplyr)\riris = rename(iris, flowerSpecies = Species, SepalLength = Sepal.Length)\rColumns of a data frame can be rearranged by using column names or by using column index, as shown in the code snippet.\r\rdata(cars)\r# Checking current column names\rnames(cars)\r# Reordering by using column names\rcars \u0026lt;- cars[c(\u0026quot;dist\u0026quot;, \u0026quot;speed\u0026quot;)]\r# Reordering by using column index\rcars \u0026lt;- cars[c(2, 1)]\rHow to identify missing values You can look for missing values by column or by row. To check by column we can use describe() function from Hmisc R package or summary() function from stats R package. The describe() function returns a column called as missing, whereas summary() function indicates the presence of the missing values by NA count.\nlibrary(Hmisc)\r# using describe() function\rdescribe(mtcars)\r# Using summary() function\rsummary(mtcars)\rApart from these two functions you also use is.na() function along with apply() and any() function to identify if a variable contains missing value or not. If a variable contains a missing value it will return TRUE else it will return FALSE.\n# Column wise missing value check\rapply(mtcars, 2, function(x){any(is.na(x))})\r# Row wise missing value check\rapply(mtcars, 1, function(x){any(is.na(x))})\rHow to filtering missing values To filter missing values we can make use of is.na() function or we can also use a function named complete.cases(). Lets us see how to use these functions.\n# Using is.na to get complete observations\rCO2[!is.na(CO2), ]\r# Using complete.cases to get complete rows\rCO2[complete.cases(CO2), ]\rHow to impute missing values There are many ways in which one can fill the missing values. However, in this section, we will learn how to replace missing values by mean and how to use a machine learning algorithm like KNN or Random Forest to impute the missing values.\n# Filling missing values with Mean\riris$Sepal.Length[is.na(iris$Sepal.Length)] = mean(iris$Sepal.Length, na.rm=TRUE)\r# alternative way is to use ifelse\riris = transform(iris, y = ifelse(is.na(Sepal.Length), mean(Sepal.Length, na.rm=TRUE), Sepal.Length))\r# Using knn imputation to fill missing values\rlibrary(bnstruct)\rknn.impute(iris, k = 3, cat.var = 1:ncol(iris),\rto.impute = 1:nrow(iris),\rusing = 1:nrow(iris))\r# Using random forest to fill missing value\rset.seed(50)\riris \u0026lt;- rfImpute(Species ~ ., iris.na)\rSorting variables in a data frame You can sort a column in ascending or descending order. Also, one can use more than one column to sort the dataset. We will be demonstrating how to sort the data frame by using order() and arrange() functions. The arrange() function is from dplyr package in R.\ndata(mtcars)\r# Sorting mtcars by mpg(ascen) variable - order function\rmtcars[order(mtcars$mpg),]\r# Sorting mtcars by mpg(desc) variable - order function\rmtcars[order(mtcars$mpg, decreasing = TRUE),]\r# we can use (-)ve sign\rmtcars[order(-mtcars$mpg),]\r# Sorting by more than two variables\rmtcars[order(mtcars$mpg, mtcars$cyl),]\r# Using arrange function from dplyr package\rrequire(dplyr)\rarrange(mtcars, mpg) # ascending order one variable\rarrange(mtcars, mpg, cyl) # ascending order two variables\rarrange(mtcars, desc(mpg)) # descending order one variable\rRemoving duplicated values from a data frame Duplicate values are mostly considered from a row\u0026rsquo;s perspective, but one may require to identify duplicate columns. In this example, we will see how to remove duplicated rows as well as duplicated columns.\ndata(CO2)\r# Adding duplicated row\rCO2 \u0026lt;- rbind(CO2, CO2[2,])\rnrow(CO2)\r# Removing duplicated rows\rCO2[!duplicated(CO2), ]\rnrow(CO2)\r# Adding duplicate columns for demonstration\rdata(CO2)# Resting the data\rCO2 \u0026lt;- cbind(CO2, CO2[,2])\rcolnames(CO2)[6] \u0026lt;- \u0026quot;Type\u0026quot;\r# Removing duplicated columns\rCO2[, !duplicated(colnames(CO2))]\rdata(CO2) # Resting the data\rCO2 \u0026lt;- cbind(CO2, CO2[,2])\rcolnames(CO2)[6] \u0026lt;- \u0026quot;Type\u0026quot; # naming the column\r# Removing duplicated columns and keeping the last columns\rCO2[, !duplicated(colnames(CO2), fromLast = TRUE)]\rRenaming levels of a factor variable in a data frame To rename levels of factor variable can be tricky but is pretty simple. First, you need to call the levels of a factor variable by using levels() function and then assign new names to the desired levels. Remember, the order of new variable names should be the same.\n# Renaming Mississippi level to Miss\rlevels(CO2$)[2] \u0026lt;- \u0026quot;Miss\u0026quot;\r# Renaming all the levels\rlevels(CO2$) \u0026lt;- c(\u0026quot;Queb\u0026quot;, \u0026quot;Missi\u0026quot;) # follow the order\rGenerating frequency table(one-way, two-way etc) To generate a frequency table use table() or CrossTable() function from gmodels package in R. The output of a CrossTable() functions resembels the output of ctable in SAS. The output also includes Row Totals, Column Totals, Table Total and chi-sqaure contribution information.\n# Building frequency table - univariate\rtable(mtcars$cyl) # ingnors NA\rtable(mtcars$cyl, useNA = \u0026quot;ifany\u0026quot;) # gives freq of NA if present\rtable(mtcars$cyl, useNA = \u0026quot;always\u0026quot;) # always prints NA count in a table if NA is missing 0 count is considered.\r# Building frequency table - Bivariate\rtable(mtcars$cyl, mtcars$am)\r# Building frequency table - multivariate\rtable(mtcars$cyl, mtcars$am, mtcars$gear)\r# Using crosstable function to generate contigency table\rlibrary(gmodels)\rCrossTable(mtcars$cyl, mtcars$am)\rAdding and deleting columns in a data frame The addition and deletion of the new column is a part of a regular job for a data analyst. Let us see how we can add a column called time to the cars dataset by using the distance over speed formula.\n# Adding new column\rcars$time \u0026lt;- round(cars$dist/cars$speed, 1)\r# Deleting a column\rcars$time \u0026lt;- NULL\rMerging, combining and appending data frames You can merge function to combine the two dataframe in R. This function is available in the base package. Using merge function, one can get the inner, outer, left, right, and cross joins.\n# Inner Join\rmerge(x = data1, y = data2, by = \u0026quot;common Variable\u0026quot;)\r# Outer join\rmerge(x = data1, y = data2, by = \u0026quot;common Variable\u0026quot;, all = TRUE)\r# Left outer join\rmerge(x = data1, y = data2, by = \u0026quot;common Variable\u0026quot;, all.x = TRUE)\r# Right outer\rmerge(x = data1, y = data2, by = \u0026quot;common Variable\u0026quot;, all.y = TRUE)\r# Cross join\rmerge(x = data1, y = data2, by = NULL)\rSummarizing data Many different packages in R provide a different set of functions with divergent statistics. Here is the list of functions that you can consume directly.\n summary() function - available in basic R describe() function - there are two different function by same name. One is available in psych package and the other is available in Hmisc package fivenum() function - available in base R - stats package  Summarizing data by grouped variable It is always a good idea to look at the data over different slices. For example, you may want to look at the average mileage of a car by cylinder variable. Below is a list of functions that we will be using to achieve the task mentioned above.\n Using aggregate() function Using tapply() function Using group_by() and summary() function from dplyr package  You can use these functions to aggregate data by multiple variables as well.\r\r# Using aggregate function\raggregate(x = mtcars$mpg, by = list(mtcars$cyl), FUN = \u0026quot;mean\u0026quot;)\r# Using aggregate() to group by more than one variable\raggregate(x = mtcars$mpg, by = list(mtcars$cyl, mtcars$gear), FUN = \u0026quot;mean\u0026quot;)\r# Using tapply function\rtapply(mtcars$mpg, mtcars$cyl, mean)\r# Using functions from dplyr package\r# Loading library\rlibrary(dplyr)\rmtcars %\u0026gt;%\rdplyr::group_by(cyl) %\u0026gt;%\rdplyr::summarize(mean_mileage = mean(mpg))\r# Using multiple variables to group\rmtcars %\u0026gt;%\rdplyr::group_by(cyl, gear) %\u0026gt;%\rdplyr::summarize(mean_mileage = mean(mpg))\rTransforming data between long and wide format While working on data in R, you will realize that a lot of functions expect you to pass data in the long-form. These tasks can be achieved using functions from tidyr package and rehape2 package in R.\nWe will first create two data frames, one with a wide format and one with the long format containing the same information.\nCreating data frame with wide format\nwide \u0026lt;- read.table(header=TRUE, text='\rsubject sex control cond1 cond2\r1 M 7.9 12.3 10.7\r2 F 6.3 10.6 11.1\r3 F 9.5 13.1 13.8\r4 M 11.5 13.4 12.9 ')\r# Coneverting subject variable to factor variable\rwide$subject \u0026lt;- factor(wide$subject)\rCreating data frame with long format\nlong \u0026lt;- read.table(header=TRUE, text='\rsubject sex condition measurement\r1 M control 7.9\r1 M cond1 12.3\r1 M cond2 10.7\r2 F control 6.3\r2 F cond1 10.6\r2 F cond2 11.1\r3 F control 9.5\r3 F cond1 13.1\r3 F cond2 13.8\r4 M control 11.5\r4 M cond1 13.4\r4 M cond2 12.9 ')\r# Coneverting subject variable to factor variable\rlong$subject \u0026lt;- factor(long$subject)\rNote {tidyr} package is just a new interface for the reshape2 package.\r\rUsing gather() function from tidyr package to convert the data from wide to long format. # loading package\rlibrary(tidyr)\rwide_to_long \u0026lt;- gather(wide, condition, measurement, control:cond2, factor_key=TRUE)\rwide_to_long\rUsing spread() function from tidyr package to convert the data from long to wide format. long_to_wide \u0026lt;- spread(long, condition, measurement)\rlong_to_wide\rFunctions which can be used from reshape2 package in R are listed below:\n  melt() function - used for converting wide formate to long format\n  dcast() function - used for converting long formate to wide format\n  Sub setting or filtering a data frame Although the subset of the data (based upon a condition) can be done in many different ways in R. In this chapter, we will be exploring three different approaches or say functions. For example, say you want to take a subset of CO2 data by the Treatment variable. Say you wish to create a new data frame that has all the observations belonging to a nonchilled treatment category.\n# Using square brackets\rsub1 \u0026lt;- CO2[CO2$Treatment == \u0026quot;nonchilled\u0026quot;, ]\r# Using subset function\rsub2 \u0026lt;- subset(CO2, CO2$Treatment == \u0026quot;nonchilled\u0026quot;)\r# Using filter function from dplyr package\rlibrary(dplyr)\rsub3 \u0026lt;- filter(CO2, Treatment == \u0026quot;nonchilled\u0026quot;)\rYou can use logical operators to use more than one condition spread across multiple variables. For example, get the subset of data where Treatment is nonchilled, and Type is Quebec.\n# Using square brackets\rsub1 \u0026lt;- CO2[CO2$Treatment == \u0026quot;nonchilled\u0026quot; \u0026amp; CO2$Type == \u0026quot;Quebec\u0026quot;, ]\r# Using subset function\rsub2 \u0026lt;- subset(CO2, CO2$Treatment == \u0026quot;nonchilled\u0026quot; \u0026amp; CO2$Type == \u0026quot;Quebec\u0026quot; )\r# Using filter function from dplyr package\rlibrary(dplyr)\rsub3 \u0026lt;- filter(CO2, Treatment == \u0026quot;nonchilled\u0026quot;, CO2$Type == \u0026quot;Quebec\u0026quot;)\rInspecting top and bottom rows of a data frame You can use head() and tail() functions to inspect the top and bottom n observations. By default both these function print top 6 and bottom 6 observations. However, you can fetch data for more or less observations.\nhead(mtcars) # default top 6 observations\rhead(mtcars, 10) # prints top 10 observations\rtail(mtcars) # default bottom 6 observations\rtail(mtcars, 10) # prints bottom 10 observations\r"});index.add({'id':23,'href':'/docs/foundational-algorithms/k-means-clustering/','title':"k-Mean Clustering",'content':"Overview Clustering is one of the most popular and widespread unsupervised machine learning method used for data analysis and mining patterns. At its core, clustering is the grouping of similar observations based upon the characteristics. There are multiple approaches for generating clusters of similar objects. However, in this section, you will learn how to build groups based on the k-Means algorithm.\nWhat is k-means clustering? In simple words, k-means clustering is a technique that aims to divide the data into k number of clusters. The method is relatively simple. The principal idea is to define k centers, one representing each cluster. Below is the explanation of the working of the algorithm:\n Randomly place k points (also known as centroids) in data space. The placement of these points can severely impact the final results. Considering that a better choice is to place these points as far as possible. The next step is to calculate the distance between each point and the k centers. Now calculate the distance between each point and centroid. Based on the shortest distance, each point is associated with a particular centroid. Now that we have points assigned to a centroid, you can think of this as early groups. At this point, we re-calculate new centroid as recalibrated centers based on the points within the group. Now the steps from 2 to 4 are repeated in a loop. At each step, the centroid changes its position step by step, and the algorithm stops when centers do not move any further.  What is the objective function The object of the k-means clustering is to minimize the total variance within the clusters and have high variance between the clusters. The total intra-cluster variance objective function (which is also known as the squared error function) is calculated using the following formula:\nAnimated illustration of how the clustering algorithm works Illustrating k-means algorithm\r\rAdvantages of k-means clustering  K- means clustering is simple to implement. It is relatively fast when compared to hierarchal methods. Algorithm scales to large datasets. The algorithm adapts to new examples reasonably easily.  Disadvantages of k-means Clustering  The final results of K-means are dependent on the initial values of K. Although this dependency is lower for small values of K, however, as the K increases, one may be required to figure out a better way of initializing centroids. K-means algorithm is influenced by outliers. The centroids in k-means can be dragged towards outliers, or you may get a complete cluster consisting of outliers instead of being ignored. To avoid that, you should remove or clip outliers before running the clustering. The effectiveness of the k-means algorithm to distinguish between examples decreases as the number of dimensions increases. It happens because of the decrease in the ratio of the standard deviation to the mean distance between examples. K-means is not good when it comes to cluster data with varying sizes and density. A better choice would be to use a gaussian mixture model.  k-means clustering in R You can use kmeans() function to compute the clusters in R. The function returns a list containing different components. Here we are creating 3 clusters on the wine dataset. The data set is readily available in rattle.data package in R. For the illustration purpose, we are using only a few columns. The columns are on different scales, so we are also using scale() function to bring them on the same scales.\nThe kmeans() function can take multiple arguments. For example, centers = 3 or k = 3 will group the data into 3 different groups. You can mention the maximum number of iterations for which algorithm should run before generating the final solution using iter.max =. One of the recommended argument is nstart=. In our case, we have mentioned nstart = 25, which means the algorithm will attempt 25 initial configurations before reporting the best one.\nlibrary(rattle.data)\rdata(wine, package=\u0026quot;rattle.data\u0026quot;)\rwine_subset \u0026lt;- scale(wine[ , c(2:4)])\rwine_cluster \u0026lt;- kmeans(wine_subset, centers = 3,\riter.max = 10,\rnstart = 25)\rUnderstanding k-means clustering output The output of k-means is an output of class kmeans.\n\u0026gt; wine_cluster\rK-means clustering with 3 clusters of sizes 60, 48, 70\rCluster means:\rAlcohol Malic Ash\r1 0.8914655 -0.4522073 0.5406223\r2 0.1470536 1.3907328 0.2534220\r3 -0.8649501 -0.5660390 -0.6371656\rClustering vector:\r[1] 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 3 1 1 1 1 1\r[34] 1 1 1 1 1 3 2 1 2 1 2 3 2 2 1 1 1 3 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3\r[67] 3 3 1 3 3 1 1 1 3 3 3 3 3 2 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\r[100] 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 2 3 3 1 2 2 2 3 3 3 3 2 3 2\r[133] 3 2 3 3 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 1 2 3 2 2 2 1 1 2 2 2 2 1\r[166] 2 2 2 1 2 3 3 1 2 2 2 1 2\rWithin cluster sum of squares by cluster:\r[1] 67.98619 73.71460 111.63512\r(between_SS / total_SS = 52.3 %)\rAvailable components:\r[1] \u0026quot;cluster\u0026quot; \u0026quot;centers\u0026quot; \u0026quot;totss\u0026quot; \u0026quot;withinss\u0026quot; [5] \u0026quot;tot.withinss\u0026quot; \u0026quot;betweenss\u0026quot; \u0026quot;size\u0026quot; \u0026quot;iter\u0026quot; [9] \u0026quot;ifault\u0026quot; Printing the cluster object - We see that as expected, we have 3 clusters comprising of 60, 48, and 70 number of observations. You can also see that we have the average value of three groups by each variable. Cluster vector provides assignment information at the observation level. Here one observation is assigned to group 1, whereas 2nd observation is assigned to the 3rd group and so on. The next output is self-explanatory. It provides within cluster sum of squares value. In the end, we have a list consisting of multiple components. Some of them are explained below.\n cluster - A vector of integers (from 1:k) indicating the cluster to which each point is allocated. centers - A matrix of cluster centres. totss - The total sum of squares. withinss - Vector of within-cluster sum of squares, one component per cluster. tot.withinss - Total within-cluster sum of squares, i.e. sum(withinss). betweenss - The between-cluster sum of squares, i.e. totss-tot.withinss. size - The number of points in each cluster. iter - The number of (outer) iterations.  Visualizing the output of k-means clusters in R To visualize the output of the three clusters, we will use fviz_cluster() from the factoextra package. The function not just provides a nice visualization but also converts the input information to PCA(principal components) if there are more than two variables. Once we have the principal components, a graph is generated between the first two components showing how much variance is explained by each component.\nlibrary(factoextra)\rfviz_cluster(wine_cluster, data = wine_subset)\r\r\rAnother useful way of visualizing clusters is to illustrate groups by the original variable. Here we will use with() and pair() function to generate a pairwise plot showing each group by color.\n\r\rHow to select the initial value of K in k-Means The initial value of k can be selected using the below mentioned rule of thumb. If you have N number of observation in your dataset then the initial value of K can be selected using:\n \r\r\\(\rK = \\sqrt{2/N}\r\\)\r\rHow to select an optimal number of clusters in k-Means To choose the optimal number of clusters, we can use elbow method. The method comprises of plotting a graph explaining the percentage of variance by each cluster. The expectation is that with-in group heterogeneity decreases as the number of clusters increases. The implementation of the method involves fitting k-means for different values of K. The part of the chart, which resembles an \u0026ldquo;elbow\u0026rdquo; is a good indication of the best model.\n\r\rTo generate the elbow plot, we are utilizing a user-defined function. I took this function from the stack overflow along time back.\n# Function to compute total within-cluster sum of square\rwssplot \u0026lt;- function(data, nc=15, seed=1234){\rwss \u0026lt;- (nrow(data)-1)*sum(apply(data,2,var))\rfor (i in 2:nc){\rset.seed(seed)\rwss[i] \u0026lt;- sum(kmeans(data, centers=i)$withinss)}\rplot(1:nc, wss, type=\u0026quot;b\u0026quot;, xlab=\u0026quot;Number of Clusters\u0026quot;,\rylab=\u0026quot;Within groups sum of squares\u0026quot;)\r}\r# plotting values for each cluster starting from 1 to 9\rwssplot(wine_subset, nc = 9)\r\r\r"});index.add({'id':24,'href':'/docs/pre-processing-tasks/Analysing-zero-variance-predictor/','title':"Analysing zero variance predictor",'content':" Some variables in the dataset contain very little information because they mostly consist of a single value (e.g. zero). These variables are called a zero variance variable.\n Overview Many a time it happens that in the dataset, we have variables which either have unique values or have only a handful of unique values. The variables with only one exceptional value, when passed to fit the model, can cause problems like unstable models or, in some cases, can also cause the model to crash. Such predictor variables are called as Zero-variance variables.\nThe variables which have a few unique values with low frequency can also lead to problems. For example, in the survey dataset, some of the levels of exercise variable have a low-frequency rate. These variables may become zero-variance predictors during the train/test split or at the time of the cross-validation process. They are also sometimes termed as near-zero variance predictors.\nMethods to identify such variables All such variables which fall into these categories need to be identified and excluded from the datasets. There are many different things one can do to identify these variables like you can check unique values, generate frequency tables, etc. Also, one can use the following metrics to determine the zero variance or near-zero variance predictors –\n  Frequency Ratio – It is the ratio of the most common value over the second most prevalent value. If the value is close to one, then the predictor is good to use. However, a substantial value indicates that the variable is unbalanced and should be eliminated.\n  Percentage of Unique Values – It is calculated by dividing the number of unique values divided by the total number of samples that approaches zero as the granularity of the data increases. Any variable which crosses the predefined threshold for frequency ratio and has a frequency of unique values percentage less than the limit should be considered as zero variance predictor.\n  Not all low granularity variables are near-zero variance predictors. These variables can often have evenly distributed distribution and can lead to false identification of zero variance predictors. To avoid any mistakes, one must use both the metrics mentioned above.\r\rcaret package in R provides a function called nearZeroVar() to identify and to remove such problematic variables before modeling. Some of the arguments which you must be aware or are -\n x = Dataset freqCut = the cutoff for the ratio of the most common value to the second most common value. The default value is set at 95/5 uniqueCut = the cutoff for the percentage of distinct values out of the number of total samples. The default value is 10. saveMetrics = takes logical input. If false, only the position of zero- and near-zero variables is returned. If true, a dataframe with predictor information is returned.\n  The default values mentioned for freqCut and uniqueCut are fairly conservative. So, based upon your requirements, feel free to change these numbers.\n# Identifying near-zero variance variable\rnearZeroVar(iris[, -5], saveMetrics = TRUE)\r#Output\rfreqRatio percentUnique zeroVar nzv\rSepal.Length 1.111111 23.33333 FALSE FALSE\rSepal.Width 1.857143 15.33333 FALSE FALSE\rPetal.Length 1.000000 28.66667 FALSE FALSE\rPetal.Width 2.230769 14.66667 FALSE FALSE\rIdentify zero variance conditioned on group variable checkConditionalX() function can be used to look at the distribution of the columns of x conditioned on the levels of y. This function identifies columns of data that are sparse within groups of y variable.\nset.seed(1)\rclasses \u0026lt;- factor(rep(letters[1:3], each = 30))\rx \u0026lt;- data.frame(x1 = rep(c(0, 1), 45),\rx2 = c(rep(0, 10), rep(1, 80)))\rlapply(x, table, y = classes)\rcheckConditionalX(x, classes)\r# Output\rlapply(x, table, y = classes)\r$x1\ry\ra b c\r0 15 15 15\r1 15 15 15\r$x2\ry\ra b c\r0 10 0 0\r1 20 30 30\rcheckConditionalX(x, classes)\r[1] 2\rIn this chapter, you learned about what is a zero variance and near-zero variance variables. You also learned how to identify such variables conditioned on a grouped variable or at an individual level.\n"});index.add({'id':25,'href':'/docs/pre-processing-tasks/Standardization-MinMax-Scaling/','title':"Standardization And Scaling",'content':" While building a Machine Learning model, do not throw away all of your information! Normalize your features.\n Overview It is a general requirement for many machine learning algorithms to have features with similar scales. It is important because if we pass features with different scales to algorithms like SVMs, perceptron’s, neural networks, optimization algorithms in logistic regressions, and more prominently, you can think of algorithms which use gradient descent will end up having a faster update for some feature values as compared to others. This is because the feature values play an important role in weight updates.\n\r\rBelow is the list of algorithms which get highly impacted by the differing scale of features:\n  K-Nearest Neighbours and K-Means algorithm which uses Euclidean distance as a measure\n  If you are using gradient descent-based optimization in algorithms like logistics regression, neural network, SVM, perceptron etc. we should bring the features on the same scale else some weights will get updated faster when compared to others.\n  In the case of principal component analysis, kernel principal component analysis and linear discriminant analysis as these algorithms intent on finding the direction of maximum variance; you must pass features with similar scales otherwise the algorithm will put much more emphasis on features with the higher measurement as compared to ones with smaller scales.\n  When we wish to bring the features on the same scale, we can broadly do the following:\n Standardization also sometimes called Z-score normalization - When you apply this technique, the features are scaled in such a way that they end up having properties of a standard normal distribution with mean equal to zero and standard deviation of one. We simply calculate the Z-score of each observation in the dataset for the feature.  A Z-Score is calculated using the following formula:\n\r\r\\(\r\\Zeta = (x- \\mu)/\\sigma\r\\)\r\rMin-Max scaling also sometimes refers to Normalization - Often, people confuse the Min-Max scaling with the Z-Score Normalization. In this approach, the data is scaled in such a way that the values usually range between 0 – 1. In contrast to the standardization, the min-max scaling results into smaller standard deviations. What this essentially means is that we will be suppressing the effects of outliers.  A min-max scaling is typically done using the following formula:\n\r\rWhich is better Z-score standardization or Min-Max scaling? Both Standardization and Min-Max scaling have there pros and cons. I am listing these below, and hopefully, this will help you make a call on when to use what.\nMin-max Scaling - All features end up having the same scale, but it does not handle outliers well.\nZ-score Standardization - Handles outliers, but the data produced does not have the exact same scale.\nHow it can be done using R programming The below code we are creating a UDF for min-max scaling. We have created a sored list of numeric values that are randomly generated from the exponential distribution. We then apply our UDF, which is named as MinMaxScaling. You will notice that the first value is zero, and the last value is one as we squeeze the values between 1 and 0.\n# generating random numbers\rnum = sort(rexp(10))\r# Min-Max Scaling function\rMinMaxScaling \u0026lt;- function(x){\rreturn((x-min(x))/(max(x)-min(x)))\r}\r# Calling the function\rMinMaxScaling(num)\r# Output\r[1] 0.00000000 0.07356108 0.08615292 0.12786590\r[5] 0.22440422 0.31220196 0.41876790 0.52747519\r[9] 0.55234810 1.00000000\rFor the Z-score standardization, you can use the scale() from the base R package. The function is very easy to use. And let\u0026rsquo;s just apply the same on the above numerical values. For your reference, we are sharing the minimum, maximum, mean, and standard deviation for the scaled values.\n# Usinf scale function\rscaledValues \u0026lt;- scale(num)\r# Output\r\u0026gt; scaledValues\r[,1]\r[1,] -1.09310641\r[2,] -0.85110980\r[3,] -0.80968594\r[4,] -0.67246119\r[5,] -0.35487550\r[6,] -0.06604399\r[7,] 0.28452994\r[8,] 0.64214835\r[9,] 0.72397370\r[10,] 2.19663083\rattr(,\u0026quot;scaled:center\u0026quot;)\r[1] 0.6872931\rattr(,\u0026quot;scaled:scale\u0026quot;)\r[1] 0.5836444\r\u0026gt; min(scaledValues)\r[1] -1.093106\r\u0026gt; max(scaledValues)\r[1] 2.196631\r\u0026gt; round(mean(scaledValues),0)\r[1] 0\r\u0026gt; round(sd(scaledValues),0)\r[1] 1\rIn this chapter, you learned about different techniques of normalizing your data. When using machine learning algorithms, it is imperative that you bring your variables on the same scale. You might have an amazing dataset with many significant features, but if you do not normalize your features, one of those features might completely dominate the others.\n"});index.add({'id':26,'href':'/docs/pre-processing-tasks/Principal-Component-Analysis/','title':"Principal Component Analysis",'content':" Principal component analysis(PCA) is an unsupervised machine learning technique that is used to reduce the dimensions of a large multi-dimensional dataset without losing much of the information. It is often also used to visualize and explore these high dimensional datasets.\n Overview One of the challenges among others that large datasets present is the time to model or learn the relationship between independent variables and the target variables. Thus it becomes essential for us to reduce the number of variables that we want to pass into the model to predict the target variable. There are many different statistical or machine learning algorithms like SVD, ICA, tSNE, feature selection etc. that we can use to reduce the dimension in order to speed up the model training process. Among these methods, one of the most talked and widely used algorithms is principal component analysis. In this article, we will learn how to use principal component analysis and identify the latent features of any dataset to reduce the dimensions of the dataset without losing much of the variance/information.\nAt a high level, the independent X variables are passed through a function which returns n number of principal components. Here n is the total number of variables in the dataset. Each principal component has hidden information of all the independent variables. However, the first principal component explains maximum variability in the data and each following component for the remaining variance as possible. Thus, the Explainability of the variance is maximum with first and lowest with the last principal component.\nWhen to use Principal Component Analysis In the following scenario, we can use principal components.\n  When you want to reduce the dimensionality of the large dataset.\n  When you want to transform several correlated variables into some uncorrelated variables. Sometimes you may find correlated variables within your independent variables. However, you believe that all variables are important and should not be deleted. In such scenarios you can make use of principal components as principal components are uncorrelated latent variables.\n  The principal components are less interpretable in comparison to actual variables.\r\rIntuitive understanding of Principal Components In layman terms, through principal components, we are actually trying to figure out the magnitude and direction of data. To get these two components, we can only use straight lines that are orthogonal to each other. To understand this, let\u0026rsquo;s consider the below example.\n Imagine that your data points are distributed in the shape of an oval, as shown below. Each dark blue point represents the location of an individual data point in terms of variable x1 and variable x2.  \r\rThe next step is to draw straight lines, which kind of capture the maximum variance in the data. So, let\u0026rsquo;s draw the straight lines and remember these lines need to be orthogonal to each other. The solid green and red lines are the ones that are orthogonal. You can see that the green line clearly captures the maximum information and thus becomes your principal component 1, and the red line becomes your principal component 2.  \r\rNow on each line, imagine the projection of all the data points(represented by dotted lines). These projections on the solid lines are represented by eigenvectors(Intuitively, an eigenvector is a vector whose direction remains unchanged when a linear transformation is applied). So you can see that on both the lines, we actually have some representation of all the data points. The only difference is that the green line captures much more variance of data as compared to the red line.  \r\rWe now transform our data use PC1 and PC2 as are new axis in place of X1 and X2 variables. Further information can be represented linearly on one axis as given below.  \r\rYou now understand that principal component analysis tries to figure out a new coordinate system such that every data point has a new (x,y) value. Thus, each principal component is nothing but a combination of x1 and x2 variables. These are finally arranged in such a way that the first PC gets to explain lots of variation.\r\r2D Example Visualizing Principal Component coordinates \r\rMathematical explanation of principal components In this section, we will look into the mathematical steps involved in calculating principal components. We will not do manual calculations for each and every step. I feel that it is not required because we have many prebuilt packages which can do these calculations in the possible know efficient ways.\n Take all the independent variables, ignoring the target variable. Calculate the mean for every variable. Calculate the covariance matrix of the complete dataset. Calculate the eigenvectors and generate the corresponding eigenvalues. Finally, Sort these eigenvectors in decreasing order. Based upon your variance explained by these components, select top k eigenvectors with the most significant values. Use this new matrix consisting of eigenvalues to transform the data into a new space.  How to run PCA in R For this example, we are using the USDA National Nutrient Database data set. The dataset has 8619 observations and around 48 variables, including both categorical and numeric variables.\nreading the raw dataset We will read the dataset into R and keep only independent variables. For this example, we are only considering numeric variables. The dataset is then divided into train and test. As this is just a working example, we are not doing the random sampling.\nrawdata \u0026lt;- read.csv(\u0026quot;nndb_flat.csv\u0026quot;)\r#Diving the dataset into test and train\rpca.train \u0026lt;- rawdata[1:7000, c(8:45)]\rpca.test \u0026lt;- rawdata[7001: nrow(rawdata), c(8:45)]\rRunning Principal component analysis If the variables in the dataset have widely varying scales, then it is advised to bring all the data points on the same scale. The function I am using provides the functionality of bringing the data values on the same scale. In case you decide to use some other function to ensure that you follow this particular step.\nIt is important to bring variables on the same scale in case they are on different scales. This can be achieved through scaling or standardization techniques.\r\rWe will be using the prcomp() function for generating principal components. This function comes along with the default {stats} package in R. For scaling the variables use scale. argument to do scaling computation.\n# Generating principal components\rprin_comp \u0026lt;- prcomp(pca.train, scale. = T)\rPlotting principal components to understand cumulative variance explained We can use a simple cumulative chart to shortlist the adequate amount of principal components. Generally, a cumulative variance explained up to 95% is considered a good cut-off. However, this number is not a strict guideline. Based upon the requirement, one can select anywhere between 85% to 99%.\n# Extrating and calculating the variance of each PC\rprin_comp$rotation[1:5,1:4]\rstd_dev \u0026lt;- prin_comp$sdev\rpr_var \u0026lt;- std_dev^2\rprop_varex \u0026lt;- pr_var/sum(pr_var)\r# Generating Cumulative point chart\rplot(cumsum(prop_varex), xlab = \u0026quot;Principal Component\u0026quot;,\rylab = \u0026quot;Cumulative Proportion of Variance Explained\u0026quot;,\rtype = \u0026quot;b\u0026quot;)\r\r\rFrom the above graph, it looks like around 96.4% of the variance is explained by just 18 variables.\n# Getting the explained by 18 principal components\rsum(prop_varex[1:18])\r# Output\r[1] 0.9639597\rReplacing actual variables with principal components This step is relatively simple; all we need is to concatenate the principal components to the current dataset using the cbind() function. The actual variables are dropped, and only the target variable is kept in the final training dataset.\nNote: In the below example, we do not have a target variable. However, I request you to include the target variable into the final pca.train dataset while working on an actual dataset.\n# selecting principal components from model\rpca.train \u0026lt;- cbind(pca.train, prin_comp$x)\rpca.train \u0026lt;- as.data.frame(pca.train[, 39:56])\r# Examining the first 4 variables and top 6 obs\rhead(pca.train[0:4])\r# Output\rPC1 PC2 PC3 PC4\r1 1.76851457 -0.3880891 -0.4975637 1.3199199\r2 1.76438516 -0.3953145 -0.5022858 1.3365616\r3 1.73022080 -0.3956281 -0.6435845 1.6293108\r4 -0.37948652 0.1763231 -2.5346533 -0.5315933\r5 -0.40721247 0.2832664 -3.3679989 -0.3626819\r6 0.06090458 -0.4432112 -0.7140177 -0.6492139\rWe can now use this new dataset and run either regression or classification machine learning algorithm based upon the nature of the target variable.\nHow to generate/predict PCA in the test dataset You know that the train and test dataset needs to have the same set of variables. If this condition is not met, then your model will produce an error. You can use regular predict() function to generate principal components in the test dataset, as well.\n# Predicting PC in test dataset\rtest.data \u0026lt;- predict(prin_comp, newdata = pca.test)\r# PCA returns a matrix, so converting back to the data frame\rtest.data \u0026lt;- as.data.frame(test.data[1:18])\rIn this chapter, we learned how to reduce the dimension of a large dataset using principal component analysis. We did by understanding principal components intuitively as well as learning all the mathematical steps involved in generating principal components. Finally, we walked through step by step R programming code and created PC on a USDA National Nutrient Database.\n"});index.add({'id':27,'href':'/docs/pre-processing-tasks/Splitting-Data/','title':"Splitting Data",'content':" To create the best model which generalizes well to new unseen data. You must ensure that your test set serves as a proxy for actual dataset IE it represents the new dataset.\n Overview The very first step after pre-processing of the dataset is to split the data into training and test datasets. We usually split the data around 70%-30% between training and testing stages. The training set is the one that we use to learn the relationship between independent variables and the target variable. This relationship is either stored in terms of mathematical function or is captured as a set of rules.\nWe then use the learnings from the training dataset and test it on the testing dataset. Our results are considered satisfactory if we get comparable results in training and testing datasets.\nWhile we look for this split, we need to make sure that our test set meets the following two conditions:\n Test data should be large enough to yield statistically meaningful results. We need to pick a test set such that it represents the data set as a whole. This means that the characteristics of testing and training datasets should be similar.  Six functions in R for splitting data into train and test 1. Using sample() function set.seed(222)\rsample_size = round(nrow(mtcars)*.70) # setting what is 70%\rindex \u0026lt;- sample(seq_len(nrow(mtcars)), size = sample_size)\rtrain \u0026lt;- mtcars[index, ]\rtest \u0026lt;- mtcars[-index, ]\r2. Using sample.int() function set.seed(222)\rsample \u0026lt;- sample.int(n = nrow(CO2),\rsize = floor(.70*nrow(CO2)), # Selecting 70% of data\rreplace = F)\rtrain \u0026lt;- CO2[sample, ]\rtest \u0026lt;- CO2[-sample, ]\r3. Using sample_n() function from {dplyr} package library(dplyr)\rsample_n(mtcars, 3)\rsample_n(mtcars, 10, replace = TRUE)\r# Using the above function to create 70 - 30 slipt into test and train\rsample_size = round(nrow(iris)*.70) # setting what is 70%\rtrain \u0026lt;- sample_n(iris, sample_size)\rsample_id \u0026lt;- as.numeric(rownames(train)) # rownames() returns character so as.numeric\rtest \u0026lt;- iris[-sample_id,]\r4. Using sample_frac() function from {dplyr} package library(dplyr)\rsample_frac(mtcars, .3)\rsample_frac(mtcars, .3, replace = TRUE)\r# Using the above function to create 70 - 30 slipt into test and train\rtrain \u0026lt;- sample_frac(iris, 0.7)\rsample_id \u0026lt;- as.numeric(rownames(train)) # rownames() returns character so as.numeric\rtest \u0026lt;- iris[-sample_id,]\r5. Using createDataPartition() function from {caret} package library(caret)\rindex = createDataPartition(iris$Species, p = 0.70, list = FALSE)\rtrain = iris[index, ]\rtest = iris[-index, ]\r6. Using sample.split() function from {caTools} package require(caTools)\rset.seed(101)\rsample = sample.split(iris$Species, SplitRatio = .75)\rtrain = subset(iris, sample == TRUE)\rtest = subset(iris, sample == FALSE)\r"});index.add({'id':28,'href':'/posts/7-Useful-Interactive-Charts-in-R-with-sample-code/','title':"7 Useful Interactive Charts in R (with sample code)",'content':"Introduction An interactive chart gives a lot of freedom to both presenter and the audience. Interactive graphs allow you to share complex ideas in a more engaging way. Using interactive charts allows users to perform actions like choosing a variable, zoom-in, zoom-out, hover to get tooltip, take a snapshot, and more. In this section, I will walk you through some of the packages and functions which we can use to plot different types of interactive plots.\nInteractive dendrogram with collapsible tree A collapsible dendrogram is an interactive tree-like chart where when you can click on a node to either reveal the following branch or collapse the current node. Click the button to see an example.\nInteractive Dendrogram Example\r\rYou can use the collapsibleTree package to generate hierarchical visualization from a data frame object without worrying about the nested lists or JSON objects. Here we will use geography data from data.world.\nDownload\r\rlibrary(\u0026quot;collapsibleTree\u0026quot;)\rgeo \u0026lt;- read.csv(\u0026quot;M:/DSB/datasets/Machine-Learning/Geography.csv\u0026quot;)\rdendo \u0026lt;- collapsibleTree(\rgeo,\rhierarchy = c(\u0026quot;region\u0026quot;, \u0026quot;sub_region\u0026quot;),\rwidth = 700,\rzoomable = TRUE\r)\rdendo\r\rInteractive time series charts I find interactive time series charts of particular interest. The Time series graphs provide information about the evolution of one or multiple variables through time. Here we will use the dygraphs package to generate impressive time series charts.\nlibrary(dygraphs)\rchart \u0026lt;- dygraph(AirPassengers)\rchart\rEncourage you to select a section of the chart and zoom in\n\rDue to some technical challenges for some charts I am sharing video clippings for now..\r\rInteractive scatter plot The best way to build an interactive scatter plot from plotly in R is through the use of plot_ly function. Here we are using iris data for creating a scatter plot between Sepal.Length and Petal.width variables.\nlibrary(plotly)\rfig \u0026lt;- plot_ly(data = iris, x = ~Sepal.Length, y = ~Petal.Length)\rfig\rIllustrating k-means algorithm\r\rInteractive area plot An area chart is very close to a line plot. We can build an interactive area plot in plotly using two different functions, plot_ly() and ggplotyly(). Here we will build an area chart showing the density of AirPassengers data using plot_ly() function.\ndensity \u0026lt;- density(AirPassengers)\rfig \u0026lt;- plot_ly(x = ~density$x, y = ~density$y ,\rtype = 'scatter', mode = 'lines', fill = \u0026quot;tozeroy)\rfig\rIllustrating scatter plot\r\rOnce again, we will draw an interactive area chart using the ggplotly() function from the plotly package. The ggplotly() function is a special one as it turns a ggplot2 design version interactive. So if you have a ggplot2 graph already created, the ggplotly() can be very handy. Now, we will first create a static area chart using ggplot function and then will add interactive features to it using ggplotly().\nlibrary(plotly)\rlibrary(ggplot2)\rfig \u0026lt;- ggplot(diamonds, aes(x=carat)) +\rgeom_area(stat = \u0026quot;bin\u0026quot;) +\rtheme_minimal()\r# Turn figure interactive with ggplotly\rfig \u0026lt;- ggplotly(fig)\rfig\rinteractive area chart\r\rInteractive bubble plot Again, the easiest way to draw an interactive bubble plot would be first to use geom_point() function from ggplot2 package to draw the chart and then render the graph to get the interactive version of it.\nlibrary(ggplot2)\rlibrary(plotly)\rmtcars$cyl \u0026lt;- as.factor(mtcars$cyl)\rfig \u0026lt;- ggplot(mtcars ,aes(mpg, wt, size = disp, color=cyl)) +\rgeom_point() +\rtheme_bw()\rfig \u0026lt;- ggplotly(fig)\rfig\rinteractive bubble chart\r\rInteractive heatmaps Heatmaps are a great visualization tool to show the magnitude of information. The graph uses colors to depict the information stored in a matrix. Here we will see how we can use plotly and d3heatmap package to generate heatmaps. The d3heatmap package creates a D3.js-based heatmap widget.\nlibrary(d3heatmap)\rfig \u0026lt;- d3heatmap(mtcars, scale = \u0026quot;column\u0026quot;, colors = \u0026quot;Greens\u0026quot;)\rfig\rinteractive d3 heatmap chart\r\rTo generate heatmap with plotly is simple. Generate heatmap using ggplot2 and render that to the interactive version using ggplotly function. Here we first generate the correlation matrix using cor function. We then use the melt() function from the reshape2 package to bring the correlation matrix into the desired format. It\u0026rsquo;s a little tricky, but worth the shot.\nlibrary(ggplot)\rlibrary(reshape2)\rmatrix \u0026lt;- cor(mtcars)\rmelted_cor_mat \u0026lt;- melt(matrix)\rfig \u0026lt;- ggplot(melted_cor_mat,\raes(x=Var1, y=Var2, fill=value))+\rgeom_tile()+\rtheme_minimal()\rfig \u0026lt;- ggplotly(fig + theme(legend.position = \u0026quot;none\u0026quot;))\rfig\rinteractive ggplot heatmap chart\r\rInteractive network Building a network graph is kind of complicated. The primary reason for complexity comes from the fact that there can be many different input formats. There are also other parameters that you should be aware of, like whether the network you are trying to build is weighted or unweighted, and if it is directed or undirected. We will have a dedicated article on the input formats and talk more about it. Here we will use one of the rather simple forms of data where we have just two columns - Source and Target. Now to build the network graph, we will use the simpleNetwork() function from the networkD3 package in R.\nlibrary(networkD3)\rnetwork \u0026lt;- read.csv(\u0026quot;M:/DSB/datasets/Machine-Learning/network_dummy.csv\u0026quot;)\r# create the network object\r# Plot\rp \u0026lt;- simpleNetwork(network, height=\u0026quot;200px\u0026quot;, width=\u0026quot;200px\u0026quot;, Source = 1,\rTarget = 2,\rfontSize = 18, linkColour = \u0026quot;#777\u0026quot;, nodeColour = \u0026quot;#F47E5E\u0026quot;, opacity = 0.7, zoom = T)\r\rWe hope you learned something new with this article. If so, please leave comment, like or share. You can also let us know what topics you would like us to write and share.\n"});index.add({'id':29,'href':'/docs/quick-r-tutorial/','title':"Quick R Tutorial",'content':""});index.add({'id':30,'href':'/docs/useful-r-packages/','title':"Useful R Packages",'content':""});index.add({'id':31,'href':'/docs/pre-processing-tasks/','title':"Pre Processing Tasks",'content':""});index.add({'id':32,'href':'/docs/foundational-algorithms/','title':"Foundational Algorithms",'content':""});index.add({'id':33,'href':'/docs/text-mining/','title':"Text Mining",'content':""});index.add({'id':34,'href':'/docs/','title':"Docs",'content':""});index.add({'id':35,'href':'/posts/','title':"Blog",'content':""});index.add({'id':36,'href':'/privacy-policy/','title':"Privacy Policy",'content':"Privacy Policy Your privacy is important to us. It is R Statistics Blog\u0026rsquo;s policy to respect your privacy regarding any information we may collect from you across our website, http://www.rstatisticsblog.com/, and other sites we own and operate.\nWe only ask for personal information when we truly need it to provide a service to you. We collect it by fair and lawful means, with your knowledge and consent. We also let you know why we’re collecting it and how it will be used.\nWe only retain collected information for as long as necessary to provide you with your requested service. What data we store, we’ll protect within commercially acceptable means to prevent loss and theft, as well as unauthorised access, disclosure, copying, use or modification.\nWe don’t share any personally identifying information publicly or with third-parties, except when required to by law. Our website may link to external sites that are not operated by us. Please be aware that we have no control over the content and practices of these sites, and cannot accept responsibility or liability for their respective privacy policies.\nYou are free to refuse our request for your personal information, with the understanding that we may be unable to provide you with some of your desired services.\nYour continued use of our website will be regarded as acceptance of our practices around privacy and personal information. If you have any questions about how we handle user data and personal information, feel free to contact us.\nThis policy is effective as of 1 January 2020.\nPrivacy Policy created with GetTerms.\n"});})();