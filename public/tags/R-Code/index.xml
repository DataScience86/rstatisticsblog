<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R Code on R Statistics Blog</title>
    <link>//localhost:1313/tags/R-Code/</link>
    <description>Recent content in R Code on R Statistics Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 30 Apr 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="//localhost:1313/tags/R-Code/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ggplot2</title>
      <link>//localhost:1313/docs/useful-r-packages/ggplot2-tutorial/</link>
      <pubDate>Wed, 15 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/useful-r-packages/ggplot2-tutorial/</guid>
      <description>Overview The ggplot2 package in R provides a reliable system for describing and building graphs. The package is capable of creating elegant and aesthetically pleasing graphics. The framework of ggplot2 is quite different (in comparison to graphics package) and is based on the grammar of graphics(introduced initially by Leland Wilkinson). At first, you may not find it intuitive, but don&amp;rsquo;t worry, we are here to help. Together, we will master it to the core.</description>
    </item>
    
    <item>
      <title>Text Processing</title>
      <link>//localhost:1313/docs/text-mining/text-processing/</link>
      <pubDate>Wed, 15 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/text-mining/text-processing/</guid>
      <description>Overview The ggplot2 package in R provides a reliable system for describing and building graphs. The package is capable of creating elegant and aesthetically pleasing graphics. The framework of ggplot2 is quite different (in comparison to graphics package) and is based on the grammar of graphics(introduced initially by Leland Wilkinson). At first, you may not find it intuitive, but don&amp;rsquo;t worry, we are here to help. Together, we will master it to the core.</description>
    </item>
    
    <item>
      <title>Analysing Outliers</title>
      <link>//localhost:1313/docs/pre-processing-tasks/Identifying-Outliers/</link>
      <pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/pre-processing-tasks/Identifying-Outliers/</guid>
      <description>Outlier is a value that does not follow the usual norms of the data. For almost all the statistical methods, outliers present a particular challenge, and so it becomes crucial to identify and treat them. Let&amp;rsquo;s see which all packages and functions can be used in R to deal with outliers.
 Overview The presence of outliers in the dataset can be a result of an error, or it can be a real value present in the data as a result of the actual distribution of the data.</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>//localhost:1313/docs/foundational-algorithms/Linear-Regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/foundational-algorithms/Linear-Regression/</guid>
      <description>Linear regression is a supervised machine learning algorithm that is used to predict the continuous variable. The algorithm assumes that the relation between the dependent variable(Y) and independent variables(X), is linear and is represented by a line of best fit. In this chapter, we will learn how to execute linear regression in R using some select functions and test its assumptions before we use it for a final prediction on test data.</description>
    </item>
    
    <item>
      <title>data.table</title>
      <link>//localhost:1313/docs/useful-r-packages/datatable-tutorial-data-manipulation/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/useful-r-packages/datatable-tutorial-data-manipulation/</guid>
      <description>Overview If you are looking for fast execution of your code on large datasets, then you must read through this tutorial. Data manipulation tasks such as aggregations, add/remove/update of columns, joins, reading large files, etc., are all very important for any data science-related project. Keeping all these operations into mind, Matt Dowle and Arun Shrinivasan created a package called data.table.
data.table package is an extension of data.frame package in R. It is one of the first choices for data scientists while they work on large datasets.</description>
    </item>
    
    <item>
      <title>Ridge Regression</title>
      <link>//localhost:1313/docs/foundational-algorithms/Ridge-Regression/</link>
      <pubDate>Sun, 29 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/foundational-algorithms/Ridge-Regression/</guid>
      <description>Ridge Regression is a variation of linear regression. We use ridge regression to tackle the multicollinearity problem. Due to multicollinearity, the model estimates (least square) see a large variance. Ridge regression is a method by which we add a degree of bias to the regression estimates.
 Overview - Ridge Regression Ridge regression is a parsimonious model that performs L2 regularization. The L2 regularization adds a penalty equivalent to the square of the magnitude of regression coefficients and tries to minimize them.</description>
    </item>
    
    <item>
      <title>Missing Value Imputation</title>
      <link>//localhost:1313/docs/pre-processing-tasks/Missing-Value-Imputation/</link>
      <pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/pre-processing-tasks/Missing-Value-Imputation/</guid>
      <description>Overview Having missing values in a data set is a very common phenomenon. There are many reasons due to which a missing value occurs in a dataset. It is vital to figure out the reason for missing values. However, in this article, we will only focus on how to identify and impute the missing values.
Before we start, let us first, randomly add some missing values to the iris dataset.</description>
    </item>
    
    <item>
      <title>Leaftlet</title>
      <link>//localhost:1313/docs/useful-r-packages/Leaftlet-Package-for-Maps/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/useful-r-packages/Leaftlet-Package-for-Maps/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sentiment Analysis</title>
      <link>//localhost:1313/docs/text-mining/sentiment-analysis/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/text-mining/sentiment-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Lasso Regression</title>
      <link>//localhost:1313/docs/foundational-algorithms/Lasso-Regression/</link>
      <pubDate>Sun, 16 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/foundational-algorithms/Lasso-Regression/</guid>
      <description>LASSO stands for Least Absolute Shrinkage and Selection Operator. The algorithm is another variation of linear regression, just like ridge regression. We use lasso regression when we have a large number of predictor variables.
 Overview - Lasso Regression Lasso regression is a parsimonious model that performs L1 regularization. The L1 regularization adds a penalty equivalent to the absolute magnitude of regression coefficients and tries to minimize them. The equation of lasso is similar to ridge regression and looks like as given below.</description>
    </item>
    
    <item>
      <title>Summarizing data</title>
      <link>//localhost:1313/docs/pre-processing-tasks/Summarizing-Data/</link>
      <pubDate>Sun, 12 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/pre-processing-tasks/Summarizing-Data/</guid>
      <description>The very first task in any project related to data modeling is to explore the data, formally known as,
 Overview Exploratory Data Analysis(EDA) - There are many statistics that we calculate as part of EDA. However, in this chapter we will learn how to summarize data using descriptive statistics.
Data is a collection of observations and there features (known as variables). When we try to summarize the data, the variable type plays an important role in deciding which statistic we will be considering to summarize the variable.</description>
    </item>
    
    <item>
      <title>Topic Modeling Using LDA</title>
      <link>//localhost:1313/docs/text-mining/topic-modeling-lda/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/text-mining/topic-modeling-lda/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Binary Logistic Regression</title>
      <link>//localhost:1313/docs/foundational-algorithms/Binary-Logistics-Regression/</link>
      <pubDate>Mon, 10 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/foundational-algorithms/Binary-Logistics-Regression/</guid>
      <description>Logistics Regression is used to explain the relationship between the categorical dependent variable and one or more independent variables. When the dependent variable is dichotomous, we use binary logistic regression. However, by default, a binary logistic regression is almost always called logistics regression.
 Overview - Logistic Regression The logistic regression model is used to model the relationship between a binary target variable and a set of independent variables. These independent variables can be either qualitative or quantitative.</description>
    </item>
    
    <item>
      <title>Hypothesis Testing</title>
      <link>//localhost:1313/docs/pre-processing-tasks/Hypothesis-Testing/</link>
      <pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/pre-processing-tasks/Hypothesis-Testing/</guid>
      <description>Hypothesis testing uses concepts from statistics to determine the probability that a given assumption is valid. In this chapter, you will learn about several types of statistical tests, their practical applications, and how to interpret the results of hypothesis testing.
 Overview Through hypothesis testing, one can make inferences about the population parameters by analysing the sample statistics.
Typically hypothesis testing starts with an assumption or an assertion about a population parameter.</description>
    </item>
    
    <item>
      <title>Multinomial Logistic Regression</title>
      <link>//localhost:1313/docs/foundational-algorithms/Multinomial-Logistic-Regressions/</link>
      <pubDate>Sat, 22 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/foundational-algorithms/Multinomial-Logistic-Regressions/</guid>
      <description>Multinomial logistic regression is used when the target variable is categorical with more than two levels. It is an extension of binomial logistic regression.
 Overview - Multinomial Regression Multinomial regression is used to predict the nominal target variable. In case the target variable is of ordinal type, then we need to use ordinal logistic regression. In this tutorial, we will see how we can run multinomial logistic regression. As part of data preparation, ensure that data is free of multicollinearity, outliers, and high influential leverage points.</description>
    </item>
    
    <item>
      <title>Correlation</title>
      <link>//localhost:1313/docs/pre-processing-tasks/Correlation/</link>
      <pubDate>Sat, 07 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/pre-processing-tasks/Correlation/</guid>
      <description>Overview Correlation coefficients are used to describe the degree of association between quantitative variables. The value of the correlation lies between +1 to -1. The signs only indicate the direction of the relationship. That means a +0.86 value is equal to -0.86. However, -ve sign indicates that if one variable increases, the other decreases, and +ve sign indicates that if one variable increases, the other also increases. A value in the range of +0.</description>
    </item>
    
    <item>
      <title>k-Mean Clustering</title>
      <link>//localhost:1313/docs/foundational-algorithms/k-means-clustering/</link>
      <pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/foundational-algorithms/k-means-clustering/</guid>
      <description>Clustering is one of the most popular and widespread unsupervised machine learning method used for data analysis and mining patterns. At its core, clustering is the grouping of similar observations based upon the characteristics. There are multiple approaches for generating clusters of similar objects. However, in this section, you will learn how to build groups based on the k-Means algorithm.
 What is k-means clustering? In simple words, k-means clustering is a technique that aims to divide the data into k number of clusters.</description>
    </item>
    
    <item>
      <title>Analysing zero variance predictor</title>
      <link>//localhost:1313/docs/pre-processing-tasks/Analysing-zero-variance-predictor/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/pre-processing-tasks/Analysing-zero-variance-predictor/</guid>
      <description>Some variables in the dataset contain very little information because they mostly consist of a single value (e.g. zero). These variables are called a zero variance variable.
 Overview Many a time it happens that in the dataset, we have variables which either have unique values or have only a handful of unique values. The variables with only one exceptional value, when passed to fit the model, can cause problems like unstable models or, in some cases, can also cause the model to crash.</description>
    </item>
    
    <item>
      <title>Standardization And Scaling</title>
      <link>//localhost:1313/docs/pre-processing-tasks/Standardization-MinMax-Scaling/</link>
      <pubDate>Fri, 10 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/pre-processing-tasks/Standardization-MinMax-Scaling/</guid>
      <description>While building a Machine Learning model, do not throw away all of your information! Normalize your features.
 Overview It is a general requirement for many machine learning algorithms to have features with similar scales. It is important because if we pass features with different scales to algorithms like SVMs, perceptron’s, neural networks, optimization algorithms in logistic regressions, and more prominently, you can think of algorithms which use gradient descent will end up having a faster update for some feature values as compared to others.</description>
    </item>
    
    <item>
      <title>Principal Component Analysis</title>
      <link>//localhost:1313/docs/pre-processing-tasks/Principal-Component-Analysis/</link>
      <pubDate>Sat, 28 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/docs/pre-processing-tasks/Principal-Component-Analysis/</guid>
      <description>Principal component analysis(PCA) is an unsupervised machine learning technique that is used to reduce the dimensions of a large multi-dimensional dataset without losing much of the information. It is often also used to visualize and explore these high dimensional datasets.
 Overview One of the challenges among others that large datasets present is the time to model or learn the relationship between independent variables and the target variables. Thus it becomes essential for us to reduce the number of variables that we want to pass into the model to predict the target variable.</description>
    </item>
    
  </channel>
</rss>